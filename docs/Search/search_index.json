{
    "docs": [
        {
            "location": "/",
            "text": "You will find here a collection of cheatsheets, code templates and snippets that I have collected over the years...\n\n\nGiven that they were created for my own use, these notes are often very terse and dense. Thank you for your patience, while I am slowly improving their readability. I also have hundreds more to move to GitHub Pages :-) \n\n\nIn the meanwhile, feel free to use as you wish. Please email me suggestions and corrections.",
            "title": "Home"
        },
        {
            "location": "/Command_Prompt_Here/",
            "text": "Just type \"cmd\" to the location bar. It will start a new command prompt in current path.\n\n\nOR\n\n\nHold the \"Shift\" key while right-clicking a blank space in the desired folder to bring up a more verbose context menu. One of the options is \"Open PowerShell Here\". \n\n\nTo re-enable the \"Open Command Primpt Here\" (disabled by the Windows 10 Creators Update):\n\n\n\n\nLink\n\n\nLink 2",
            "title": "Command Prompt Here"
        },
        {
            "location": "/Big_Data/Hadoop_Ecosystem/",
            "text": "Hadoop Ecosystem\n\u00b6\n\n\nHadoop is not a single product, but rather a software family. Its common components consist of the following:\n\n\n\n\nPig, a scripting language used to quickly write MapReduce code to handle unstructured sources \n\n\nHive, used to facilitate structure for the data \n\n\nHCatalog, used to provide inter-operatability between these internal systems \n\n\nHBase, which is essentially a database built on top of Hadoop \n\n\nHDFS, the actual file system for hadoop.\n\n\nApache Mahout\n\n\nPackaging for Hadoop: \nBigTop\n\n\n\n\nHadoop structures data using Hive, but can handle unstructured data easily using Pig.\n\n\nHadoop and Mongo\n\u00b6\n\n\n\n\nHadoop and MongoDB\n\n\nHadoop and MongoDB Use Cases\n\n\n\n\nAWS EMR\n\u00b6\n\n\nAmazon EMR Best Practices\n\n\nAmazon EMR includes\n\n\n\n\nGanglia\n\n\nHadoop\n\n\nHBase\n\n\nHCatalog\n\n\nHive\n\n\nHue\n\n\nMahout\n\n\nOozie\n\n\nPhoenix\n\n\nPig\n\n\nPrest0\n\n\nSpark\n\n\nSqoop\n\n\nTez\n\n\nZeppelin\n\n\nZooKeeper",
            "title": "Hadoop Ecosystem"
        },
        {
            "location": "/Big_Data/Hadoop_Ecosystem/#hadoop-ecosystem",
            "text": "Hadoop is not a single product, but rather a software family. Its common components consist of the following:   Pig, a scripting language used to quickly write MapReduce code to handle unstructured sources   Hive, used to facilitate structure for the data   HCatalog, used to provide inter-operatability between these internal systems   HBase, which is essentially a database built on top of Hadoop   HDFS, the actual file system for hadoop.  Apache Mahout  Packaging for Hadoop:  BigTop   Hadoop structures data using Hive, but can handle unstructured data easily using Pig.",
            "title": "Hadoop Ecosystem"
        },
        {
            "location": "/Big_Data/Hadoop_Ecosystem/#hadoop-and-mongo",
            "text": "Hadoop and MongoDB  Hadoop and MongoDB Use Cases",
            "title": "Hadoop and Mongo"
        },
        {
            "location": "/Big_Data/Hadoop_Ecosystem/#aws-emr",
            "text": "Amazon EMR Best Practices  Amazon EMR includes   Ganglia  Hadoop  HBase  HCatalog  Hive  Hue  Mahout  Oozie  Phoenix  Pig  Prest0  Spark  Sqoop  Tez  Zeppelin  ZooKeeper",
            "title": "AWS EMR"
        },
        {
            "location": "/Big_Data/Install_Spark_2.3_Locally/",
            "text": "Install Spark 2.3 Locally\n\u00b6\n\n\nSpark runs on Java 8+, Python 2.7+/3.4+ and R 3.1+. For the Scala API, Spark 2.3.0 uses Scala 2.11. \n\n\nDownload Spark\n\u00b6\n\n\n Link \n\n\nJava\n\u00b6\n\n\nAll you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.\n\n\njava -version \n\n\n\n\n\nScala\n\u00b6\n\n\nDownload the Scala binaries for windows -- you will need Scala 11.x (not 10.x or 12.x) for Spark 2.3\n\n\n\n\nLatest Scala\n\n\nScala version 2.11.12\n\n\n\n\nTest correct installation of scala:\n\n\nscala -version\n\n\n\n\n\nSet PATH for Scala if needed:\n\n\nexport\n \nPATH\n \n=\n \n$PATH\n:/usr/local/scala/bin\n\n\n\n\n\nTest that Spark is properly installed:\n\n\n./bin/spark-shell --master local\n[\n2\n]\n\n\n\n\n\n\nOn Windows, use CMD or PowerShell, not git bash\n\n\nError: Failure to locate the winutils binary in the hadoop binary path\n\u00b6\n\n\n\n\nHADOOP_HOME (or the variable hadoop.home.dir property) needs to be set properly.\n\n\nKnown Hadoop for Windows issue: winutils is not included in the Apache distribution\n\n\n\n\nYou can fix this problem in two ways\n\n\n\n\nInstall a full native windows Hadoop version. The ASF does not currently release such a version; releases are available externally.\nOr: get the WINUTILS.EXE binary from a Hadoop redistribution. There is a repository of this for some Hadoop versions on github.\n\n\n\n\nThen\n\n\n\n\nSet the environment variable %HADOOP_HOME% to point to the directory above the BIN dir containing WINUTILS.EXE.\n\n\nOr: run the Java process with the system property hadoop.home.dir set to the home directory.\n\n\n\n\nExplanation on Hadoop Wiki\n\n\nStack Overflow\n\n\nWindows binaries for some Hadoop versions\n\n\nRun Spark on the local machine\n\u00b6\n\n\nTo run Spark interactively in a Python interpreter, use \nbin/pyspark\n:\n\n\n./bin/pyspark --master local\n[\n2\n]\n\n\n\n\n\n\nOr submit Spark jobs:\n\n\n./bin/spark-submit examples/src/main/python/pi.py \n10\n\n\n\n\n\n\nAdditional Links\n\u00b6\n\n\nSpark Installation Tutorial",
            "title": "Install Spark 2.3 Locally"
        },
        {
            "location": "/Big_Data/Install_Spark_2.3_Locally/#install-spark-23-locally",
            "text": "Spark runs on Java 8+, Python 2.7+/3.4+ and R 3.1+. For the Scala API, Spark 2.3.0 uses Scala 2.11.",
            "title": "Install Spark 2.3 Locally"
        },
        {
            "location": "/Big_Data/Install_Spark_2.3_Locally/#download-spark",
            "text": "Link",
            "title": "Download Spark"
        },
        {
            "location": "/Big_Data/Install_Spark_2.3_Locally/#java",
            "text": "All you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.  java -version",
            "title": "Java"
        },
        {
            "location": "/Big_Data/Install_Spark_2.3_Locally/#scala",
            "text": "Download the Scala binaries for windows -- you will need Scala 11.x (not 10.x or 12.x) for Spark 2.3   Latest Scala  Scala version 2.11.12   Test correct installation of scala:  scala -version  Set PATH for Scala if needed:  export   PATH   =   $PATH :/usr/local/scala/bin  Test that Spark is properly installed:  ./bin/spark-shell --master local [ 2 ]   On Windows, use CMD or PowerShell, not git bash",
            "title": "Scala"
        },
        {
            "location": "/Big_Data/Install_Spark_2.3_Locally/#error-failure-to-locate-the-winutils-binary-in-the-hadoop-binary-path",
            "text": "HADOOP_HOME (or the variable hadoop.home.dir property) needs to be set properly.  Known Hadoop for Windows issue: winutils is not included in the Apache distribution   You can fix this problem in two ways   Install a full native windows Hadoop version. The ASF does not currently release such a version; releases are available externally.\nOr: get the WINUTILS.EXE binary from a Hadoop redistribution. There is a repository of this for some Hadoop versions on github.   Then   Set the environment variable %HADOOP_HOME% to point to the directory above the BIN dir containing WINUTILS.EXE.  Or: run the Java process with the system property hadoop.home.dir set to the home directory.   Explanation on Hadoop Wiki  Stack Overflow  Windows binaries for some Hadoop versions",
            "title": "Error: Failure to locate the winutils binary in the hadoop binary path"
        },
        {
            "location": "/Big_Data/Install_Spark_2.3_Locally/#run-spark-on-the-local-machine",
            "text": "To run Spark interactively in a Python interpreter, use  bin/pyspark :  ./bin/pyspark --master local [ 2 ]   Or submit Spark jobs:  ./bin/spark-submit examples/src/main/python/pi.py  10",
            "title": "Run Spark on the local machine"
        },
        {
            "location": "/Big_Data/Install_Spark_2.3_Locally/#additional-links",
            "text": "Spark Installation Tutorial",
            "title": "Additional Links"
        },
        {
            "location": "/Big_Data/Spark_APIs/",
            "text": "DataFrames APIs\n\u00b6\n\n\nDataFrame operations: \n\n\n\n\nprintSchema()\n\n\nselect()\n\n\nshow()\n\n\ncount()\n\n\ngroupBy()\n\n\nsum()\n\n\nlimit()\n\n\norderBy()\n\n\nfilter()\n\n\nwithColumnRenamed()\n\n\njoin()\n\n\nwithColumn()\n\n\n\n\nExample:\n\n\n// In the Regular Expression below:\n\n\n// ^  - Matches beginning of line\n\n\n// .* - Matches any characters, except newline\n\n\n\ndf\n\n \n.\nfilter\n(\n$\n\"article\"\n.\nrlike\n(\n\"\"\"^Apache_.*\"\"\"\n))\n\n \n.\norderBy\n(\n$\n\"requests\"\n.\ndesc\n)\n\n \n.\nshow\n()\n \n// By default, show will return 20 rows\n\n\n\n// Import the sql functions package, which includes statistical functions like sum, max, min, avg, etc.\n\n\nimport\n \norg.apache.spark.sql.functions._\n\n\n\ndf\n.\ngroupBy\n(\n\"project\"\n).\nsum\n().\nshow\n()\n\n\n\n\n\n\nColumns\n\u00b6\n\n\nA new column is constructed based on the input columns present in a dataframe:\n\n\ndf\n(\n\"columnName\"\n)\n            \n// On a specific DataFrame.\n\n\ncol\n(\n\"columnName\"\n)\n           \n// A generic column no yet associated with a DataFrame.\n\n\ncol\n(\n\"columnName.field\"\n)\n     \n// Extracting a struct field\n\n\ncol\n(\n\"`a.column.with.dots`\"\n)\n \n// Escape `.` in column names.\n\n\n$\n\"columnName\"\n               \n// Scala short hand for a named column.\n\n\nexpr\n(\n\"a + 1\"\n)\n               \n// A column that is constructed from a parsed SQL Expression.\n\n\nlit\n(\n\"abc\"\n)\n                  \n// A column that produces a literal (constant) value.\n\n\n\n\n\n\nColumn objects can be composed to form complex expressions:\n\n\n$\n\"a\"\n \n+\n \n1\n\n\n$\n\"a\"\n \n===\n \n$\n\"b\"\n\n\n\n\n\n\nFile Read\n\u00b6\n\n\nCSV - Create a DataFrame with the anticipated structure\n\n\nval\n \nclickstreamDF\n \n=\n \nsqlContext\n.\nread\n.\nformat\n(\n\"com.databricks.spark.csv\"\n)\n\n  \n.\noption\n(\n\"header\"\n,\n \n\"true\"\n)\n\n  \n.\noption\n(\n\"delimiter\"\n,\n \n\"\\\\t\"\n)\n\n  \n.\noption\n(\n\"mode\"\n,\n \n\"PERMISSIVE\"\n)\n\n  \n.\noption\n(\n\"inferSchema\"\n,\n \n\"true\"\n)\n\n  \n.\nload\n(\n\"dbfs:///databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed\"\n)\n\n\n\n\n\n\nPARQUET - To create Dataset[Row] using SparkSession\n\n\nval\n \npeople\n \n=\n \nspark\n.\nread\n.\nparquet\n(\n\"...\"\n)\n\n\nval\n \ndepartment\n \n=\n \nspark\n.\nread\n.\nparquet\n(\n\"...\"\n)\n\n\n\npeople\n.\nfilter\n(\n\"age > 30\"\n)\n\n  \n.\njoin\n(\ndepartment\n,\n \npeople\n(\n\"deptId\"\n)\n \n===\n \ndepartment\n(\n\"id\"\n))\n\n  \n.\ngroupBy\n(\ndepartment\n(\n\"name\"\n),\n \n\"gender\"\n)\n\n  \n.\nagg\n(\navg\n(\npeople\n(\n\"salary\"\n)),\n \nmax\n(\npeople\n(\n\"age\"\n)))\n\n\n\n\n\n\nRepartitioning / Caching\n\u00b6\n\n\nval\n \nclickstreamNoIDs8partDF\n \n=\n \nclickstreamNoIDsDF\n.\nrepartition\n(\n8\n)\n\n\nclickstreamNoIDs8partDF\n.\nregisterTempTable\n(\n\"Clickstream\"\n)\n\n\nsqlContext\n.\ncacheTable\n(\n\"Clickstream\"\n)\n\n\n\n\n\n\nAn ideal partition size in Spark is about 50 MB - 200 MB.\nThe cache gets stored in Project Tungsten binary compressed columnar format.",
            "title": "Spark APIs"
        },
        {
            "location": "/Big_Data/Spark_APIs/#dataframes-apis",
            "text": "DataFrame operations:    printSchema()  select()  show()  count()  groupBy()  sum()  limit()  orderBy()  filter()  withColumnRenamed()  join()  withColumn()   Example:  // In the Regular Expression below:  // ^  - Matches beginning of line  // .* - Matches any characters, except newline  df \n  . filter ( $ \"article\" . rlike ( \"\"\"^Apache_.*\"\"\" )) \n  . orderBy ( $ \"requests\" . desc ) \n  . show ()   // By default, show will return 20 rows  // Import the sql functions package, which includes statistical functions like sum, max, min, avg, etc.  import   org.apache.spark.sql.functions._  df . groupBy ( \"project\" ). sum (). show ()",
            "title": "DataFrames APIs"
        },
        {
            "location": "/Big_Data/Spark_APIs/#columns",
            "text": "A new column is constructed based on the input columns present in a dataframe:  df ( \"columnName\" )              // On a specific DataFrame.  col ( \"columnName\" )             // A generic column no yet associated with a DataFrame.  col ( \"columnName.field\" )       // Extracting a struct field  col ( \"`a.column.with.dots`\" )   // Escape `.` in column names.  $ \"columnName\"                 // Scala short hand for a named column.  expr ( \"a + 1\" )                 // A column that is constructed from a parsed SQL Expression.  lit ( \"abc\" )                    // A column that produces a literal (constant) value.   Column objects can be composed to form complex expressions:  $ \"a\"   +   1  $ \"a\"   ===   $ \"b\"",
            "title": "Columns"
        },
        {
            "location": "/Big_Data/Spark_APIs/#file-read",
            "text": "CSV - Create a DataFrame with the anticipated structure  val   clickstreamDF   =   sqlContext . read . format ( \"com.databricks.spark.csv\" ) \n   . option ( \"header\" ,   \"true\" ) \n   . option ( \"delimiter\" ,   \"\\\\t\" ) \n   . option ( \"mode\" ,   \"PERMISSIVE\" ) \n   . option ( \"inferSchema\" ,   \"true\" ) \n   . load ( \"dbfs:///databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed\" )   PARQUET - To create Dataset[Row] using SparkSession  val   people   =   spark . read . parquet ( \"...\" )  val   department   =   spark . read . parquet ( \"...\" )  people . filter ( \"age > 30\" ) \n   . join ( department ,   people ( \"deptId\" )   ===   department ( \"id\" )) \n   . groupBy ( department ( \"name\" ),   \"gender\" ) \n   . agg ( avg ( people ( \"salary\" )),   max ( people ( \"age\" )))",
            "title": "File Read"
        },
        {
            "location": "/Big_Data/Spark_APIs/#repartitioning-caching",
            "text": "val   clickstreamNoIDs8partDF   =   clickstreamNoIDsDF . repartition ( 8 )  clickstreamNoIDs8partDF . registerTempTable ( \"Clickstream\" )  sqlContext . cacheTable ( \"Clickstream\" )   An ideal partition size in Spark is about 50 MB - 200 MB.\nThe cache gets stored in Project Tungsten binary compressed columnar format.",
            "title": "Repartitioning / Caching"
        },
        {
            "location": "/Big_Data/Spark_Basics/",
            "text": "Spark Basics\n\u00b6\n\n\n\n\nMain Web Site\n\n\nApache Spark on Wikipedia\n\n\n\n\nUseful Links\n\u00b6\n\n\n\n\nAmpcamp big data bootcamp\n\n\nRDDs Simplified\n\n\nElasticsearch and Apache Lucene for Apache Spark and MLlib\n\n\nSpark on AWS\n\n\nRunning Apache Spark on AWS\n\n\nRunning Apache Spark EMR and EC2 scripts on AWS with read write S3\n\n\nSpark on EMR - How to Submit a Spark Application with EMR Steps\n\n\nDatabricks Reference Apps\n\n\nIntroduction to Apache Spark with Examples and Use Cases\n\n\n\n\nSpark and MongoDB\n\u00b6\n\n\n\n\nUsing MongoDB with Apache Spark\n\n\nMongoDB Spark connector\n\n\n\n\nSpark and NLP\n\u00b6\n\n\n\n\nDictionary Based Annotation at Scale with Spark, SolrTextTagger and OpenNLP\n\n\n\n\nHere is a complete set of example on how to use DL4J (Deep Learning for Java) that uses UIMA on the SPARK platform\n\n\nDeep Learning for Java\n\n\nand in the following project the use of CTAKES UIMA module from within the Spark framework\n\n\nNatural Language Processing with Apache Spark\n\n\nGraphX\n\u00b6\n\n\n\n\nGraphx programming guide\n\n\n\n\nApache Zeppelin\n\u00b6\n\n\nConnect to Zeppelin using the same SSH tunneling method to connect to other web servers on the master node. Zeppelin server is found at port 8890.\n\n\nZeppelin",
            "title": "Spark Basics"
        },
        {
            "location": "/Big_Data/Spark_Basics/#spark-basics",
            "text": "Main Web Site  Apache Spark on Wikipedia",
            "title": "Spark Basics"
        },
        {
            "location": "/Big_Data/Spark_Basics/#useful-links",
            "text": "Ampcamp big data bootcamp  RDDs Simplified  Elasticsearch and Apache Lucene for Apache Spark and MLlib  Spark on AWS  Running Apache Spark on AWS  Running Apache Spark EMR and EC2 scripts on AWS with read write S3  Spark on EMR - How to Submit a Spark Application with EMR Steps  Databricks Reference Apps  Introduction to Apache Spark with Examples and Use Cases",
            "title": "Useful Links"
        },
        {
            "location": "/Big_Data/Spark_Basics/#spark-and-mongodb",
            "text": "Using MongoDB with Apache Spark  MongoDB Spark connector",
            "title": "Spark and MongoDB"
        },
        {
            "location": "/Big_Data/Spark_Basics/#spark-and-nlp",
            "text": "Dictionary Based Annotation at Scale with Spark, SolrTextTagger and OpenNLP   Here is a complete set of example on how to use DL4J (Deep Learning for Java) that uses UIMA on the SPARK platform  Deep Learning for Java  and in the following project the use of CTAKES UIMA module from within the Spark framework  Natural Language Processing with Apache Spark",
            "title": "Spark and NLP"
        },
        {
            "location": "/Big_Data/Spark_Basics/#graphx",
            "text": "Graphx programming guide",
            "title": "GraphX"
        },
        {
            "location": "/Big_Data/Spark_Basics/#apache-zeppelin",
            "text": "Connect to Zeppelin using the same SSH tunneling method to connect to other web servers on the master node. Zeppelin server is found at port 8890.  Zeppelin",
            "title": "Apache Zeppelin"
        },
        {
            "location": "/Big_Data/Spark_Development_with_sbt_and_InteliJ/",
            "text": "Setup a Spark Development Environment with IntelliJ and sbt\n\u00b6\n\n\nUseful Links\n\u00b6\n\n\n Hortonworks tutorial \n\n\nPackaging and Submission Steps using sbt\n\u00b6\n\n\nPackage a jar containing your application:\n\n\n$ sbt package\n...\n\n[\ninfo\n]\n Packaging \n{\n..\n}\n/\n{\n..\n}\n/target/scala-2.11/simple-project_2.11-1.0.jar\n\n\n\n\n\nDon't use \nsbt run\n \n\n\nThen use [spark submit] ( https://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit )\n to run your application\n\n\nYOUR_SPARK_HOME/bin/spark-submit \n\\\n\n  --class \n\"SimpleApp\"\n \n\\\n\n  --master local\n[\n4\n]\n \n\\\n\n  target/scala-2.11/simple-project_2.11-1.0.jar\n\n\n\n\n\nOpen the Spark UI to monitor: \n http://localhost:4040 \n\n\nPlugins\n\u00b6\n\n\nsbt-spark-package\n\u00b6\n\n\nThe \n Sbt Plugin for Spark Packages \n is a Sbt plugin that aims to simplify the use and development of Spark Packages.\n\n\n Blog \n\n\nIntelliJ plugin for Spark\n\u00b6\n\n\nNote: does not work with IntelliJ 2018.1\n\n\nThe \n IntelliJ plugin for Spark \n supports for deployment spark application and cluster monitoring. \n\n\n\n\nTo install, download the plugin\n\n\nFile > Settings, Plugins tab, browse repos... point to the zip file",
            "title": "Setup a Spark Development Environment with IntelliJ and sbt"
        },
        {
            "location": "/Big_Data/Spark_Development_with_sbt_and_InteliJ/#setup-a-spark-development-environment-with-intellij-and-sbt",
            "text": "",
            "title": "Setup a Spark Development Environment with IntelliJ and sbt"
        },
        {
            "location": "/Big_Data/Spark_Development_with_sbt_and_InteliJ/#useful-links",
            "text": "Hortonworks tutorial",
            "title": "Useful Links"
        },
        {
            "location": "/Big_Data/Spark_Development_with_sbt_and_InteliJ/#packaging-and-submission-steps-using-sbt",
            "text": "Package a jar containing your application:  $ sbt package\n... [ info ]  Packaging  { .. } / { .. } /target/scala-2.11/simple-project_2.11-1.0.jar  Don't use  sbt run    Then use [spark submit] ( https://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit )\n to run your application  YOUR_SPARK_HOME/bin/spark-submit  \\ \n  --class  \"SimpleApp\"   \\ \n  --master local [ 4 ]   \\ \n  target/scala-2.11/simple-project_2.11-1.0.jar  Open the Spark UI to monitor:   http://localhost:4040",
            "title": "Packaging and Submission Steps using sbt"
        },
        {
            "location": "/Big_Data/Spark_Development_with_sbt_and_InteliJ/#plugins",
            "text": "",
            "title": "Plugins"
        },
        {
            "location": "/Big_Data/Spark_Development_with_sbt_and_InteliJ/#sbt-spark-package",
            "text": "The   Sbt Plugin for Spark Packages   is a Sbt plugin that aims to simplify the use and development of Spark Packages.   Blog",
            "title": "sbt-spark-package"
        },
        {
            "location": "/Big_Data/Spark_Development_with_sbt_and_InteliJ/#intellij-plugin-for-spark",
            "text": "Note: does not work with IntelliJ 2018.1  The   IntelliJ plugin for Spark   supports for deployment spark application and cluster monitoring.    To install, download the plugin  File > Settings, Plugins tab, browse repos... point to the zip file",
            "title": "IntelliJ plugin for Spark"
        },
        {
            "location": "/Big_Data/Spark_on_AWS_EMR/",
            "text": "Spark on AWS EMR\n\u00b6\n\n\nKey Links\n\u00b6\n\n\nSpark on AWS EMR\n\n\nCreate a EMR Cluster with Spark using the AWS Console\n\u00b6\n\n\nThe following procedure creates a cluster with Spark installed.\n\n\n\n\nOpen the Amazon EMR console at \nhttps://console.aws.amazon.com/elasticmapreduce/\n.\n\n\nChoose Create cluster to use Quick Create.\n\n\nFor the Software Configuration field, choose Amazon Release Version emr-5.0.0 or later.\n\n\nIn the Select Applications field, choose either All Applications or Spark.\n\n\nSelect other options as necessary and then choose Create cluster\n\n\n\n\n\n\n\n\nCreate a EMR Cluster with Spark using the AWS CLI\n\u00b6\n\n\nSimple cluster:\n\n\naws emr create-cluster --name \n\"Spark cluster\"\n --release-label emr-5.0.0 --applications \nName\n=\nSpark \n\\ \n\n--ec2-attributes \nKeyName\n=\nmyKey --instance-type m3.xlarge --instance-count \n3\n --use-default-roles\n\n\n\n\n\nNote: For Windows, replace the above Linux line continuation character () with the caret (^).\n\n\nWhen using a config file:\n\n\naws emr create-cluster --release-label --applications \nName\n=\nSpark \n\\\n\n--instance-type m3.xlarge --instance-count \n3\n --configurations https://s3.amazonaws.com/mybucket/myfolder/myConfig.json\n\n\n\n\n\nSample myConfig.json:\n\n\n[\n\n  \n{\n\n    \n\"Classification\"\n:\n \n\"spark\"\n,\n\n    \n\"Properties\"\n:\n \n{\n\n      \n\"maximizeResourceAllocation\"\n:\n \n\"true\"\n\n    \n}\n\n  \n}\n\n\n]\n\n\n\n\n\n\nUsing Spot instances:\n\n\naws emr create-cluster --name \n\"Spot cluster\"\n --release-label emr-5.0.0 --applications \nName\n=\nSpark \n\\\n\n--use-default-roles --ec2-attributes \nKeyName\n=\nmyKey \n\\\n\n--instance-groups \nInstanceGroupType\n=\nMASTER,InstanceType\n=\nm3.xlarge,InstanceCount\n=\n1\n,BidPrice\n=\n0\n.25 \n\\\n\n\nInstanceGroupType\n=\nCORE,BidPrice\n=\n0\n.03,InstanceType\n=\nm3.xlarge,InstanceCount\n=\n2\n\n\n\n# InstanceGroupType=TASK,BidPrice=0.10,InstanceType=m3.xlarge,InstanceCount=3\n\n\n\n\n\n\nIn Java:\n\n\n// start Spark on EMR in java\n\n\nAmazonElasticMapReduceClient\n \nemr\n \n=\n \nnew\n \nAmazonElasticMapReduceClient\n(\ncredentials\n);\n\n\nApplication\n \nsparkApp\n \n=\n \nnew\n \nApplication\n()\n \n.\nwithName\n(\n\"Spark\"\n);\n\n\nApplications\n \nmyApps\n \n=\n \nnew\n \nApplications\n();\n\n\nmyApps\n.\nadd\n(\nsparkApp\n);\n\n\nRunJobFlowRequest\n \nrequest\n \n=\n \nnew\n \nRunJobFlowRequest\n()\n \n.\nwithName\n(\n\"Spark Cluster\"\n)\n \n.\nwithApplications\n(\nmyApps\n)\n \n.\nwithReleaseLabel\n(\n\"\"\n)\n \n.\nwithInstances\n(\nnew\n \nJobFlowInstancesConfig\n()\n \n.\nwithEc2KeyName\n(\n\"myKeyName\"\n)\n \n.\nwithInstanceCount\n(\n1\n)\n \n.\nwithKeepJobFlowAliveWhenNoSteps\n(\ntrue\n)\n \n.\nwithMasterInstanceType\n(\n\"m3.xlarge\"\n)\n \n.\nwithSlaveInstanceType\n(\n\"m3.xlarge\"\n)\n \n);\n \nRunJobFlowResult\n \nresult\n \n=\n \nemr\n.\nrunJobFlow\n(\nrequest\n);\n\n\n\n\n\n\nConnect to the Master Node using SSH\n\u00b6\n\n\nTo connect to the master node using SSH, you need the public DNS name of the master node and your Amazon EC2 key pair private key. The Amazon EC2 key pair private key is specified when you launch the cluster.\n\n\n\n\nTo retrieve the cluster identifier / the public DNS name of the master node, type the following command:\n\n\n\n\naws emr list-clusters\n\n\n\n\n\nThe output lists your clusters including the cluster IDs. Note the cluster ID for the cluster to which you are connecting.\n\n\n\"Status\"\n:\n \n{\n     \n\"Timeline\"\n:\n \n{\n         \n\"ReadyDateTime\"\n:\n \n1408040782.374\n,\n         \n\"CreationDateTime\"\n:\n \n1408040501.213\n     \n},\n     \n\"State\"\n:\n \n\"WAITING\"\n,\n     \n\"StateChangeReason\"\n:\n \n{\n         \n\"Message\"\n:\n \n\"Waiting after step completed\"\n     \n}\n \n}\n,\n \n\"NormalizedInstanceHours\"\n:\n \n4\n,\n\"Id\"\n:\n \n\"j-2AL4XXXXXX5T9\"\n,\n \n\"Name\"\n:\n \n\"My cluster\"\n\n\n\n\n\n\n\n\nTo list the cluster instances including the master public DNS name for the cluster, type one of the following commands. Replace j-2AL4XXXXXX5T9 with the cluster ID returned by the previous command.\n\n\n\n\naws emr list-instances --cluster-id j-2AL4XXXXXX5T9Or:aws emr describe-clusters --cluster-id j-2AL4XXXXXX5T9\n\n\n\n\n\nView the Web Interfaces Hosted on Amazon EMR Clusters\n\u00b6\n\n\n\n\n\n\nView Web Interfaces Hosted on Amazon EMR Clusters\n\n\n\n\n\n\nYARN ResourceManager: \nhttp://master-public-dns-name:8088\n\n\n\n\nYARN NodeManager: \nhttp://slave-public-dns-name:8042\n\n\nHadoop HDFS NameNode: \nhttp://master-public-dns-name:50070\n\n\nHadoop HDFS DataNode: \nhttp://slave-public-dns-name:50075\n\n\nSpark HistoryServer: \nhttp://master-public-dns-name:18080\n\n\nZeppelin: \nhttp://master-public-dns-name:8890\n\n\nHue: \nhttp://master-public-dns-name:8888\n\n\nGanglia: \nhttp://master-public-dns-name/ganglia\n\n\nHBase UI: \nhttp://master-public-dns-name:16010",
            "title": "Spark on AWS EMR"
        },
        {
            "location": "/Big_Data/Spark_on_AWS_EMR/#spark-on-aws-emr",
            "text": "",
            "title": "Spark on AWS EMR"
        },
        {
            "location": "/Big_Data/Spark_on_AWS_EMR/#key-links",
            "text": "Spark on AWS EMR",
            "title": "Key Links"
        },
        {
            "location": "/Big_Data/Spark_on_AWS_EMR/#create-a-emr-cluster-with-spark-using-the-aws-console",
            "text": "The following procedure creates a cluster with Spark installed.   Open the Amazon EMR console at  https://console.aws.amazon.com/elasticmapreduce/ .  Choose Create cluster to use Quick Create.  For the Software Configuration field, choose Amazon Release Version emr-5.0.0 or later.  In the Select Applications field, choose either All Applications or Spark.  Select other options as necessary and then choose Create cluster",
            "title": "Create a EMR Cluster with Spark using the AWS Console"
        },
        {
            "location": "/Big_Data/Spark_on_AWS_EMR/#create-a-emr-cluster-with-spark-using-the-aws-cli",
            "text": "Simple cluster:  aws emr create-cluster --name  \"Spark cluster\"  --release-label emr-5.0.0 --applications  Name = Spark  \\  \n--ec2-attributes  KeyName = myKey --instance-type m3.xlarge --instance-count  3  --use-default-roles  Note: For Windows, replace the above Linux line continuation character () with the caret (^).  When using a config file:  aws emr create-cluster --release-label --applications  Name = Spark  \\ \n--instance-type m3.xlarge --instance-count  3  --configurations https://s3.amazonaws.com/mybucket/myfolder/myConfig.json  Sample myConfig.json:  [ \n   { \n     \"Classification\" :   \"spark\" , \n     \"Properties\" :   { \n       \"maximizeResourceAllocation\" :   \"true\" \n     } \n   }  ]   Using Spot instances:  aws emr create-cluster --name  \"Spot cluster\"  --release-label emr-5.0.0 --applications  Name = Spark  \\ \n--use-default-roles --ec2-attributes  KeyName = myKey  \\ \n--instance-groups  InstanceGroupType = MASTER,InstanceType = m3.xlarge,InstanceCount = 1 ,BidPrice = 0 .25  \\  InstanceGroupType = CORE,BidPrice = 0 .03,InstanceType = m3.xlarge,InstanceCount = 2  # InstanceGroupType=TASK,BidPrice=0.10,InstanceType=m3.xlarge,InstanceCount=3   In Java:  // start Spark on EMR in java  AmazonElasticMapReduceClient   emr   =   new   AmazonElasticMapReduceClient ( credentials );  Application   sparkApp   =   new   Application ()   . withName ( \"Spark\" );  Applications   myApps   =   new   Applications ();  myApps . add ( sparkApp );  RunJobFlowRequest   request   =   new   RunJobFlowRequest ()   . withName ( \"Spark Cluster\" )   . withApplications ( myApps )   . withReleaseLabel ( \"\" )   . withInstances ( new   JobFlowInstancesConfig ()   . withEc2KeyName ( \"myKeyName\" )   . withInstanceCount ( 1 )   . withKeepJobFlowAliveWhenNoSteps ( true )   . withMasterInstanceType ( \"m3.xlarge\" )   . withSlaveInstanceType ( \"m3.xlarge\" )   );   RunJobFlowResult   result   =   emr . runJobFlow ( request );",
            "title": "Create a EMR Cluster with Spark using the AWS CLI"
        },
        {
            "location": "/Big_Data/Spark_on_AWS_EMR/#connect-to-the-master-node-using-ssh",
            "text": "To connect to the master node using SSH, you need the public DNS name of the master node and your Amazon EC2 key pair private key. The Amazon EC2 key pair private key is specified when you launch the cluster.   To retrieve the cluster identifier / the public DNS name of the master node, type the following command:   aws emr list-clusters  The output lists your clusters including the cluster IDs. Note the cluster ID for the cluster to which you are connecting.  \"Status\" :   {       \"Timeline\" :   {           \"ReadyDateTime\" :   1408040782.374 ,           \"CreationDateTime\" :   1408040501.213       },       \"State\" :   \"WAITING\" ,       \"StateChangeReason\" :   {           \"Message\" :   \"Waiting after step completed\"       }   } ,   \"NormalizedInstanceHours\" :   4 , \"Id\" :   \"j-2AL4XXXXXX5T9\" ,   \"Name\" :   \"My cluster\"    To list the cluster instances including the master public DNS name for the cluster, type one of the following commands. Replace j-2AL4XXXXXX5T9 with the cluster ID returned by the previous command.   aws emr list-instances --cluster-id j-2AL4XXXXXX5T9Or:aws emr describe-clusters --cluster-id j-2AL4XXXXXX5T9",
            "title": "Connect to the Master Node using SSH"
        },
        {
            "location": "/Big_Data/Spark_on_AWS_EMR/#view-the-web-interfaces-hosted-on-amazon-emr-clusters",
            "text": "View Web Interfaces Hosted on Amazon EMR Clusters    YARN ResourceManager:  http://master-public-dns-name:8088   YARN NodeManager:  http://slave-public-dns-name:8042  Hadoop HDFS NameNode:  http://master-public-dns-name:50070  Hadoop HDFS DataNode:  http://slave-public-dns-name:50075  Spark HistoryServer:  http://master-public-dns-name:18080  Zeppelin:  http://master-public-dns-name:8890  Hue:  http://master-public-dns-name:8888  Ganglia:  http://master-public-dns-name/ganglia  HBase UI:  http://master-public-dns-name:16010",
            "title": "View the Web Interfaces Hosted on Amazon EMR Clusters"
        },
        {
            "location": "/Big_Data/Spark_on_EC2/",
            "text": "Install Spark on EC2 with Flintrock\n\u00b6\n\n\nKey Links\n\u00b6\n\n\n Flintrock GitHub Repo\n\n\nConfigurable CLI Defaults\n\u00b6\n\n\nFlintrock lets you persist your desired configuration to a YAML file so that you don't have to keep typing out the same options over and over at the command line.\n\n\nTo setup and edit the default config file, run this:\n\n\nflintrock configure\n\n\n\n\n\nSample config.yaml\n\u00b6\n\n\nprovider\n:\n \nec2\n\n\n\nservices\n:\n\n  \nspark\n:\n\n    \nversion\n:\n \n2.2.0\n\n\n\nlaunch\n:\n\n  \nnum-slaves\n:\n \n1\n\n\n\nproviders\n:\n\n  \nec2\n:\n\n    \nkey-name\n:\n \nkey_name\n\n    \nidentity-file\n:\n \n/path/to/.ssh/key.pem\n\n    \ninstance-type\n:\n \nm3.medium\n\n    \nregion\n:\n \nus-east-1\n\n    \nami\n:\n \nami-97785bed\n\n    \nuser\n:\n \nec2-user\n\n\n\n\n\n\nWith a config file like that, you can now launch a cluster:\n\n\nflintrock launch test-cluster",
            "title": "Install Spark on EC2 with Flintrock"
        },
        {
            "location": "/Big_Data/Spark_on_EC2/#install-spark-on-ec2-with-flintrock",
            "text": "",
            "title": "Install Spark on EC2 with Flintrock"
        },
        {
            "location": "/Big_Data/Spark_on_EC2/#key-links",
            "text": "Flintrock GitHub Repo",
            "title": "Key Links"
        },
        {
            "location": "/Big_Data/Spark_on_EC2/#configurable-cli-defaults",
            "text": "Flintrock lets you persist your desired configuration to a YAML file so that you don't have to keep typing out the same options over and over at the command line.  To setup and edit the default config file, run this:  flintrock configure",
            "title": "Configurable CLI Defaults"
        },
        {
            "location": "/Big_Data/Spark_on_EC2/#sample-configyaml",
            "text": "provider :   ec2  services : \n   spark : \n     version :   2.2.0  launch : \n   num-slaves :   1  providers : \n   ec2 : \n     key-name :   key_name \n     identity-file :   /path/to/.ssh/key.pem \n     instance-type :   m3.medium \n     region :   us-east-1 \n     ami :   ami-97785bed \n     user :   ec2-user   With a config file like that, you can now launch a cluster:  flintrock launch test-cluster",
            "title": "Sample config.yaml"
        },
        {
            "location": "/Big_Data/Spark_on_Kubernetes/",
            "text": "Background\n\u00b6\n\n\nIntroduction to Spark on Kubernetes\n\n\nRunning Spark on Kubernetes\n\u00b6\n\n\nMain Page\n \n\n\nPrerequisites:\n\n\n\n\nA runnable distribution of Spark 2.3 or above.\n\n\nA running Kubernetes cluster at version >= 1.6 with access configured to it using kubectl. If you do not already have a working Kubernetes cluster, you may setup a test cluster on your local machine using minikube.\nWe recommend using the latest release of minikube with the DNS addon enabled.\n\n\nBe aware that the default minikube configuration is not enough for running Spark applications. We recommend 3 CPUs and 4g of memory to be able to start a simple Spark application with a single executor.\n\n\nYou must have appropriate permissions to list, create, edit and delete pods in your cluster. You can verify that you can list these resources by running kubectl auth can-i \n pods.\nThe service account credentials used by the driver pods must be allowed to create pods, services and configmaps.\n\n\nYou must have Kubernetes DNS configured in your cluster.\n\n\n\n\nSteps\n\u00b6\n\n\n\n\n\n\nNeed Kubernetes version 1.6 and above.\nTo check the version, enter \nkubectl version\n.\n\n\n\n\n\n\nThe cluster must be configured to use the kube-dns addon. Check with\n\n\n\n\n\n\nminikube addons list\n\n\n\n\n\nKubernetes DNS Page\n\n\n\n\nStart minikube with the recommended configuration for Spark\n\n\n\n\nminikube start --cpus \n3\n --memory \n4096\n\n\n\n\n\n\n\n\nSubmit a Spark job using: \n\n\n\n\n$ bin/spark-submit \n\\\n\n    --master k8s://https://<k8s-apiserver-host>:<k8s-apiserver-port> \n\\\n\n    --deploy-mode cluster \n\\\n\n    --name spark-pi \n\\\n\n    --class org.apache.spark.examples.SparkPi \n\\\n\n    --conf spark.executor.instances\n=\n3\n \n\\\n\n    --conf spark.kubernetes.container.image\n=\n<spark-image> \n\\\n\n    local:///path/to/examples.jar\n\n\n\n\n\nUse \nkubectl cluster-info\n to get the K8s API server URL\n\n\nSpark (starting with version 2.3) ships with a Dockerfile in the \nkubernetes/dockerfiles/\n directory.\n\n\n\n\nAccess logs:\n\n\n\n\n$ kubectl -n\n=\n<namespace> logs -f <driver-pod-name>\n\n\n\n\n\n\n\nAccessing Driver UI:\n\n\n\n\n$ kubectl port-forward <driver-pod-name> \n4040\n:4040\n\n\n\n\n\nThen go to \n http://localhost:4040 \n\n\nAlternatives\n\u00b6\n\n\nHelm Chart for Spark\n\n\nThe same on KubeApps \n\n\nhelm install --name my-spark-release --version \n0\n.1.12 stable/spark",
            "title": "Spark 2.3 on Kubernetes"
        },
        {
            "location": "/Big_Data/Spark_on_Kubernetes/#background",
            "text": "Introduction to Spark on Kubernetes",
            "title": "Background"
        },
        {
            "location": "/Big_Data/Spark_on_Kubernetes/#running-spark-on-kubernetes",
            "text": "Main Page    Prerequisites:   A runnable distribution of Spark 2.3 or above.  A running Kubernetes cluster at version >= 1.6 with access configured to it using kubectl. If you do not already have a working Kubernetes cluster, you may setup a test cluster on your local machine using minikube.\nWe recommend using the latest release of minikube with the DNS addon enabled.  Be aware that the default minikube configuration is not enough for running Spark applications. We recommend 3 CPUs and 4g of memory to be able to start a simple Spark application with a single executor.  You must have appropriate permissions to list, create, edit and delete pods in your cluster. You can verify that you can list these resources by running kubectl auth can-i   pods.\nThe service account credentials used by the driver pods must be allowed to create pods, services and configmaps.  You must have Kubernetes DNS configured in your cluster.",
            "title": "Running Spark on Kubernetes"
        },
        {
            "location": "/Big_Data/Spark_on_Kubernetes/#steps",
            "text": "Need Kubernetes version 1.6 and above.\nTo check the version, enter  kubectl version .    The cluster must be configured to use the kube-dns addon. Check with    minikube addons list  Kubernetes DNS Page   Start minikube with the recommended configuration for Spark   minikube start --cpus  3  --memory  4096    Submit a Spark job using:    $ bin/spark-submit  \\ \n    --master k8s://https://<k8s-apiserver-host>:<k8s-apiserver-port>  \\ \n    --deploy-mode cluster  \\ \n    --name spark-pi  \\ \n    --class org.apache.spark.examples.SparkPi  \\ \n    --conf spark.executor.instances = 3   \\ \n    --conf spark.kubernetes.container.image = <spark-image>  \\ \n    local:///path/to/examples.jar  Use  kubectl cluster-info  to get the K8s API server URL  Spark (starting with version 2.3) ships with a Dockerfile in the  kubernetes/dockerfiles/  directory.   Access logs:   $ kubectl -n = <namespace> logs -f <driver-pod-name>   Accessing Driver UI:   $ kubectl port-forward <driver-pod-name>  4040 :4040  Then go to   http://localhost:4040",
            "title": "Steps"
        },
        {
            "location": "/Big_Data/Spark_on_Kubernetes/#alternatives",
            "text": "Helm Chart for Spark  The same on KubeApps   helm install --name my-spark-release --version  0 .1.12 stable/spark",
            "title": "Alternatives"
        },
        {
            "location": "/Cloud/AWS/",
            "text": "AWS Services Overview\n\u00b6\n\n\nBasic Services\n\u00b6\n\n\n\n\nCompute: EC2 (autoscaling, ELB load balancing)\n\n\nNetworking / Security: VPC (security groups), IAM (users/groups/application roles)\n\n\n\n\nStorage\n\u00b6\n\n\n\n\nS3: secure, scalable object-level storage, static web site hosting...\n\n\nGlacier: long-term storage\n\n\nEBS: block-level storage (for EC2 instances)\n\n\n\n\nDatabases\n\u00b6\n\n\n\n\nRDS: relational databases (MySQL, PostgreSQL, MSSQL, MariaDB, Aurora...)\n\n\nDynamoDB: scalable NoSQL database backed by solid-state drives\n\n\n\n\nAnalytics\n\u00b6\n\n\n\n\nRedShift: PostgreSQL-based columnstore OLAP database that uses SQL. MPP architecture.\n\n\nEMR: Hadoop cluster (Hive, Pig, HBase, Spark...).\n\n\n\n\nETL / ELT / Batch Processing\n\u00b6\n\n\n\n\nGlue\n\n\nBatch\n\n\nData Pipeline: orchestrate data transfers between S3, DynamoDB, Redshift\n\n\n\n\nApplication Services\n\u00b6\n\n\n\n\nNotifications: SNS (alerts by email, SMS...), SES (bulk email)\n\n\nQueue: SQS (async message queues for component decoupling) \n\n\nWorkflows, State Machine as a Service: AWS Step Functions, SWF (task-oriented workflows - complicated)\n\n\nDocument Search: ElasticSearch, CloudSearch\n\n\n\n\nMonitoring\n\u00b6\n\n\n\n\nCloudwatch (monitor services and instances e.g. CPU utilization, etc...)\n\n\nCloudTrail (monitor API calls)\n\n\n\n\nInfrastructure Deployment / Automation\n\u00b6\n\n\n\n\nElastic Beanstalk (simple, mostly web or Linux worker)\n\n\nCloudFormation (JSON / YAML templates - more difficult, but many existing templates)\n\n\nOpsWork (higher level than CloudFormation, uses non-native components - Chef-based)\n\n\n\n\nDesktop in the Cloud\n\u00b6\n\n\n\n\nWorkSpaces\n\n\n\n\nDetails\n\u00b6\n\n\nTools\n\u00b6\n\n\n\n\n\n\nUnix tools on Windows: \nCygwin\n\n\n\n\n\n\nPutty SSH client for Windows \ndoc\n\n\n\n\n\n\nDownload and install PuTTY \nlink\n. Be sure to install the entire suite.\n\n\n\n\nStart PuTTYgen (for example, from the Start menu, click All Programs > PuTTY > PuTTYgen).\n\n\nUnder Type of key to generate, select SSH-2 RSA.\n\n\nLoad the .pem file (private key) downloaded from the console (in \"credentials\" folder) \n\n\n\n\nSave private key\n\n\n\n\n\n\nAWS \ncommand line interface\n\n\n\n\n\n\nAWS toolkit for Visual Studio\n\n\n\n\n\n\nAWS tools for PowerShell\n\n\n\n\n\n\nAWS EC2\n\u00b6\n\n\n\n\nLog onto instance with \nPutty SSH\n\n\n\n\nlogin as: ec2-user (Amazon Linux) or: ubuntu\n\n\nBash shell documentation\n\n\n\n\n\n\nUse a shell script to configure the instance \nlink\n\n\n\n\n\n\nUser data: You can specify user data to configure an instance during launch, or to run a configuration script. To attach a file, select the \"As file\" option and browse for the file to attach.\n\n\n\n\n\n\nAWS S3\n\u00b6\n\n\nGUI tools to upload / manage files:\n\n\n\n\nAWS Console\n\n\nS3 Browser\n\n\nCloudBerry\n\n\n\n\nCommand-line s3 clients:\n\n\n\n\nAWS command line (see above)\n\n\nS3 command line tools\n\n\n\n\nRedshift\n\u00b6\n\n\n1) Use Case\n\n\n\n\nLarge-scale SQL analytical database\n\n\nQuerying in Redshift is FAST\n\n\nFull SQL compared to \nHiveQL\n\n\nRedshift isn\u2019t a complete replacement for a Hadoop system (no streaming, no text processing)\n\n\n\n\n2) Tools\n\n\n\n\ninstall SQL \ntool\n\n\nor \nAginity\n\n\nMicrosoft \nSSDT\n\n\n\n\n3) Get data into Redshift:\n\n\n\n\nCOPY from S3 (delimited text files)\n\n\nCOPY from DynamoDB (NoSQL datastore)\n\n\nJDBC/ODBC transactions (not efficient for bulk loading)\n\n\n\n\nTables have \u2018keys\u2019 that define how the data is split across slices. The recommended practice is to split based upon commonly-joined columns, so that joined data resides on the same slice, thus avoiding the need to move data between systems.\n\n\n4) Examples:\n\n\nCOPY\n \ntable1\n \nFROM\n \n's3://bucket1/'\n \ncredentials\n \n'aws_access_key_id=abc;aws_secret_access_key=xyz'\n \ndelimiter\n \n'|'\n \ngzip\n \nremovequotes\n \ntruncatecolumns\n \nmaxerror\n \n1000\n\n\nSELECT\n \nDISTINCT\n \nfield1\n \nFROM\n \ntable1\n\n\nSELECT\n \nCOUNT\n(\nDISTINCT\n \nfield2\n)\n \nFROM\n \ntable1\n\n\n\n\n\n\nEMR\n\u00b6\n\n\n\n\nEMR FAQs\n\n\nExtract, Transform, and Load (ETL) Data with Amazon EMR\n\n\nEMR article\n\n\n\n\nSWF\n\u00b6\n\n\nThe Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components. In Amazon SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing intertask dependencies, scheduling, and concurrency in accordance with the logical flow of the application. Amazon SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state.\n\n\nWhen using Amazon SWF, you implement workers to perform tasks. These workers can run either on cloud infrastructure, such as Amazon Elastic Compute Cloud (Amazon EC2), or on your own premises. You can create tasks that are long-running, or that may fail, time out, or require restarts\u2014or that may complete with varying throughput and latency. Amazon SWF stores tasks and assigns them to workers when they are ready, tracks their progress, and maintains their state, including details on their completion. To coordinate tasks, you write a program that gets the latest state of each task from Amazon SWF and uses it to initiate subsequent tasks. Amazon SWF maintains an application's execution state durably so that the application is resilient to failures in individual components. With Amazon SWF, you can implement, deploy, scale, and modify these application components independently.\n\n\nAmazon SWF offers capabilities to support a variety of application requirements. It is suitable for a range of use cases that require coordination of tasks, including media processing, web application back-ends, business process workflows, and analytics pipelines.",
            "title": "AWS Services Overview"
        },
        {
            "location": "/Cloud/AWS/#aws-services-overview",
            "text": "",
            "title": "AWS Services Overview"
        },
        {
            "location": "/Cloud/AWS/#basic-services",
            "text": "Compute: EC2 (autoscaling, ELB load balancing)  Networking / Security: VPC (security groups), IAM (users/groups/application roles)",
            "title": "Basic Services"
        },
        {
            "location": "/Cloud/AWS/#storage",
            "text": "S3: secure, scalable object-level storage, static web site hosting...  Glacier: long-term storage  EBS: block-level storage (for EC2 instances)",
            "title": "Storage"
        },
        {
            "location": "/Cloud/AWS/#databases",
            "text": "RDS: relational databases (MySQL, PostgreSQL, MSSQL, MariaDB, Aurora...)  DynamoDB: scalable NoSQL database backed by solid-state drives",
            "title": "Databases"
        },
        {
            "location": "/Cloud/AWS/#analytics",
            "text": "RedShift: PostgreSQL-based columnstore OLAP database that uses SQL. MPP architecture.  EMR: Hadoop cluster (Hive, Pig, HBase, Spark...).",
            "title": "Analytics"
        },
        {
            "location": "/Cloud/AWS/#etl-elt-batch-processing",
            "text": "Glue  Batch  Data Pipeline: orchestrate data transfers between S3, DynamoDB, Redshift",
            "title": "ETL / ELT / Batch Processing"
        },
        {
            "location": "/Cloud/AWS/#application-services",
            "text": "Notifications: SNS (alerts by email, SMS...), SES (bulk email)  Queue: SQS (async message queues for component decoupling)   Workflows, State Machine as a Service: AWS Step Functions, SWF (task-oriented workflows - complicated)  Document Search: ElasticSearch, CloudSearch",
            "title": "Application Services"
        },
        {
            "location": "/Cloud/AWS/#monitoring",
            "text": "Cloudwatch (monitor services and instances e.g. CPU utilization, etc...)  CloudTrail (monitor API calls)",
            "title": "Monitoring"
        },
        {
            "location": "/Cloud/AWS/#infrastructure-deployment-automation",
            "text": "Elastic Beanstalk (simple, mostly web or Linux worker)  CloudFormation (JSON / YAML templates - more difficult, but many existing templates)  OpsWork (higher level than CloudFormation, uses non-native components - Chef-based)",
            "title": "Infrastructure Deployment / Automation"
        },
        {
            "location": "/Cloud/AWS/#desktop-in-the-cloud",
            "text": "WorkSpaces",
            "title": "Desktop in the Cloud"
        },
        {
            "location": "/Cloud/AWS/#details",
            "text": "",
            "title": "Details"
        },
        {
            "location": "/Cloud/AWS/#tools",
            "text": "Unix tools on Windows:  Cygwin    Putty SSH client for Windows  doc    Download and install PuTTY  link . Be sure to install the entire suite.   Start PuTTYgen (for example, from the Start menu, click All Programs > PuTTY > PuTTYgen).  Under Type of key to generate, select SSH-2 RSA.  Load the .pem file (private key) downloaded from the console (in \"credentials\" folder)    Save private key    AWS  command line interface    AWS toolkit for Visual Studio    AWS tools for PowerShell",
            "title": "Tools"
        },
        {
            "location": "/Cloud/AWS/#aws-ec2",
            "text": "Log onto instance with  Putty SSH   login as: ec2-user (Amazon Linux) or: ubuntu  Bash shell documentation    Use a shell script to configure the instance  link    User data: You can specify user data to configure an instance during launch, or to run a configuration script. To attach a file, select the \"As file\" option and browse for the file to attach.",
            "title": "AWS EC2"
        },
        {
            "location": "/Cloud/AWS/#aws-s3",
            "text": "GUI tools to upload / manage files:   AWS Console  S3 Browser  CloudBerry   Command-line s3 clients:   AWS command line (see above)  S3 command line tools",
            "title": "AWS S3"
        },
        {
            "location": "/Cloud/AWS/#redshift",
            "text": "1) Use Case   Large-scale SQL analytical database  Querying in Redshift is FAST  Full SQL compared to  HiveQL  Redshift isn\u2019t a complete replacement for a Hadoop system (no streaming, no text processing)   2) Tools   install SQL  tool  or  Aginity  Microsoft  SSDT   3) Get data into Redshift:   COPY from S3 (delimited text files)  COPY from DynamoDB (NoSQL datastore)  JDBC/ODBC transactions (not efficient for bulk loading)   Tables have \u2018keys\u2019 that define how the data is split across slices. The recommended practice is to split based upon commonly-joined columns, so that joined data resides on the same slice, thus avoiding the need to move data between systems.  4) Examples:  COPY   table1   FROM   's3://bucket1/'   credentials   'aws_access_key_id=abc;aws_secret_access_key=xyz'   delimiter   '|'   gzip   removequotes   truncatecolumns   maxerror   1000  SELECT   DISTINCT   field1   FROM   table1  SELECT   COUNT ( DISTINCT   field2 )   FROM   table1",
            "title": "Redshift"
        },
        {
            "location": "/Cloud/AWS/#emr",
            "text": "EMR FAQs  Extract, Transform, and Load (ETL) Data with Amazon EMR  EMR article",
            "title": "EMR"
        },
        {
            "location": "/Cloud/AWS/#swf",
            "text": "The Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components. In Amazon SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing intertask dependencies, scheduling, and concurrency in accordance with the logical flow of the application. Amazon SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state.  When using Amazon SWF, you implement workers to perform tasks. These workers can run either on cloud infrastructure, such as Amazon Elastic Compute Cloud (Amazon EC2), or on your own premises. You can create tasks that are long-running, or that may fail, time out, or require restarts\u2014or that may complete with varying throughput and latency. Amazon SWF stores tasks and assigns them to workers when they are ready, tracks their progress, and maintains their state, including details on their completion. To coordinate tasks, you write a program that gets the latest state of each task from Amazon SWF and uses it to initiate subsequent tasks. Amazon SWF maintains an application's execution state durably so that the application is resilient to failures in individual components. With Amazon SWF, you can implement, deploy, scale, and modify these application components independently.  Amazon SWF offers capabilities to support a variety of application requirements. It is suitable for a range of use cases that require coordination of tasks, including media processing, web application back-ends, business process workflows, and analytics pipelines.",
            "title": "SWF"
        },
        {
            "location": "/Cloud/AWS_Lambda/",
            "text": "Useful Links\n\u00b6\n\n\nBuilding a Dynamic DNS for Route 53 using CloudWatch Events and Lambda\n\n\nLambkin\n - CLI tool for generating and managing simple functions in AWS Lambda",
            "title": "AWS Lambda"
        },
        {
            "location": "/Cloud/AWS_Lambda/#useful-links",
            "text": "Building a Dynamic DNS for Route 53 using CloudWatch Events and Lambda  Lambkin  - CLI tool for generating and managing simple functions in AWS Lambda",
            "title": "Useful Links"
        },
        {
            "location": "/Cloud/Serverless/",
            "text": "Serverless Cheatsheet\n\u00b6\n\n\nServerless home page\n\n\nInstall\n\u00b6\n\n\nnpm install -g serverless\n\n\n\n\n\nExamples\n\u00b6\n\n\nServerless Examples\n\n\nServerless Starter\n\n\nPython example\n\n\nC# example\n\n\nCheatsheet\n\u00b6\n\n\n\n\nCreate a Service:\n\n\n\n\n# NodeJS\n\nserverless create -p \n[\nSERVICE NAME\n]\n -t aws-nodejs\n\n\n# C#\n\nserverless create --path serverlessCSharp --template aws-csharp\n\n\n\n\n\n\n\nInstall a Service\n\n\n\n\nThis is a convenience method to install a pre-made Serverless Service locally by downloading the Github repo and unzipping it.\n\n\nserverless install -u \n[\nGITHUB URL OF SERVICE\n]\n\n\n\n\n\n\n\n\nDeploy All\n\n\n\n\nUse this when you have made changes to your Functions, Events or Resources in \nserverless.yml\n or you simply want to deploy all changes within your Service at the same time.\n\n\nserverless deploy -s \n[\nSTAGE NAME\n]\n -r \n[\nREGION NAME\n]\n -v\n\n\n\n\n\n\n\nDeploy Function\n\n\n\n\nUse this to quickly overwrite your AWS Lambda code on AWS, allowing you to develop faster.\n\n\nserverless deploy \nfunction\n -f \n[\nFUNCTION NAME\n]\n -s \n[\nSTAGE NAME\n]\n -r \n[\nREGION NAME\n]\n\n\n\n\n\n\n\n\nInvoke Function\n\n\n\n\nInvokes an AWS Lambda Function on AWS and returns logs.\n\n\nserverless invoke -f \n[\nFUNCTION NAME\n]\n -s \n[\nSTAGE NAME\n]\n -r \n[\nREGION NAME\n]\n -l\n\n\n\n\n\n\n\nStreaming Logs\n\n\n\n\nOpen up a separate tab in your console and stream all logs for a specific Function using this command.\n\n\nserverless logs -f \n[\nFUNCTION NAME\n]\n -s \n[\nSTAGE NAME\n]\n -r \n[\nREGION NAME\n]",
            "title": "Serverless Cheatsheet"
        },
        {
            "location": "/Cloud/Serverless/#serverless-cheatsheet",
            "text": "Serverless home page",
            "title": "Serverless Cheatsheet"
        },
        {
            "location": "/Cloud/Serverless/#install",
            "text": "npm install -g serverless",
            "title": "Install"
        },
        {
            "location": "/Cloud/Serverless/#examples",
            "text": "Serverless Examples  Serverless Starter  Python example  C# example",
            "title": "Examples"
        },
        {
            "location": "/Cloud/Serverless/#cheatsheet",
            "text": "Create a Service:   # NodeJS \nserverless create -p  [ SERVICE NAME ]  -t aws-nodejs # C# \nserverless create --path serverlessCSharp --template aws-csharp   Install a Service   This is a convenience method to install a pre-made Serverless Service locally by downloading the Github repo and unzipping it.  serverless install -u  [ GITHUB URL OF SERVICE ]    Deploy All   Use this when you have made changes to your Functions, Events or Resources in  serverless.yml  or you simply want to deploy all changes within your Service at the same time.  serverless deploy -s  [ STAGE NAME ]  -r  [ REGION NAME ]  -v   Deploy Function   Use this to quickly overwrite your AWS Lambda code on AWS, allowing you to develop faster.  serverless deploy  function  -f  [ FUNCTION NAME ]  -s  [ STAGE NAME ]  -r  [ REGION NAME ]    Invoke Function   Invokes an AWS Lambda Function on AWS and returns logs.  serverless invoke -f  [ FUNCTION NAME ]  -s  [ STAGE NAME ]  -r  [ REGION NAME ]  -l   Streaming Logs   Open up a separate tab in your console and stream all logs for a specific Function using this command.  serverless logs -f  [ FUNCTION NAME ]  -s  [ STAGE NAME ]  -r  [ REGION NAME ]",
            "title": "Cheatsheet"
        },
        {
            "location": "/Data_Science/Data_Manipulation/",
            "text": "Pandas\n\u00b6\n\n\n\n\nPandas cheatsheet\n\n\nPandas cheatsheet 2\n\n\n\n\nData Wrangling with .NET\n\u00b6\n\n\nQuora: Which is the best machine learning library for .NET?\n\n\nDeedle- Exploratory data library for .NET\n\n\nDeedle is an easy to use library for data and time series manipulation and for scientific programming. It supports working with structured data frames, ordered and unordered data, as well as time series. Deedle is designed to work well for exploratory programming using F# and C# interactive console, but can be also used in efficient compiled .NET code.\n\n\nThe library implements a wide range of operations for data manipulation including advanced indexing and slicing, joining and aligning data, handling of missing values, grouping and aggregation, statistics and more.\n\n\nAccord.NET Framework\n\n\nAccord.NET provides statistical analysis, machine learning, image processing and computer vision methods for .NET applications. The Accord.NET Framework extends the popular AForge.NET with new features, adding to a more complete environment for scientific computing in .NET.",
            "title": "Data Manipulation"
        },
        {
            "location": "/Data_Science/Data_Manipulation/#pandas",
            "text": "Pandas cheatsheet  Pandas cheatsheet 2",
            "title": "Pandas"
        },
        {
            "location": "/Data_Science/Data_Manipulation/#data-wrangling-with-net",
            "text": "Quora: Which is the best machine learning library for .NET?  Deedle- Exploratory data library for .NET  Deedle is an easy to use library for data and time series manipulation and for scientific programming. It supports working with structured data frames, ordered and unordered data, as well as time series. Deedle is designed to work well for exploratory programming using F# and C# interactive console, but can be also used in efficient compiled .NET code.  The library implements a wide range of operations for data manipulation including advanced indexing and slicing, joining and aligning data, handling of missing values, grouping and aggregation, statistics and more.  Accord.NET Framework  Accord.NET provides statistical analysis, machine learning, image processing and computer vision methods for .NET applications. The Accord.NET Framework extends the popular AForge.NET with new features, adding to a more complete environment for scientific computing in .NET.",
            "title": "Data Wrangling with .NET"
        },
        {
            "location": "/Data_Science/Data_Visualization/",
            "text": "Basics\n\u00b6\n\n\nData visualization - Wikipedia\n\n\n19 Tools for Data Visualization Projects\n\n\n22 free tools for data visualization and analysis - Computerworld\n\n\n22 free tools for data visualization and analysis\n\n\nJavaScript libraries / APIs\n\u00b6\n\n\nD3.js - Data-Driven Documents\n\n\nD3 provides many built-in reusable functions and function factories, such as graphical primitives for area, line and pie charts.\n\n\nD3 building blocks\n\n\nC3.js\n\n\nGoogle Charts\n\n\nTools\n\u00b6\n\n\nplot.ly\n\n\nTableau\n\n\nQlik\n\n\nQuadrigram",
            "title": "Data Visualization"
        },
        {
            "location": "/Data_Science/Data_Visualization/#basics",
            "text": "Data visualization - Wikipedia  19 Tools for Data Visualization Projects  22 free tools for data visualization and analysis - Computerworld  22 free tools for data visualization and analysis",
            "title": "Basics"
        },
        {
            "location": "/Data_Science/Data_Visualization/#javascript-libraries-apis",
            "text": "D3.js - Data-Driven Documents  D3 provides many built-in reusable functions and function factories, such as graphical primitives for area, line and pie charts.  D3 building blocks  C3.js  Google Charts",
            "title": "JavaScript libraries / APIs"
        },
        {
            "location": "/Data_Science/Data_Visualization/#tools",
            "text": "plot.ly  Tableau  Qlik  Quadrigram",
            "title": "Tools"
        },
        {
            "location": "/Data_Science/Machine_Learning/",
            "text": "Useful Links\n\u00b6\n\n\nDataTau\n\n\nDatasets\n\u00b6\n\n\n\n\nUC Irvine Machine Learning Repository\n\n\nQuandl\n\n\nKaggle\n\n\n\n\nAlgorithms\n\u00b6\n\n\nRestricted Boltzmann Machines\n\u00b6\n\n\nA Beginner's Tutorial for Restricted Boltzmann Machines - Deeplearning4j- Open-source, Distributed Deep Learning for the JVM\n\n\nEmbedding\n\u00b6\n\n\nt-distributed stochastic neighbor embedding - Wikipedia\n\n\nReinforcement Learning\n\u00b6\n\n\nLecture 10 Reinforcement Learning I\n\n\nPyBrain\n\n\nPyBrain - a simple neural networks library in Python\n\n\nCyBrain\n\n\nML Plaforms\n\u00b6\n\n\nPalladium\n\n\nGUI tools\n\u00b6\n\n\n\n\nOrange\n\n\nProvides a design tool for visual programming allowing you to connect together data preparation, algorithms, and result evaluation\ntogether to create machine learning \u201cprograms\u201d. Provides over 100 widgets for the environment and also provides a Python API and library for\nintegrating into your application.\n\n\n\n\n\n\nWeka explorer\n\n\nA graphical machine learning workbench. It provides an explorer that you can use to prepare data, run algorithms and review results. It\nalso provides an experimenter where you can perform the same tasks in a controlled environment and design a batch of algorithm runs that could\nrun for an extended period of time and then review the results. Finally, it also provides a data flow interface where you can plug algorithms\ntogether like a flow diagram. Under the covers you can use Weka as a Java library and write programs that make use of the algorithms.\n\n\n\n\n\n\nBigML\n\n\nA web service where you can upload your data, prepare it and run algorithms on it. It provides clean and easy to use interfaces for\nconfiguring algorithms (decision trees) and reviewing the results. The best feature of this service is that it is all in the cloud, meaning that\nall you need is a web browser to get started. It also provides an API so that if you like it you can build an application around it.\n\n\n\n\n\n\n\n\nTutorials\n\u00b6\n\n\n\n\nmachinelearningmastery.com\n\n\nKaggle\n\n\n\n\nBooks\n\u00b6\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\n\nBoosting Foundations and Algorithms",
            "title": "Machine Learning"
        },
        {
            "location": "/Data_Science/Machine_Learning/#useful-links",
            "text": "DataTau",
            "title": "Useful Links"
        },
        {
            "location": "/Data_Science/Machine_Learning/#datasets",
            "text": "UC Irvine Machine Learning Repository  Quandl  Kaggle",
            "title": "Datasets"
        },
        {
            "location": "/Data_Science/Machine_Learning/#algorithms",
            "text": "",
            "title": "Algorithms"
        },
        {
            "location": "/Data_Science/Machine_Learning/#restricted-boltzmann-machines",
            "text": "A Beginner's Tutorial for Restricted Boltzmann Machines - Deeplearning4j- Open-source, Distributed Deep Learning for the JVM",
            "title": "Restricted Boltzmann Machines"
        },
        {
            "location": "/Data_Science/Machine_Learning/#embedding",
            "text": "t-distributed stochastic neighbor embedding - Wikipedia",
            "title": "Embedding"
        },
        {
            "location": "/Data_Science/Machine_Learning/#reinforcement-learning",
            "text": "Lecture 10 Reinforcement Learning I  PyBrain  PyBrain - a simple neural networks library in Python  CyBrain",
            "title": "Reinforcement Learning"
        },
        {
            "location": "/Data_Science/Machine_Learning/#ml-plaforms",
            "text": "Palladium",
            "title": "ML Plaforms"
        },
        {
            "location": "/Data_Science/Machine_Learning/#gui-tools",
            "text": "Orange  Provides a design tool for visual programming allowing you to connect together data preparation, algorithms, and result evaluation\ntogether to create machine learning \u201cprograms\u201d. Provides over 100 widgets for the environment and also provides a Python API and library for\nintegrating into your application.    Weka explorer  A graphical machine learning workbench. It provides an explorer that you can use to prepare data, run algorithms and review results. It\nalso provides an experimenter where you can perform the same tasks in a controlled environment and design a batch of algorithm runs that could\nrun for an extended period of time and then review the results. Finally, it also provides a data flow interface where you can plug algorithms\ntogether like a flow diagram. Under the covers you can use Weka as a Java library and write programs that make use of the algorithms.    BigML  A web service where you can upload your data, prepare it and run algorithms on it. It provides clean and easy to use interfaces for\nconfiguring algorithms (decision trees) and reviewing the results. The best feature of this service is that it is all in the cloud, meaning that\nall you need is a web browser to get started. It also provides an API so that if you like it you can build an application around it.",
            "title": "GUI tools"
        },
        {
            "location": "/Data_Science/Machine_Learning/#tutorials",
            "text": "machinelearningmastery.com  Kaggle",
            "title": "Tutorials"
        },
        {
            "location": "/Data_Science/Machine_Learning/#books",
            "text": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction  Boosting Foundations and Algorithms",
            "title": "Books"
        },
        {
            "location": "/Databases/Mongodb/",
            "text": "Import from CSV\n\u00b6\n\n\nmongoimport --db users --collection contacts --type csv --headerline --file contacts.csv\n\n\n\n\n\nSpecifying \n--headerline\n instructs mongoimport to determine the name of the fields using the first line in the CSV file.\nUse the \n--ignoreBlanks\n option to ignore blank fields. For CSV and TSV imports, this option provides the desired functionality in most cases, because it avoids inserting fields with null values into your collection.\n\n\nMongoImport documentation\n\n\nPrint from a Cursor\n\u00b6\n\n\nmyCursor\n.\nforEach\n(\nprintjson\n);\n\n\n\n// or\n\n\nwhile\n \n(\nmyCollection\n.\nhasNext\n())\n \n{\n\n   \nprintjson\n(\nmyCollection\n.\nnext\n());\n\n\n}\n \n\n\n\n\n\nAggregation Tips\n\u00b6\n\n\n// lowercase a string \n\n\n{\n \n$project\n:\n \n{\n \n\"address\"\n:\n \n{\n \n$toLower\n:\n \n\"$address\"\n \n}\n \n}\n \n},\n\n\n\n// extract field within embedded document\n\n\n{\n \n$project\n:\n \n{\n \n\"experience.location\"\n:\n \n1\n \n}\n \n},\n\n\n\n// flatten \n\n\n{\n \n$unwind\n:\n \n\"$experience\"\n},\n\n\n{\n \n$group\n:\n \n{\n \n_id\n:\n \n\"$_id\"\n,\n \nlocs\n:\n \n{\n \n$push\n:\n \n{\n \n$ifNull\n:\n \n[\n \n\"$experience.location\"\n,\n \n\"undefined\"\n \n]\n \n}\n \n}\n \n}\n \n}\n\n\n\n// output a collection\n\n\n{\n \n$out\n:\n \n\"myCollection2\"\n \n}\n\n\n\n// get unique values \n\n\n{\n \n$group\n:\n \n{\n \n_id\n:\n \n\"$fulladdress\"\n \n}\n \n}\n\n\n\n\n\n\nMake a Copy\n\u00b6\n\n\nDon't use copyTo - it is fully blocking... and deprecated in 3.x\n\n\n\n\nUse the Aggregation framework:\n\n\n\n\ndb\n \n=\n \ndb\n.\ngetSiblingDB\n(\n\"myDB\"\n);\n \n// set current db for $out\n\n\nvar\n \nmyCollection\n \n=\n \ndb\n.\ngetCollection\n(\n\"myCollection\"\n);\n\n\n\n// project if needed, get uniques if needed, create a new collection\n\n\nmyCollection\n.\naggregate\n([{\n \n$project\n:\n{\n \n\"fulladdress\"\n:\n \n1\n \n}\n \n},{\n \n$group\n:\n{\n \n_id\n:\n \n\"$fulladdress\"\n \n}\n \n},{\n \n$out\n:\n \n\"outputCollection\"\n \n}],\n \n{\n \nallowDiskUse\n:\ntrue\n \n});\n \n\n\n\n\n\n\n\nOr use bulk update:\n\n\n\n\nvar\n \noutputColl\n \n=\n \ndb\n.\ngetCollection\n(\n \n\"outputCollection\"\n \n);\n\n\nvar\n \noutputBulk\n \n=\n \noutputColl\n.\ninitializeUnorderedBulkOp\n();\n\n\nmyCollection\n.\nfind\n(\n \n{},\n \n{\n \n\"fulladdress\"\n:\n \n1\n \n}\n \n).\nforEach\n(\n \nfunction\n(\ndoc\n)\n \n{\n\n     \noutputBulk\n.\ninsert\n(\ndoc\n);\n\n\n});\n\n\noutputBulk\n.\nexecute\n();\n\n\n\n\n\n\nLonger Example\n\u00b6\n\n\nAdd a count field to all records\n\n\nfunction\n \ngatherStats\n()\n \n{\n   \n    \nvar\n \nstart\n \n=\n \nDate\n.\nnow\n();\n\n\n    \nvar\n \ninputDB\n \n=\n \ndb\n.\ngetSiblingDB\n(\n\"inputDB\"\n);\n\n    \nvar\n \ninputColl\n \n=\n \ninputDB\n.\ngetCollection\n(\n\"inputColl\"\n);\n\n\n    \n// debug: inputColl.find( {} ).limit(2).forEach(printjson);  \n\n\n    \noutputDB\n \n=\n \ndb\n.\ngetSiblingDB\n(\n\"outputDB\"\n);\n \n    \ndb\n \n=\n \noutputDB\n;\n \n// set current database for the next aggregate step\n\n\n    \n// create temporary collection with count\n\n    \ninputColl\n.\naggregate\n(\n  \n[\n \n    \n{\n \n$group\n:\n \n{\n \n_id\n:\n \n{\n \n$toLower\n:\n \n\"$address\"\n \n},\n \ncount\n:\n \n{\n \n$sum\n:\n \n1\n \n}\n \n}\n \n},\n \n    \n{\n \n$sort\n:\n \n{\n \n\"count\"\n:\n \n-\n1\n \n}\n \n},\n\n    \n{\n \n$limit\n:\n \n100000\n \n},\n                 \n// limit to 100k addresses with highest count  \n\n    \n{\n \n$out\n:\n \n\"stats\"\n \n}\n\n    \n],\n  \n{\n \nallowDiskUse\n:\n \ntrue\n \n}\n \n);\n       \n// returns { _id, count } where _id is the address\n\n\n    \nvar\n \nstatsColl\n \n=\n \noutputDB\n.\ngetCollection\n(\n\"stats\"\n);\n\n\n   \n// create output collection\n\n    \nvar\n \noutputColl\n \n=\n \noutputDB\n.\ngetCollection\n(\n\"outputColl\"\n);\n \n    \nvar\n \noutputBulk\n \n=\n \noutputColl\n.\ninitializeUnorderedBulkOp\n();\n \n    \nvar\n \ncounter\n \n=\n \n0\n;\n \n\n    \nvar\n \ninputCursor\n \n=\n \ninputColl\n.\nfind\n(\n \n{},\n \n{}\n \n);\n \n    \ninputCursor\n.\nforEach\n(\n \nfunction\n(\ndoc\n)\n \n{\n \n        \nvar\n \nstatDoc\n \n=\n \nstatsColl\n.\nfindOne\n(\n \n{\n \n_id\n:\n \ndoc\n.\naddress\n \n}\n \n);\n\n        \nif\n \n(\nstatDoc\n)\n \n{\n\n            \ndoc\n.\ncount\n \n=\n \nstatDoc\n.\ncount\n;\n\n            \noutputBulk\n.\ninsert\n(\ndoc\n);\n\n            \ncounter\n++\n;\n  \n            \nif\n \n(\n \ncounter\n \n%\n \n1000\n \n==\n \n0\n \n)\n \n{\n\n                    \noutputBulk\n.\nexecute\n();\n\n                    \n// you have to reset\n\n                    \noutputBulk\n \n=\n \noutputColl\n.\ninitializeUnorderedBulkOp\n();\n \n                \n}\n\n            \n}\n\n        \n}\n\n    \n);\n\n\n    \nif\n \n(\n \ncounter\n \n%\n \n1000\n \n>\n \n0\n \n)\n\n        \noutputBulk\n.\nexecute\n();\n\n\n\n    \n// print the results\n\n    \noutputColl\n.\nfind\n({}).\nsort\n({\ncount\n:\n \n-\n1\n}).\nforEach\n(\nprintjson\n);\n \n\n    \nvar\n \nend\n \n=\n \nDate\n.\nnow\n();\n\n    \nvar\n \nduration\n \n=\n \n(\nend\n \n-\n \nstart\n)\n/\n1000\n;\n\n    \nprintjson\n(\n\"Duration: \"\n \n+\n \nduration\n \n+\n \n\" seconds\"\n);\n\n\n    \nprintjson\n(\n\" | DONE | \"\n);\n\n\n}\n\n\n\ngatherStats\n();\n\n\n\n\n\n\nAlternatively move data to memory:\n\n\n    \nvar\n \nstatsDict\n \n=\n \n{};\n \n// or better Object.create(null);    \n\n    \nstatsColl\n.\nfind\n({}).\nforEach\n(\n \nfunction\n(\ndoc\n)\n \n{\n \nstatsDict\n[\ndoc\n.\n_id\n]\n \n=\n \ndoc\n.\ncount\n \n}\n \n);\n\n\n    \n// could also use: var statsArray = statsCursor.toArray();\n\n\n    \ninputCursor\n.\nforEach\n(\n \nfunction\n(\ndoc\n)\n \n{\n\n        \nif\n \n(\ndoc\n.\naddress\n \nin\n \nstatsDict\n)\n\n        \n{\n \n            \ndoc\n[\n\"count\"\n]\n \n=\n \nstatsDict\n[\ndoc\n.\naddress\n];\n \n            \noutputBulk\n.\ninsert\n(\ndoc\n);\n \n        \n}\n\n    \n});\n\n    \noutputBulk\n.\nexecute\n();",
            "title": "MongoDB Cheatsheet"
        },
        {
            "location": "/Databases/Mongodb/#import-from-csv",
            "text": "mongoimport --db users --collection contacts --type csv --headerline --file contacts.csv  Specifying  --headerline  instructs mongoimport to determine the name of the fields using the first line in the CSV file.\nUse the  --ignoreBlanks  option to ignore blank fields. For CSV and TSV imports, this option provides the desired functionality in most cases, because it avoids inserting fields with null values into your collection.  MongoImport documentation",
            "title": "Import from CSV"
        },
        {
            "location": "/Databases/Mongodb/#print-from-a-cursor",
            "text": "myCursor . forEach ( printjson );  // or  while   ( myCollection . hasNext ())   { \n    printjson ( myCollection . next ());  }",
            "title": "Print from a Cursor"
        },
        {
            "location": "/Databases/Mongodb/#aggregation-tips",
            "text": "// lowercase a string   {   $project :   {   \"address\" :   {   $toLower :   \"$address\"   }   }   },  // extract field within embedded document  {   $project :   {   \"experience.location\" :   1   }   },  // flatten   {   $unwind :   \"$experience\" },  {   $group :   {   _id :   \"$_id\" ,   locs :   {   $push :   {   $ifNull :   [   \"$experience.location\" ,   \"undefined\"   ]   }   }   }   }  // output a collection  {   $out :   \"myCollection2\"   }  // get unique values   {   $group :   {   _id :   \"$fulladdress\"   }   }",
            "title": "Aggregation Tips"
        },
        {
            "location": "/Databases/Mongodb/#make-a-copy",
            "text": "Don't use copyTo - it is fully blocking... and deprecated in 3.x   Use the Aggregation framework:   db   =   db . getSiblingDB ( \"myDB\" );   // set current db for $out  var   myCollection   =   db . getCollection ( \"myCollection\" );  // project if needed, get uniques if needed, create a new collection  myCollection . aggregate ([{   $project : {   \"fulladdress\" :   1   }   },{   $group : {   _id :   \"$fulladdress\"   }   },{   $out :   \"outputCollection\"   }],   {   allowDiskUse : true   });     Or use bulk update:   var   outputColl   =   db . getCollection (   \"outputCollection\"   );  var   outputBulk   =   outputColl . initializeUnorderedBulkOp ();  myCollection . find (   {},   {   \"fulladdress\" :   1   }   ). forEach (   function ( doc )   { \n      outputBulk . insert ( doc );  });  outputBulk . execute ();",
            "title": "Make a Copy"
        },
        {
            "location": "/Databases/Mongodb/#longer-example",
            "text": "Add a count field to all records  function   gatherStats ()   {    \n     var   start   =   Date . now (); \n\n     var   inputDB   =   db . getSiblingDB ( \"inputDB\" ); \n     var   inputColl   =   inputDB . getCollection ( \"inputColl\" ); \n\n     // debug: inputColl.find( {} ).limit(2).forEach(printjson);   \n\n     outputDB   =   db . getSiblingDB ( \"outputDB\" );  \n     db   =   outputDB ;   // set current database for the next aggregate step \n\n     // create temporary collection with count \n     inputColl . aggregate (    [  \n     {   $group :   {   _id :   {   $toLower :   \"$address\"   },   count :   {   $sum :   1   }   }   },  \n     {   $sort :   {   \"count\" :   - 1   }   }, \n     {   $limit :   100000   },                   // limit to 100k addresses with highest count   \n     {   $out :   \"stats\"   } \n     ],    {   allowDiskUse :   true   }   );         // returns { _id, count } where _id is the address \n\n     var   statsColl   =   outputDB . getCollection ( \"stats\" ); \n\n    // create output collection \n     var   outputColl   =   outputDB . getCollection ( \"outputColl\" );  \n     var   outputBulk   =   outputColl . initializeUnorderedBulkOp ();  \n     var   counter   =   0 ;  \n\n     var   inputCursor   =   inputColl . find (   {},   {}   );  \n     inputCursor . forEach (   function ( doc )   {  \n         var   statDoc   =   statsColl . findOne (   {   _id :   doc . address   }   ); \n         if   ( statDoc )   { \n             doc . count   =   statDoc . count ; \n             outputBulk . insert ( doc ); \n             counter ++ ;   \n             if   (   counter   %   1000   ==   0   )   { \n                     outputBulk . execute (); \n                     // you have to reset \n                     outputBulk   =   outputColl . initializeUnorderedBulkOp ();  \n                 } \n             } \n         } \n     ); \n\n     if   (   counter   %   1000   >   0   ) \n         outputBulk . execute (); \n\n\n     // print the results \n     outputColl . find ({}). sort ({ count :   - 1 }). forEach ( printjson );  \n\n     var   end   =   Date . now (); \n     var   duration   =   ( end   -   start ) / 1000 ; \n     printjson ( \"Duration: \"   +   duration   +   \" seconds\" ); \n\n     printjson ( \" | DONE | \" );  }  gatherStats ();   Alternatively move data to memory:       var   statsDict   =   {};   // or better Object.create(null);     \n     statsColl . find ({}). forEach (   function ( doc )   {   statsDict [ doc . _id ]   =   doc . count   }   ); \n\n     // could also use: var statsArray = statsCursor.toArray(); \n\n     inputCursor . forEach (   function ( doc )   { \n         if   ( doc . address   in   statsDict ) \n         {  \n             doc [ \"count\" ]   =   statsDict [ doc . address ];  \n             outputBulk . insert ( doc );  \n         } \n     }); \n     outputBulk . execute ();",
            "title": "Longer Example"
        },
        {
            "location": "/Databases/Redshift/",
            "text": "Redshift Best Practices\n\u00b6\n\n\n\n\nSmaller node types load data faster\n\n\nBest Practices for data load:\n\n\n1 file in S3 per slice (instances in RedShift)\n\n\nCompressed using gzip compression\n\n\nFile size: 1MB to 1GB compressed \n\n\nCOPY from S3 is the fastest\n\n\nCOPY from EMR HDFS may be faster, but most people don't use HDFS - they store data in S3\n\n\n\n\nFirst column of SORTKEY should not be compressed\n\n\n\n\n\n\nWorkflows: move from staging table to production table\n\n\n\n\nMake sure to wrap the entire workflow into ONE transaction\n\n\nCOMMITs are very expensive in RedShift\n\n\nDisable statistics on staging tables\n\n\nMake sure that the distribution keys match between staging and prod tables\n\n\n\n\nCompress your staging tables\n\n\n\n\n\n\nDo ANALYZE after VACUUM",
            "title": "RedShift"
        },
        {
            "location": "/Databases/Redshift/#redshift-best-practices",
            "text": "Smaller node types load data faster  Best Practices for data load:  1 file in S3 per slice (instances in RedShift)  Compressed using gzip compression  File size: 1MB to 1GB compressed   COPY from S3 is the fastest  COPY from EMR HDFS may be faster, but most people don't use HDFS - they store data in S3   First column of SORTKEY should not be compressed    Workflows: move from staging table to production table   Make sure to wrap the entire workflow into ONE transaction  COMMITs are very expensive in RedShift  Disable statistics on staging tables  Make sure that the distribution keys match between staging and prod tables   Compress your staging tables    Do ANALYZE after VACUUM",
            "title": "Redshift Best Practices"
        },
        {
            "location": "/Databases/SQL/",
            "text": "SQL Cheatsheet\n\u00b6\n\n\nDML: SELECT\n\u00b6\n\n\nFilter\n:\n\n\nSELECT\n \nLastName\n,\n \nFirstName\n,\n \nAddress\n \nFROM\n \nPersons\n\n\nWHERE\n \nAddress\n \nIS\n \nNULL\n\n\n\n\n\n\nLike\n:\n\n\nSELECT\n \n*\n \nFROM\n \nCustomers\n\n\nWHERE\n \nCity\n \nLIKE\n \n's%'\n;\n\n\n\nSELECT\n \n*\n \nFROM\n \nCustomers\n\n\nWHERE\n \nCountry\n \nLIKE\n \n'%land%'\n;\n\n\n\nSELECT\n \n*\n \nFROM\n \nCustomers\n\n\nWHERE\n \nCountry\n \nNOT\n \nLIKE\n \n'%land%'\n;\n\n\n\n\n\n\nSort\n:\n\n\nSELECT\n \n*\n \nFROM\n \nCustomers\n\n\nORDER\n \nBY\n \nCountry\n \nDESC\n;\n\n\n\nSELECT\n \n*\n \nFROM\n \nCustomers\n\n\nORDER\n \nBY\n \nCountry\n,\n \nCustomerName\n;\n\n\n\n\n\n\nLimit\n:\n\n\nSELECT\n \nTOP\n \nnumber\n|\npercent\n \ncolumn_name\n(\ns\n)\n\n\nFROM\n \ntable_name\n;\n\n\n\n-- Examples:\n\n\nSELECT\n \nTOP\n \n2\n \n*\n \nFROM\n \nCustomers\n;\n\n\n\nSELECT\n \nTOP\n \n50\n \nPERCENT\n \n*\n \nFROM\n \nCustomers\n;\n\n\n\n\n\n\nOracle Syntax\n:\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n\n\nFROM\n \ntable_name\n\n\nWHERE\n \nROWNUM\n \n<=\n \nnumber\n;\n\n\n\n\n\n\nJoins\n:\n\n\nSELECT\n \nCustomers\n.\nCustomerName\n,\n \nOrders\n.\nOrderID\n\n\nFROM\n \nCustomers\n\n\nFULL\n \nOUTER\n \nJOIN\n \nOrders\n\n\nON\n \nCustomers\n.\nCustomerID\n \n=\n \nOrders\n.\nCustomerID\n\n\nORDER\n \nBY\n \nCustomers\n.\nCustomerName\n;\n\n\n\n\n\n\nUnion\n:\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable1\n\n\nUNION\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable2\n;\n\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable1\n\n\nUNION\n \nALL\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable2\n;\n\n\n\n\n\n\nSelect Into\n:\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n\n\nINTO\n \nnewtable\n \n[\nIN\n \nexternaldb\n]\n\n\nFROM\n \ntable1\n;\n\n\n\n\n\n\nFormula\n:\n\n\nSELECT\n \nProductName\n,\n \nUnitPrice\n*\n(\nUnitsInStock\n+\nISNULL\n(\nUnitsOnOrder\n,\n0\n))\n\n\nFROM\n \nProducts\n\n\n\n\n\n\nDML: INSERT\n\u00b6\n\n\nINSERT\n \nINTO\n \ntable_name\n\n\nVALUES\n \n(\nvalue1\n,\nvalue2\n,\nvalue3\n,...);\n\n\n\nINSERT\n \nINTO\n \ntable_name\n \n(\ncolumn1\n,\ncolumn2\n,\ncolumn3\n,...)\n\n\nVALUES\n \n(\nvalue1\n,\nvalue2\n,\nvalue3\n,...);\n\n\n\n-- Example:\n\n\n\nINSERT\n \nINTO\n \nCustomers\n \n(\nCustomerName\n,\n \nCity\n,\n \nCountry\n)\n\n\nVALUES\n \n(\n'Cardinal'\n,\n \n'Stavanger'\n,\n \n'Norway'\n);\n\n\n\n\n\n\nInsert from select\n:\n\n\nINSERT\n \nINTO\n \ntable2\n(\ncolumn_name\n(\ns\n))\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n\n\nFROM\n \ntable1\n;\n\n\n\n-- Example:\n\n\n\nINSERT\n \nINTO\n \nCustomers\n \n(\nCustomerName\n,\n \nCountry\n)\n\n\nSELECT\n \nSupplierName\n,\n \nCountry\n \nFROM\n \nSuppliers\n\n\nWHERE\n \nCountry\n=\n'Germany'\n;\n\n\n\n\n\n\nDML: UPDATE\n\u00b6\n\n\nUPDATE\n \ntable_name\n\n\nSET\n \ncolumn1\n=\nvalue1\n,\ncolumn2\n=\nvalue2\n,...\n\n\nWHERE\n \nsome_column\n=\nsome_value\n;\n\n\n\n-- Example:\n\n\n\nUPDATE\n \nCustomers\n\n\nSET\n \nContactName\n=\n'Alfred Schmidt'\n,\n \nCity\n=\n'Hamburg'\n\n\nWHERE\n \nCustomerName\n=\n'Alfreds Futterkiste'\n;\n\n\n\n\n\n\nDML: DELETE\n\u00b6\n\n\nDELETE\n \nFROM\n \ntable_name\n\n\nWHERE\n \nsome_column\n=\nsome_value\n;\n\n\n\nDELETE\n \nFROM\n \nCustomers\n\n\nWHERE\n \nCustomerName\n=\n'Alfreds Futterkiste'\n \nAND\n \nContactName\n=\n'Maria Anders'\n;\n\n\n\n\n\n\nDatabases\n\u00b6\n\n\nCREATE\n \nDATABASE\n \nmy_db\n;\n\n\n\nDROP\n \nDATABASE\n \nmy_db\n;\n\n\n\n\n\n\nTables\n\u00b6\n\n\nCreate\n:\n\n\nCREATE\n \nTABLE\n \ntable_name\n\n\n(\n\n\ncolumn_name1\n \ndata_type\n(\nsize\n),\n\n\ncolumn_name2\n \ndata_type\n(\nsize\n),\n\n\ncolumn_name3\n \ndata_type\n(\nsize\n),\n\n\n....\n\n\n);\n\n\n\nCREATE\n \nTABLE\n \ntable_name\n\n\n(\n\n\ncolumn_name1\n \ndata_type\n(\nsize\n)\n \nconstraint_name\n,\n\n\ncolumn_name2\n \ndata_type\n(\nsize\n)\n \nconstraint_name\n,\n\n\ncolumn_name3\n \ndata_type\n(\nsize\n)\n \nconstraint_name\n,\n\n\n....\n\n\n);\n\n\n\n\n\n\n-- Examples\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n \nUNIQUE\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n)\n\n\n)\n\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n),\n\n\nCONSTRAINT\n \nuc_PersonID\n \nUNIQUE\n \n(\nP_Id\n,\n \nLastName\n)\n\n\n)\n\n\n\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nADD\n \nCONSTRAINT\n \nuc_PersonID\n \nUNIQUE\n \n(\nP_Id\n,\nLastName\n)\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nDROP\n \nCONSTRAINT\n \nuc_PersonID\n\n\n\n\n\n\nTemporary Table\n:\n\n\nCREATE\n \nTABLE\n \n#\nMyTempTable\n \n(\ncola\n \nINT\n \nPRIMARY\n \nKEY\n);\n\n\nINSERT\n \nINTO\n \n#\nMyTempTable\n \nVALUES\n \n(\n1\n);\n\n\n\n\n\n\nDrop / Truncate\n:\n\n\nDROP\n \nTABLE\n \ntable_name\n\n\n\nTRUNCATE\n \nTABLE\n \ntable_name\n\n\n\n\n\n\nPRIMARY KEY constraint\n\u00b6\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n \nPRIMARY\n \nKEY\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n)\n\n\n)\n\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n),\n\n\nCONSTRAINT\n \nPK_PersonID\n \nPRIMARY\n \nKEY\n \n(\nP_Id\n,\nLastName\n)\n\n\n)\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nADD\n \nCONSTRAINT\n \nPK_PersonID\n \nPRIMARY\n \nKEY\n \n(\nP_Id\n,\nLastName\n)\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nDROP\n \nCONSTRAINT\n \nPK_PersonID\n\n\n\n\n\n\nFOREIGN KEY constraints\n\u00b6\n\n\nCREATE\n \nTABLE\n \nOrders\n\n\n(\n\n\nO_Id\n \nint\n \nNOT\n \nNULL\n \nPRIMARY\n \nKEY\n,\n\n\nOrderNo\n \nint\n \nNOT\n \nNULL\n,\n\n\nP_Id\n \nint\n \nFOREIGN\n \nKEY\n \nREFERENCES\n \nPersons\n(\nP_Id\n)\n\n\n)\n\n\n\nCREATE\n \nTABLE\n \nOrders\n\n\n(\n\n\nO_Id\n \nint\n \nNOT\n \nNULL\n,\n\n\nOrderNo\n \nint\n \nNOT\n \nNULL\n,\n\n\nP_Id\n \nint\n,\n\n\nPRIMARY\n \nKEY\n \n(\nO_Id\n),\n\n\nCONSTRAINT\n \nFK_PerOrders\n \nFOREIGN\n \nKEY\n \n(\nP_Id\n)\n\n\nREFERENCES\n \nPersons\n(\nP_Id\n)\n\n\n)\n\n\n\n\n\n\nALTER\n \nTABLE\n \nOrders\n\n\nADD\n \nFOREIGN\n \nKEY\n \n(\nP_Id\n)\n\n\nREFERENCES\n \nPersons\n(\nP_Id\n)\n\n\n\nALTER\n \nTABLE\n \nOrders\n\n\nADD\n \nCONSTRAINT\n \nfk_PerOrders\n\n\nFOREIGN\n \nKEY\n \n(\nP_Id\n)\n\n\nREFERENCES\n \nPersons\n(\nP_Id\n)\n\n\n\nALTER\n \nTABLE\n \nOrders\n\n\nDROP\n \nCONSTRAINT\n \nfk_PerOrders\n\n\n\n\n\n\nCHECK Constraints\n\u00b6\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n \nCHECK\n \n(\nP_Id\n>\n0\n),\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n)\n\n\n)\n\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n),\n\n\nCONSTRAINT\n \nchk_Person\n \nCHECK\n \n(\nP_Id\n>\n0\n \nAND\n \nCity\n=\n'Sandnes'\n)\n\n\n)\n\n\n\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nADD\n \nCONSTRAINT\n \nCHK_Person\n \nCHECK\n \n(\nP_Id\n>\n0\n \nAND\n \nCity\n=\n'Sandnes'\n)\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nDROP\n \nCONSTRAINT\n \nCHK_Person\n\n\n\n\n\n\nDEFAULT Constraints\n\u00b6\n\n\nCREATE\n \nTABLE\n \nOrders\n\n\n(\n\n\nO_Id\n \nint\n \nNOT\n \nNULL\n,\n\n\nOrderNo\n \nint\n \nNOT\n \nNULL\n,\n\n\nP_Id\n \nint\n,\n\n\nOrderDate\n \ndate\n \nDEFAULT\n \nGETDATE\n()\n\n\n)\n\n\n\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nALTER\n \nCOLUMN\n \nCity\n \nSET\n \nDEFAULT\n \n'SEATTLE'\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nALTER\n \nCOLUMN\n \nCity\n \nDROP\n \nDEFAULT\n\n\n\n\n\n\nIndex\n\u00b6\n\n\nCREATE\n \nUNIQUE\n \nINDEX\n \nindex_name\n\n\nON\n \ntable_name\n \n(\ncolumn_name\n)\n\n\n\nCREATE\n \nINDEX\n \nindex_name\n\n\nON\n \ntable_name\n \n(\ncolumn_name1\n,\n \ncol_name2\n)\n\n\n\n-- Example:\n\n\n\nCREATE\n \nINDEX\n \nPIndex\n\n\nON\n \nPersons\n \n(\nLastName\n,\n \nFirstName\n)\n\n\n\n\n\n\nDROP\n \nINDEX\n \ntable_name\n.\nindex_name\n\n\n\n-- Example:\n\n\n\nDROP\n \nINDEX\n \nIX_ProductVendor_BusinessEntityID\n\n    \nON\n \nPurchasing\n.\nProductVendor\n;\n\n\n\n\n\n\nAdd / drop / alter column in table\n\u00b6\n\n\nALTER\n \nTABLE\n \ntable_name\n\n\nADD\n \ncolumn_name\n \ndatatype\n\n\n\nALTER\n \nTABLE\n \ntable_name\n\n\nDROP\n \nCOLUMN\n \ncolumn_name\n\n\n\nALTER\n \nTABLE\n \ntable_name\n\n\nALTER\n \nCOLUMN\n \ncolumn_name\n \ndatatype\n\n\n\n\n\n\nAutoincrement\n\u00b6\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nID\n \nint\n \nIDENTITY\n(\n1\n,\n1\n)\n \nPRIMARY\n \nKEY\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n)\n\n\n)\n\n\n\n\n\n\nExample:\n\n\nCREATE\n \nTABLE\n \ndbo\n.\nPurchaseOrderDetail\n\n\n(\n\n    \nPurchaseOrderID\n \nint\n \nNOT\n \nNULL\n\n        \nREFERENCES\n \nPurchasing\n.\nPurchaseOrderHeader\n(\nPurchaseOrderID\n),\n\n    \nLineNumber\n \nsmallint\n \nNOT\n \nNULL\n,\n\n    \nProductID\n \nint\n \nNULL\n\n        \nREFERENCES\n \nProduction\n.\nProduct\n(\nProductID\n),\n\n    \nUnitPrice\n \nmoney\n \nNULL\n,\n\n    \nOrderQty\n \nsmallint\n \nNULL\n,\n\n    \nReceivedQty\n \nfloat\n \nNULL\n,\n\n    \nRejectedQty\n \nfloat\n \nNULL\n,\n\n    \nDueDate\n \ndatetime\n \nNULL\n,\n\n    \nrowguid\n \nuniqueidentifier\n \nROWGUIDCOL\n  \nNOT\n \nNULL\n\n        \nCONSTRAINT\n \nDF_PurchaseOrderDetail_rowguid\n \nDEFAULT\n \n(\nnewid\n()),\n\n    \nModifiedDate\n \ndatetime\n \nNOT\n \nNULL\n\n        \nCONSTRAINT\n \nDF_PurchaseOrderDetail_ModifiedDate\n \nDEFAULT\n \n(\ngetdate\n()),\n\n    \nLineTotal\n  \nAS\n \n((\nUnitPrice\n*\nOrderQty\n)),\n\n    \nStockedQty\n  \nAS\n \n((\nReceivedQty\n-\nRejectedQty\n)),\n\n    \nCONSTRAINT\n \nPK_PurchaseOrderDetail_PurchaseOrderID_LineNumber\n\n              \nPRIMARY\n \nKEY\n \nCLUSTERED\n \n(\nPurchaseOrderID\n,\n \nLineNumber\n)\n\n              \nWITH\n \n(\nIGNORE_DUP_KEY\n \n=\n \nOFF\n)\n\n\n)\n\n\nON\n \nPRIMARY\n;\n\n\n\n\n\n\nViews\n\u00b6\n\n\nCREATE\n \nVIEW\n \nview_name\n \nAS\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n\n\nFROM\n \ntable_name\n\n\nWHERE\n \ncondition\n\n\n\n\n\n\nCREATE\n \nOR\n \nREPLACE\n \nVIEW\n \nview_name\n \nAS\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n\n\nFROM\n \ntable_name\n\n\nWHERE\n \ncondition\n\n\n\n\n\n\nDROP\n \nVIEW\n \nview_name\n\n\n\n\n\n\nExamples\n:\n\n\nCREATE\n \nVIEW\n \n[\nProducts\n \nAbove\n \nAverage\n \nPrice\n]\n \nAS\n\n\nSELECT\n \nProductName\n,\nUnitPrice\n\n\nFROM\n \nProducts\n\n\nWHERE\n \nUnitPrice\n \n>\n \n(\nSELECT\n \nAVG\n(\nUnitPrice\n)\n \nFROM\n \nProducts\n)\n\n\n\nSELECT\n \n*\n \nFROM\n \n[\nProducts\n \nAbove\n \nAverage\n \nPrice\n]\n\n\n\n\n\n\nCREATE\n \nVIEW\n \n[\nCategory\n \nSales\n \nFor\n \n1997\n]\n \nAS\n\n\nSELECT\n \nDISTINCT\n \nCategoryName\n,\n \nSum\n(\nProductSales\n)\n \nAS\n \nCategorySales\n\n\nFROM\n \n[\nProduct\n \nSales\n \nfor\n \n1997\n]\n\n\nGROUP\n \nBY\n \nCategoryName\n\n\n\n\n\n\nDates\n\u00b6\n\n\nGETDATE\n()\n  \n-- Returns the current date and time\n\n\n\nDATEPART\n()\n \n-- Returns a single part of a date/time\n\n\n\nDATEADD\n()\n  \n-- Adds or subtracts a specified time interval from a date\n\n\n\nDATEDIFF\n()\n \n-- Returns the time between two dates\n\n\n\nCONVERT\n()\n  \n-- Displays date/time data in different formats\n\n\n\n\n\n\nExample\n:\n\n\nCREATE\n \nTABLE\n \nOrders\n\n\n(\n\n\nOrderId\n \nint\n \nNOT\n \nNULL\n \nPRIMARY\n \nKEY\n,\n\n\nProductName\n \nvarchar\n(\n50\n)\n \nNOT\n \nNULL\n,\n\n\nOrderDate\n \ndatetime\n \nNOT\n \nNULL\n \nDEFAULT\n \nGETDATE\n()\n\n\n)\n\n\n\nSELECT\n \nDATEPART\n(\nyyyy\n,\nOrderDate\n)\n \nAS\n \nOrderYear\n,\n\n\nDATEPART\n(\nmm\n,\nOrderDate\n)\n \nAS\n \nOrderMonth\n,\n\n\nDATEPART\n(\ndd\n,\nOrderDate\n)\n \nAS\n \nOrderDay\n,\n\n\nFROM\n \nOrders\n\n\nWHERE\n \nOrderId\n=\n1\n\n\n\nSELECT\n \nOrderId\n,\nDATEADD\n(\nday\n,\n45\n,\nOrderDate\n)\n \nAS\n \nOrderPayDate\n\n\nFROM\n \nOrders\n\n\n\nSELECT\n \nDATEDIFF\n(\nday\n,\n'2008-06-05'\n,\n'2008-08-05'\n)\n \nAS\n \nDiffDate\n\n\n\nCONVERT\n(\nVARCHAR\n(\n19\n),\nGETDATE\n())\n\n\nCONVERT\n(\nVARCHAR\n(\n10\n),\nGETDATE\n(),\n10\n)\n\n\nCONVERT\n(\nVARCHAR\n(\n10\n),\nGETDATE\n(),\n110\n)\n\n\n\n\n\n\nSQL Server Data Types\n\u00b6\n\n\nData type / Description / Storage\n\n\nchar(n)\n\nFixed width character string. Maximum 8,000 characters\nDefined width\n\n\nvarchar(n)\n\nVariable width character string. Maximum 8,000 characters\n2 bytes + number of chars\n\n\nvarchar(max)\n\nVariable width character string. Maximum 1,073,741,824 characters\n2 bytes + number of chars\n\n\ntext\n\nVariable width character string. Maximum 2GB of text data\n4 bytes + number of chars\n\n\nnchar\n\nFixed width Unicode string. Maximum 4,000 characters\nDefined width x 2\n\n\nnvarchar\n\nVariable width Unicode string. Maximum 4,000 characters\n\n\nnvarchar(max)\n\nVariable width Unicode string. Maximum 536,870,912 characters\n\n\nntext\n\nVariable width Unicode string. Maximum 2GB of text data\n\n\nbit\n\nAllows 0, 1, or NULL\n\n\nbinary(n)\n\nFixed width binary string. Maximum 8,000 bytes\n\n\nvarbinary\n\nVariable width binary string. Maximum 8,000 bytes\n\n\nvarbinary(max)\n\nVariable width binary string. Maximum 2GB\n\n\nimage\n\nVariable width binary string. Maximum 2GB\n\n\nNumber types\n\u00b6\n\n\ntinyint\n\nAllows whole numbers from 0 to 255\n1 byte\n\n\nsmallint\n\nAllows whole numbers between -32,768 and 32,767\n2 bytes\n\n\nint\n\nAllows whole numbers between -2,147,483,648 and 2,147,483,647\n4 bytes\n\n\nbigint\n\nAllows whole numbers between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807\n8 bytes\n\n\ndecimal(p,s)\n\nFixed precision and scale numbers.\nAllows numbers from -10^38 +1 to 10^38.\n\n\nThe p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0.\n5-17 bytes\n\n\nnumeric(p,s)\n\nFixed precision and scale numbers.\nAllows numbers from -10^38 +1 to 10^38.\n\n\nThe p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0.\n5-17 bytes\n\n\nsmallmoney\n\nMonetary data from -214,748.3648 to 214,748.3647\n4 bytes\n\n\nmoney\n\nMonetary data from -922,337,203,685,477.5808 to 922,337,203,685,477.5807\n8 bytes\n\n\nfloat(n)\n\nFloating precision number data from -1.79E + 308 to 1.79E + 308.\nThe n parameter indicates whether the field should hold 4 or 8 bytes. float(24) holds a 4-byte field and float(53) holds an 8-byte field. Default value of n is 53.\n4 or 8 bytes\n\n\nreal\n\nFloating precision number data from -3.40E + 38 to 3.40E + 38\n4 bytes\n\n\nDate types\n\u00b6\n\n\ndatetime\n\nFrom January 1, 1753 to December 31, 9999 with an accuracy of 3.33 milliseconds\n8 bytes\n\n\ndatetime2\n\nFrom January 1, 0001 to December 31, 9999 with an accuracy of 100 nanoseconds\n6-8 bytes\n\n\nsmalldatetime\n\nFrom January 1, 1900 to June 6, 2079 with an accuracy of 1 minute\n4 bytes\n\n\ndate\n\nStore a date only. From January 1, 0001 to December 31, 9999\n3 bytes\n\n\ntime\n\nStore a time only to an accuracy of 100 nanoseconds\n3-5 bytes\n\n\ndatetimeoffset\n\nThe same as datetime2 with the addition of a time zone offset\n8-10 bytes\n\n\ntimestamp\n\nStores a unique number that gets updated every time a row gets created or modified. The timestamp value is based upon an internal clock and does not correspond to real time. Each table may have only one timestamp variable\n\n\nOther data types\n\u00b6\n\n\nsql_variant\n\nStores up to 8,000 bytes of data of various data types, except text, ntext, and timestamp\n\n\nuniqueidentifier\n\nStores a globally unique identifier (GUID)\n\n\nxml\n\nStores XML formatted data. Maximum 2GB\n\n\ncursor\n\nStores a reference to a cursor used for database operations\n\n\ntable\n\nStores a result-set for later processing\n\n\nSQL Aggregate Functions\n\u00b6\n\n\nSQL aggregate functions return a single value, calculated from values in a column.\n\n\nUseful aggregate functions:\n\n\n\n\nAVG()\n   - Returns the average value\n\n\nCOUNT()\n - Returns the number of rows\n\n\nTOP 1\n   - Single sample\n\n\nMAX()\n   - Returns the largest value\n\n\nMIN()\n   - Returns the smallest value\n\n\nSUM()\n   - Returns the sum\n\n\n\n\nExamples:\n\n\nSELECT\n \nCOUNT\n(\nDISTINCT\n \ncolumn_name\n)\n \nFROM\n \ntable_name\n;\n\n\n\nSELECT\n \nTOP\n \n1\n \ncolumn_name\n \nFROM\n \ntable_name\n\n\nORDER\n \nBY\n \ncolumn_name\n \nDESC\n;\n\n\n\nSELECT\n \ncolumn_name\n,\n \naggregate_function\n(\ncolumn_name\n)\n\n\nFROM\n \ntable_name\n\n\nWHERE\n \ncolumn_name\n \noperator\n \nvalue\n\n\nGROUP\n \nBY\n \ncolumn_name\n;\n\n\n\nSELECT\n \nShippers\n.\nShipperName\n,\n \nCOUNT\n(\nOrders\n.\nOrderID\n)\n \nAS\n \nNumberOfOrders\n\n\nFROM\n \nOrders\n\n\nLEFT\n \nJOIN\n \nShippers\n\n\nON\n \nOrders\n.\nShipperID\n=\nShippers\n.\nShipperID\n\n\nGROUP\n \nBY\n \nShipperName\n;\n\n\n\nSELECT\n \ncolumn_name\n,\n \naggregate_function\n(\ncolumn_name\n)\n\n\nFROM\n \ntable_name\n\n\nWHERE\n \ncolumn_name\n \noperator\n \nvalue\n\n\nGROUP\n \nBY\n \ncolumn_name\n\n\nHAVING\n \naggregate_function\n(\ncolumn_name\n)\n \noperator\n \nvalue\n;\n\n\n\nSELECT\n \nEmployees\n.\nLastName\n,\n \nCOUNT\n(\nOrders\n.\nOrderID\n)\n \nAS\n \nNumberOfOrders\n\n\nFROM\n \nOrders\n\n\nINNER\n \nJOIN\n \nEmployees\n\n\nON\n \nOrders\n.\nEmployeeID\n=\nEmployees\n.\nEmployeeID\n)\n\n\nGROUP\n \nBY\n \nLastName\n\n\nHAVING\n \nCOUNT\n(\nOrders\n.\nOrderID\n)\n \n>\n \n10\n;\n\n\n\n\n\n\nSQL Scalar functions\n\u00b6\n\n\n\n\nConverts a field to upper case: SELECT UPPER(column_name) FROM table_name;\n\n\nConverts a field to lower case: SELECT LOWER(column_name) FROM table_name;\n\n\nMID() - Extract characters from a text field\n\n\nLEN() - Returns the length of a text field\n\n\nROUND() - Rounds a numeric field to the number of decimals specified\n\n\nNOW() - Returns the current system date and time\n\n\nFORMAT() - Formats how a field is to be displayed\n\n\n\n\nSELECT\n \nProductName\n,\n \nROUND\n(\nPrice\n,\n0\n)\n \nAS\n \nRoundedPrice\n\n\nFROM\n \nProducts\n;\n\n\n\n\n\n\nVariables\n\u00b6\n\n\nDECLARE\n \n@\nmyvar\n \nchar\n(\n20\n);\n\n\nSET\n \n@\nmyvar\n \n=\n \n'This is a test'\n;\n\n\nSELECT\n \n@\nmyvar\n;\n\n\n\n\n\n\nScalar Function\n\u00b6\n\n\nCREATE\n \nFUNCTION\n \nFunctionName\n\n\n(\n\n\n-- Add the parameters for the function here\n\n\n@\np1\n \nint\n\n\n)\n\n\nRETURNS\n \nint\n\n\nAS\n\n\nBEGIN\n\n\n-- Declare the return variable here\n\n\nDECLARE\n \n@\nResult\n \nint\n\n\n-- Add the T-SQL statements to compute the return value here\n\n\nSELECT\n \n@\nResult\n \n=\n \n@\np1\n\n\n\n-- Return the result of the function\n\n\nRETURN\n \n@\nResult\n\n\nEND\n\n\n\n\n\n\nTable Value Function\n\u00b6\n\n\nIF\n \nOBJECT_ID\n \n(\nN\n'dbo.EmployeeByID'\n \n)\n \nIS\n \nNOT\n \nNULL\n\n   \nDROP\n \nFUNCTION\n \ndbo\n.\nEmployeeByID\n\n\nGO\n\n\n\nCREATE\n \nFUNCTION\n \ndbo\n.\nEmployeeByID\n(\n@\nInEmpID\n \nint\n)\n\n\nRETURNS\n \n@\nretFindReports\n \nTABLE\n\n\n(\n\n    \n-- columns returned by the function\n\n    \nEmployeeID\n \nint\n \nNOT\n \nNULL\n,\n\n    \nName\n \nnvarchar\n(\n255\n \n)\n \nNOT\n \nNULL\n,\n\n    \nTitle\n \nnvarchar\n(\n50\n \n)\n \nNOT\n \nNULL\n,\n\n    \nEmployeeLevel\n \nint\n \nNOT\n \nNULL\n\n\n)\n\n\nAS\n\n\n-- body of the function\n\n\nBEGIN\n\n   \nWITH\n \nDirectReports\n(\nName\n \n,\n \nTitle\n \n,\n \nEmployeeID\n \n,\n \nEmployeeLevel\n \n,\n \nSort\n \n)\n \nAS\n\n    \n(\nSELECT\n \nCONVERT\n(\n \nvarchar\n(\n255\n \n),\n \nc\n \n.\nFirstName\n \n+\n \n' '\n \n+\n \nc\n.\nLastName\n \n),\n\n        \ne\n.\nTitle\n \n,\n\n        \ne\n.\nEmployeeID\n \n,\n\n        \n1\n \n,\n\n        \nCONVERT\n(\nvarchar\n \n(\n255\n),\n \nc\n.\n \nFirstName\n \n+\n \n' '\n \n+\n \nc\n \n.\nLastName\n)\n\n     \nFROM\n \nHumanResources\n.\nEmployee\n \nAS\n \ne\n\n          \nJOIN\n \nPerson\n.\nContact\n \nAS\n \nc\n \nON\n \ne\n.\nContactID\n \n=\n \nc\n.\nContactID\n\n     \nWHERE\n \ne\n.\nEmployeeID\n \n=\n \n@\nInEmpID\n\n   \nUNION\n \nALL\n\n     \nSELECT\n \nCONVERT\n \n(\nvarchar\n(\n \n255\n),\n \nREPLICATE\n \n(\n \n'| '\n \n,\n \nEmployeeLevel\n)\n \n+\n\n        \nc\n.\nFirstName\n \n+\n \n' '\n \n+\n \nc\n.\n \nLastName\n),\n\n        \ne\n.\nTitle\n \n,\n\n        \ne\n.\nEmployeeID\n \n,\n\n        \nEmployeeLevel\n \n+\n \n1\n,\n\n        \nCONVERT\n \n(\n \nvarchar\n(\n255\n \n),\n \nRTRIM\n \n(\nSort\n)\n \n+\n \n'| '\n \n+\n \nFirstName\n \n+\n \n' '\n \n+\n\n                 \nLastName\n)\n\n     \nFROM\n \nHumanResources\n.\nEmployee\n \nas\n \ne\n\n          \nJOIN\n \nPerson\n.\nContact\n \nAS\n \nc\n \nON\n \ne\n.\nContactID\n \n=\n \nc\n.\nContactID\n\n          \nJOIN\n \nDirectReports\n \nAS\n \nd\n \nON\n \ne\n.\n \nManagerID\n \n=\n \nd\n.\n \nEmployeeID\n\n    \n)\n\n   \n-- copy the required columns to the result of the function\n\n\n   \nINSERT\n \n@\nretFindReports\n\n   \nSELECT\n \nEmployeeID\n,\n \nName\n,\n \nTitle\n,\n \nEmployeeLevel\n\n     \nFROM\n \nDirectReports\n\n   \nORDER\n \nBY\n \nSort\n\n   \nRETURN\n\n\nEND\n\n\nGO\n\n\n\n\n\n\nStored Procedure\n\u00b6\n\n\nCREATE\n \nPROCEDURE\n \nProcedureName\n\n        \n-- Add the parameters for the stored procedure here\n\n        \n@\np1\n \nint\n \n=\n \n0\n \n,\n\n        \n@\np2\n \nint\n \n=\n \n0\n\n\nAS\n\n\nBEGIN\n\n        \n-- SET NOCOUNT ON added to prevent extra result sets from\n\n        \n-- interfering with SELECT statements.\n\n        \nSET\n \nNOCOUNT\n \nON\n;\n\n\n    \n-- Insert statements for procedure here\n\n        \nSELECT\n \n@\np1\n \n,\n \n@\np2\n\n\nEND\n\n\nGO\n\n\n\n\n\n\nSelf-join\n\u00b6\n\n\nQ. Here's the data in a table 'orders'\n\n\ncustomer_id order_id order_day\n123        27424624    25Dec2011\n123        89690900    25Dec2010\n797        12131323    25Dec2010\n876        67145419    15Dec2011\n\n\n\n\n\nCould you give me SQL for customers who placed orders on both the days, 25th Dec 2010 and 25th Dec 2011?\n\n\n    \nSELECT\n \no\n.\ncustomer_id\n,\n \no\n.\norder_day\n\n    \nFROM\n \norders\n \nAS\n \no\n\n    \nINNER\n \nJOIN\n \norders\n \nAS\n \no1\n\n    \nON\n \no\n.\ncustomer_id\n \n=\n \no1\n.\ncustomer_id\n\n    \nWHERE\n \n...",
            "title": "SQL Cheatsheet"
        },
        {
            "location": "/Databases/SQL/#sql-cheatsheet",
            "text": "",
            "title": "SQL Cheatsheet"
        },
        {
            "location": "/Databases/SQL/#dml-select",
            "text": "Filter :  SELECT   LastName ,   FirstName ,   Address   FROM   Persons  WHERE   Address   IS   NULL   Like :  SELECT   *   FROM   Customers  WHERE   City   LIKE   's%' ;  SELECT   *   FROM   Customers  WHERE   Country   LIKE   '%land%' ;  SELECT   *   FROM   Customers  WHERE   Country   NOT   LIKE   '%land%' ;   Sort :  SELECT   *   FROM   Customers  ORDER   BY   Country   DESC ;  SELECT   *   FROM   Customers  ORDER   BY   Country ,   CustomerName ;   Limit :  SELECT   TOP   number | percent   column_name ( s )  FROM   table_name ;  -- Examples:  SELECT   TOP   2   *   FROM   Customers ;  SELECT   TOP   50   PERCENT   *   FROM   Customers ;   Oracle Syntax :  SELECT   column_name ( s )  FROM   table_name  WHERE   ROWNUM   <=   number ;   Joins :  SELECT   Customers . CustomerName ,   Orders . OrderID  FROM   Customers  FULL   OUTER   JOIN   Orders  ON   Customers . CustomerID   =   Orders . CustomerID  ORDER   BY   Customers . CustomerName ;   Union :  SELECT   column_name ( s )   FROM   table1  UNION  SELECT   column_name ( s )   FROM   table2 ;  SELECT   column_name ( s )   FROM   table1  UNION   ALL  SELECT   column_name ( s )   FROM   table2 ;   Select Into :  SELECT   column_name ( s )  INTO   newtable   [ IN   externaldb ]  FROM   table1 ;   Formula :  SELECT   ProductName ,   UnitPrice * ( UnitsInStock + ISNULL ( UnitsOnOrder , 0 ))  FROM   Products",
            "title": "DML: SELECT"
        },
        {
            "location": "/Databases/SQL/#dml-insert",
            "text": "INSERT   INTO   table_name  VALUES   ( value1 , value2 , value3 ,...);  INSERT   INTO   table_name   ( column1 , column2 , column3 ,...)  VALUES   ( value1 , value2 , value3 ,...);  -- Example:  INSERT   INTO   Customers   ( CustomerName ,   City ,   Country )  VALUES   ( 'Cardinal' ,   'Stavanger' ,   'Norway' );   Insert from select :  INSERT   INTO   table2 ( column_name ( s ))  SELECT   column_name ( s )  FROM   table1 ;  -- Example:  INSERT   INTO   Customers   ( CustomerName ,   Country )  SELECT   SupplierName ,   Country   FROM   Suppliers  WHERE   Country = 'Germany' ;",
            "title": "DML: INSERT"
        },
        {
            "location": "/Databases/SQL/#dml-update",
            "text": "UPDATE   table_name  SET   column1 = value1 , column2 = value2 ,...  WHERE   some_column = some_value ;  -- Example:  UPDATE   Customers  SET   ContactName = 'Alfred Schmidt' ,   City = 'Hamburg'  WHERE   CustomerName = 'Alfreds Futterkiste' ;",
            "title": "DML: UPDATE"
        },
        {
            "location": "/Databases/SQL/#dml-delete",
            "text": "DELETE   FROM   table_name  WHERE   some_column = some_value ;  DELETE   FROM   Customers  WHERE   CustomerName = 'Alfreds Futterkiste'   AND   ContactName = 'Maria Anders' ;",
            "title": "DML: DELETE"
        },
        {
            "location": "/Databases/SQL/#databases",
            "text": "CREATE   DATABASE   my_db ;  DROP   DATABASE   my_db ;",
            "title": "Databases"
        },
        {
            "location": "/Databases/SQL/#tables",
            "text": "Create :  CREATE   TABLE   table_name  (  column_name1   data_type ( size ),  column_name2   data_type ( size ),  column_name3   data_type ( size ),  ....  );  CREATE   TABLE   table_name  (  column_name1   data_type ( size )   constraint_name ,  column_name2   data_type ( size )   constraint_name ,  column_name3   data_type ( size )   constraint_name ,  ....  );   -- Examples  CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL   UNIQUE ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 )  )  CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 ),  CONSTRAINT   uc_PersonID   UNIQUE   ( P_Id ,   LastName )  )   ALTER   TABLE   Persons  ADD   CONSTRAINT   uc_PersonID   UNIQUE   ( P_Id , LastName )  ALTER   TABLE   Persons  DROP   CONSTRAINT   uc_PersonID   Temporary Table :  CREATE   TABLE   # MyTempTable   ( cola   INT   PRIMARY   KEY );  INSERT   INTO   # MyTempTable   VALUES   ( 1 );   Drop / Truncate :  DROP   TABLE   table_name  TRUNCATE   TABLE   table_name",
            "title": "Tables"
        },
        {
            "location": "/Databases/SQL/#primary-key-constraint",
            "text": "CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL   PRIMARY   KEY ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 )  )  CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 ),  CONSTRAINT   PK_PersonID   PRIMARY   KEY   ( P_Id , LastName )  )  ALTER   TABLE   Persons  ADD   CONSTRAINT   PK_PersonID   PRIMARY   KEY   ( P_Id , LastName )  ALTER   TABLE   Persons  DROP   CONSTRAINT   PK_PersonID",
            "title": "PRIMARY KEY constraint"
        },
        {
            "location": "/Databases/SQL/#foreign-key-constraints",
            "text": "CREATE   TABLE   Orders  (  O_Id   int   NOT   NULL   PRIMARY   KEY ,  OrderNo   int   NOT   NULL ,  P_Id   int   FOREIGN   KEY   REFERENCES   Persons ( P_Id )  )  CREATE   TABLE   Orders  (  O_Id   int   NOT   NULL ,  OrderNo   int   NOT   NULL ,  P_Id   int ,  PRIMARY   KEY   ( O_Id ),  CONSTRAINT   FK_PerOrders   FOREIGN   KEY   ( P_Id )  REFERENCES   Persons ( P_Id )  )   ALTER   TABLE   Orders  ADD   FOREIGN   KEY   ( P_Id )  REFERENCES   Persons ( P_Id )  ALTER   TABLE   Orders  ADD   CONSTRAINT   fk_PerOrders  FOREIGN   KEY   ( P_Id )  REFERENCES   Persons ( P_Id )  ALTER   TABLE   Orders  DROP   CONSTRAINT   fk_PerOrders",
            "title": "FOREIGN KEY constraints"
        },
        {
            "location": "/Databases/SQL/#check-constraints",
            "text": "CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL   CHECK   ( P_Id > 0 ),  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 )  )  CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 ),  CONSTRAINT   chk_Person   CHECK   ( P_Id > 0   AND   City = 'Sandnes' )  )   ALTER   TABLE   Persons  ADD   CONSTRAINT   CHK_Person   CHECK   ( P_Id > 0   AND   City = 'Sandnes' )  ALTER   TABLE   Persons  DROP   CONSTRAINT   CHK_Person",
            "title": "CHECK Constraints"
        },
        {
            "location": "/Databases/SQL/#default-constraints",
            "text": "CREATE   TABLE   Orders  (  O_Id   int   NOT   NULL ,  OrderNo   int   NOT   NULL ,  P_Id   int ,  OrderDate   date   DEFAULT   GETDATE ()  )   ALTER   TABLE   Persons  ALTER   COLUMN   City   SET   DEFAULT   'SEATTLE'  ALTER   TABLE   Persons  ALTER   COLUMN   City   DROP   DEFAULT",
            "title": "DEFAULT Constraints"
        },
        {
            "location": "/Databases/SQL/#index",
            "text": "CREATE   UNIQUE   INDEX   index_name  ON   table_name   ( column_name )  CREATE   INDEX   index_name  ON   table_name   ( column_name1 ,   col_name2 )  -- Example:  CREATE   INDEX   PIndex  ON   Persons   ( LastName ,   FirstName )   DROP   INDEX   table_name . index_name  -- Example:  DROP   INDEX   IX_ProductVendor_BusinessEntityID \n     ON   Purchasing . ProductVendor ;",
            "title": "Index"
        },
        {
            "location": "/Databases/SQL/#add-drop-alter-column-in-table",
            "text": "ALTER   TABLE   table_name  ADD   column_name   datatype  ALTER   TABLE   table_name  DROP   COLUMN   column_name  ALTER   TABLE   table_name  ALTER   COLUMN   column_name   datatype",
            "title": "Add / drop / alter column in table"
        },
        {
            "location": "/Databases/SQL/#autoincrement",
            "text": "CREATE   TABLE   Persons  (  ID   int   IDENTITY ( 1 , 1 )   PRIMARY   KEY ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 )  )   Example:  CREATE   TABLE   dbo . PurchaseOrderDetail  ( \n     PurchaseOrderID   int   NOT   NULL \n         REFERENCES   Purchasing . PurchaseOrderHeader ( PurchaseOrderID ), \n     LineNumber   smallint   NOT   NULL , \n     ProductID   int   NULL \n         REFERENCES   Production . Product ( ProductID ), \n     UnitPrice   money   NULL , \n     OrderQty   smallint   NULL , \n     ReceivedQty   float   NULL , \n     RejectedQty   float   NULL , \n     DueDate   datetime   NULL , \n     rowguid   uniqueidentifier   ROWGUIDCOL    NOT   NULL \n         CONSTRAINT   DF_PurchaseOrderDetail_rowguid   DEFAULT   ( newid ()), \n     ModifiedDate   datetime   NOT   NULL \n         CONSTRAINT   DF_PurchaseOrderDetail_ModifiedDate   DEFAULT   ( getdate ()), \n     LineTotal    AS   (( UnitPrice * OrderQty )), \n     StockedQty    AS   (( ReceivedQty - RejectedQty )), \n     CONSTRAINT   PK_PurchaseOrderDetail_PurchaseOrderID_LineNumber \n               PRIMARY   KEY   CLUSTERED   ( PurchaseOrderID ,   LineNumber ) \n               WITH   ( IGNORE_DUP_KEY   =   OFF )  )  ON   PRIMARY ;",
            "title": "Autoincrement"
        },
        {
            "location": "/Databases/SQL/#views",
            "text": "CREATE   VIEW   view_name   AS  SELECT   column_name ( s )  FROM   table_name  WHERE   condition   CREATE   OR   REPLACE   VIEW   view_name   AS  SELECT   column_name ( s )  FROM   table_name  WHERE   condition   DROP   VIEW   view_name   Examples :  CREATE   VIEW   [ Products   Above   Average   Price ]   AS  SELECT   ProductName , UnitPrice  FROM   Products  WHERE   UnitPrice   >   ( SELECT   AVG ( UnitPrice )   FROM   Products )  SELECT   *   FROM   [ Products   Above   Average   Price ]   CREATE   VIEW   [ Category   Sales   For   1997 ]   AS  SELECT   DISTINCT   CategoryName ,   Sum ( ProductSales )   AS   CategorySales  FROM   [ Product   Sales   for   1997 ]  GROUP   BY   CategoryName",
            "title": "Views"
        },
        {
            "location": "/Databases/SQL/#dates",
            "text": "GETDATE ()    -- Returns the current date and time  DATEPART ()   -- Returns a single part of a date/time  DATEADD ()    -- Adds or subtracts a specified time interval from a date  DATEDIFF ()   -- Returns the time between two dates  CONVERT ()    -- Displays date/time data in different formats   Example :  CREATE   TABLE   Orders  (  OrderId   int   NOT   NULL   PRIMARY   KEY ,  ProductName   varchar ( 50 )   NOT   NULL ,  OrderDate   datetime   NOT   NULL   DEFAULT   GETDATE ()  )  SELECT   DATEPART ( yyyy , OrderDate )   AS   OrderYear ,  DATEPART ( mm , OrderDate )   AS   OrderMonth ,  DATEPART ( dd , OrderDate )   AS   OrderDay ,  FROM   Orders  WHERE   OrderId = 1  SELECT   OrderId , DATEADD ( day , 45 , OrderDate )   AS   OrderPayDate  FROM   Orders  SELECT   DATEDIFF ( day , '2008-06-05' , '2008-08-05' )   AS   DiffDate  CONVERT ( VARCHAR ( 19 ), GETDATE ())  CONVERT ( VARCHAR ( 10 ), GETDATE (), 10 )  CONVERT ( VARCHAR ( 10 ), GETDATE (), 110 )",
            "title": "Dates"
        },
        {
            "location": "/Databases/SQL/#sql-server-data-types",
            "text": "Data type / Description / Storage  char(n) \nFixed width character string. Maximum 8,000 characters\nDefined width  varchar(n) \nVariable width character string. Maximum 8,000 characters\n2 bytes + number of chars  varchar(max) \nVariable width character string. Maximum 1,073,741,824 characters\n2 bytes + number of chars  text \nVariable width character string. Maximum 2GB of text data\n4 bytes + number of chars  nchar \nFixed width Unicode string. Maximum 4,000 characters\nDefined width x 2  nvarchar \nVariable width Unicode string. Maximum 4,000 characters  nvarchar(max) \nVariable width Unicode string. Maximum 536,870,912 characters  ntext \nVariable width Unicode string. Maximum 2GB of text data  bit \nAllows 0, 1, or NULL  binary(n) \nFixed width binary string. Maximum 8,000 bytes  varbinary \nVariable width binary string. Maximum 8,000 bytes  varbinary(max) \nVariable width binary string. Maximum 2GB  image \nVariable width binary string. Maximum 2GB",
            "title": "SQL Server Data Types"
        },
        {
            "location": "/Databases/SQL/#number-types",
            "text": "tinyint \nAllows whole numbers from 0 to 255\n1 byte  smallint \nAllows whole numbers between -32,768 and 32,767\n2 bytes  int \nAllows whole numbers between -2,147,483,648 and 2,147,483,647\n4 bytes  bigint \nAllows whole numbers between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807\n8 bytes  decimal(p,s) \nFixed precision and scale numbers.\nAllows numbers from -10^38 +1 to 10^38.  The p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0.\n5-17 bytes  numeric(p,s) \nFixed precision and scale numbers.\nAllows numbers from -10^38 +1 to 10^38.  The p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0.\n5-17 bytes  smallmoney \nMonetary data from -214,748.3648 to 214,748.3647\n4 bytes  money \nMonetary data from -922,337,203,685,477.5808 to 922,337,203,685,477.5807\n8 bytes  float(n) \nFloating precision number data from -1.79E + 308 to 1.79E + 308.\nThe n parameter indicates whether the field should hold 4 or 8 bytes. float(24) holds a 4-byte field and float(53) holds an 8-byte field. Default value of n is 53.\n4 or 8 bytes  real \nFloating precision number data from -3.40E + 38 to 3.40E + 38\n4 bytes",
            "title": "Number types"
        },
        {
            "location": "/Databases/SQL/#date-types",
            "text": "datetime \nFrom January 1, 1753 to December 31, 9999 with an accuracy of 3.33 milliseconds\n8 bytes  datetime2 \nFrom January 1, 0001 to December 31, 9999 with an accuracy of 100 nanoseconds\n6-8 bytes  smalldatetime \nFrom January 1, 1900 to June 6, 2079 with an accuracy of 1 minute\n4 bytes  date \nStore a date only. From January 1, 0001 to December 31, 9999\n3 bytes  time \nStore a time only to an accuracy of 100 nanoseconds\n3-5 bytes  datetimeoffset \nThe same as datetime2 with the addition of a time zone offset\n8-10 bytes  timestamp \nStores a unique number that gets updated every time a row gets created or modified. The timestamp value is based upon an internal clock and does not correspond to real time. Each table may have only one timestamp variable",
            "title": "Date types"
        },
        {
            "location": "/Databases/SQL/#other-data-types",
            "text": "sql_variant \nStores up to 8,000 bytes of data of various data types, except text, ntext, and timestamp  uniqueidentifier \nStores a globally unique identifier (GUID)  xml \nStores XML formatted data. Maximum 2GB  cursor \nStores a reference to a cursor used for database operations  table \nStores a result-set for later processing",
            "title": "Other data types"
        },
        {
            "location": "/Databases/SQL/#sql-aggregate-functions",
            "text": "SQL aggregate functions return a single value, calculated from values in a column.  Useful aggregate functions:   AVG()    - Returns the average value  COUNT()  - Returns the number of rows  TOP 1    - Single sample  MAX()    - Returns the largest value  MIN()    - Returns the smallest value  SUM()    - Returns the sum   Examples:  SELECT   COUNT ( DISTINCT   column_name )   FROM   table_name ;  SELECT   TOP   1   column_name   FROM   table_name  ORDER   BY   column_name   DESC ;  SELECT   column_name ,   aggregate_function ( column_name )  FROM   table_name  WHERE   column_name   operator   value  GROUP   BY   column_name ;  SELECT   Shippers . ShipperName ,   COUNT ( Orders . OrderID )   AS   NumberOfOrders  FROM   Orders  LEFT   JOIN   Shippers  ON   Orders . ShipperID = Shippers . ShipperID  GROUP   BY   ShipperName ;  SELECT   column_name ,   aggregate_function ( column_name )  FROM   table_name  WHERE   column_name   operator   value  GROUP   BY   column_name  HAVING   aggregate_function ( column_name )   operator   value ;  SELECT   Employees . LastName ,   COUNT ( Orders . OrderID )   AS   NumberOfOrders  FROM   Orders  INNER   JOIN   Employees  ON   Orders . EmployeeID = Employees . EmployeeID )  GROUP   BY   LastName  HAVING   COUNT ( Orders . OrderID )   >   10 ;",
            "title": "SQL Aggregate Functions"
        },
        {
            "location": "/Databases/SQL/#sql-scalar-functions",
            "text": "Converts a field to upper case: SELECT UPPER(column_name) FROM table_name;  Converts a field to lower case: SELECT LOWER(column_name) FROM table_name;  MID() - Extract characters from a text field  LEN() - Returns the length of a text field  ROUND() - Rounds a numeric field to the number of decimals specified  NOW() - Returns the current system date and time  FORMAT() - Formats how a field is to be displayed   SELECT   ProductName ,   ROUND ( Price , 0 )   AS   RoundedPrice  FROM   Products ;",
            "title": "SQL Scalar functions"
        },
        {
            "location": "/Databases/SQL/#variables",
            "text": "DECLARE   @ myvar   char ( 20 );  SET   @ myvar   =   'This is a test' ;  SELECT   @ myvar ;",
            "title": "Variables"
        },
        {
            "location": "/Databases/SQL/#scalar-function",
            "text": "CREATE   FUNCTION   FunctionName  (  -- Add the parameters for the function here  @ p1   int  )  RETURNS   int  AS  BEGIN  -- Declare the return variable here  DECLARE   @ Result   int  -- Add the T-SQL statements to compute the return value here  SELECT   @ Result   =   @ p1  -- Return the result of the function  RETURN   @ Result  END",
            "title": "Scalar Function"
        },
        {
            "location": "/Databases/SQL/#table-value-function",
            "text": "IF   OBJECT_ID   ( N 'dbo.EmployeeByID'   )   IS   NOT   NULL \n    DROP   FUNCTION   dbo . EmployeeByID  GO  CREATE   FUNCTION   dbo . EmployeeByID ( @ InEmpID   int )  RETURNS   @ retFindReports   TABLE  ( \n     -- columns returned by the function \n     EmployeeID   int   NOT   NULL , \n     Name   nvarchar ( 255   )   NOT   NULL , \n     Title   nvarchar ( 50   )   NOT   NULL , \n     EmployeeLevel   int   NOT   NULL  )  AS  -- body of the function  BEGIN \n    WITH   DirectReports ( Name   ,   Title   ,   EmployeeID   ,   EmployeeLevel   ,   Sort   )   AS \n     ( SELECT   CONVERT (   varchar ( 255   ),   c   . FirstName   +   ' '   +   c . LastName   ), \n         e . Title   , \n         e . EmployeeID   , \n         1   , \n         CONVERT ( varchar   ( 255 ),   c .   FirstName   +   ' '   +   c   . LastName ) \n      FROM   HumanResources . Employee   AS   e \n           JOIN   Person . Contact   AS   c   ON   e . ContactID   =   c . ContactID \n      WHERE   e . EmployeeID   =   @ InEmpID \n    UNION   ALL \n      SELECT   CONVERT   ( varchar (   255 ),   REPLICATE   (   '| '   ,   EmployeeLevel )   + \n         c . FirstName   +   ' '   +   c .   LastName ), \n         e . Title   , \n         e . EmployeeID   , \n         EmployeeLevel   +   1 , \n         CONVERT   (   varchar ( 255   ),   RTRIM   ( Sort )   +   '| '   +   FirstName   +   ' '   + \n                  LastName ) \n      FROM   HumanResources . Employee   as   e \n           JOIN   Person . Contact   AS   c   ON   e . ContactID   =   c . ContactID \n           JOIN   DirectReports   AS   d   ON   e .   ManagerID   =   d .   EmployeeID \n     ) \n    -- copy the required columns to the result of the function \n\n    INSERT   @ retFindReports \n    SELECT   EmployeeID ,   Name ,   Title ,   EmployeeLevel \n      FROM   DirectReports \n    ORDER   BY   Sort \n    RETURN  END  GO",
            "title": "Table Value Function"
        },
        {
            "location": "/Databases/SQL/#stored-procedure",
            "text": "CREATE   PROCEDURE   ProcedureName \n         -- Add the parameters for the stored procedure here \n         @ p1   int   =   0   , \n         @ p2   int   =   0  AS  BEGIN \n         -- SET NOCOUNT ON added to prevent extra result sets from \n         -- interfering with SELECT statements. \n         SET   NOCOUNT   ON ; \n\n     -- Insert statements for procedure here \n         SELECT   @ p1   ,   @ p2  END  GO",
            "title": "Stored Procedure"
        },
        {
            "location": "/Databases/SQL/#self-join",
            "text": "Q. Here's the data in a table 'orders'  customer_id order_id order_day\n123        27424624    25Dec2011\n123        89690900    25Dec2010\n797        12131323    25Dec2010\n876        67145419    15Dec2011  Could you give me SQL for customers who placed orders on both the days, 25th Dec 2010 and 25th Dec 2011?       SELECT   o . customer_id ,   o . order_day \n     FROM   orders   AS   o \n     INNER   JOIN   orders   AS   o1 \n     ON   o . customer_id   =   o1 . customer_id \n     WHERE   ...",
            "title": "Self-join"
        },
        {
            "location": "/Deep_Learning/Keras/",
            "text": "Useful Links\n\u00b6\n\n\n\n\nKeras\n\n\n\n\nKeras Blog\n\n\n\n\n\n\nBuilding autoencoders in Keras\n\n\n\n\n\n\nmanifold-learning-and-autoencoders\n\n\n\n\nKeras tutorial for Kaggle 2nd Annual Data Science Bowl\n\n\nSupervised Sequence Labelling with Recurrent Neural Networks\n\n\nSequence Classification with LSTM Recurrent Neural Networks in Python with Keras",
            "title": "Keras Cheatsheet"
        },
        {
            "location": "/Deep_Learning/Keras/#useful-links",
            "text": "Keras   Keras Blog    Building autoencoders in Keras    manifold-learning-and-autoencoders   Keras tutorial for Kaggle 2nd Annual Data Science Bowl  Supervised Sequence Labelling with Recurrent Neural Networks  Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras",
            "title": "Useful Links"
        },
        {
            "location": "/DevOps/CloudFormation/",
            "text": "DevOps Philosophy\n\u00b6\n\n\nWhy we use Terraform and not Chef, Puppet, Ansible, SaltStack, or CloudFormation\n\n\nTools\n\u00b6\n\n\n\n\nAWS CLI cloudformation\n\n\naws cloudformation validate-template\n\n\nboto3 cloudformation\n\n\n\n\nYAML\n\u00b6\n\n\n\n\nYAML Cheatsheet\n\n\nYAML Cheatsheet 2\n\n\n\n\nYAML notation for folded text: \n>\n\n\ndata\n:\n \n>\n\n   \nWrapped text\n\n   \nwill be folded\n\n   \ninto a single\n\n   \nparagraph\n\n\n   \nBlank lines denote\n\n   \nparagraph breaks\n\n\n\n\n\n\nSample Templates\n\u00b6\n\n\nTemplates for the US East (Northern Virginia) Region\n\n\nAWSlabs on GitHub\n\n- \nStartup kit templates\n\n- \nAWS CloudFormation Sample Templates\n\n\nCloudonaut Templates\n\n\nFree Templates for AWS CloudFormation (Cloudonaut)\n\n\nDeploying Microservices with Amazon ECS, AWS CloudFormation, and an Application Load Balancer\n\n\nTemplate Basics\n\u00b6\n\n\nTemplate Basics\n\n\nTemplate Anatomy\n\n\n---\n\n\nAWSTemplateFormatVersion\n:\n \n\"version\n \ndate\"\n\n\n\nDescription\n:\n\n  \nString\n\n\n\nMetadata\n:\n\n  \ntemplate metadata\n\n\n\nParameters\n:\n\n  \nset of parameters\n\n\n\nMappings\n:\n\n  \nset of mappings\n\n\n\nConditions\n:\n\n  \nset of conditions\n\n\n\nTransform\n:\n\n  \nset of transforms\n\n\n\nResources\n:\n\n  \nset of resources\n\n\n\nOutputs\n:\n\n  \nset of outputs\n\n\n\n\n\n\nWith examples:\n\n\n---\n\n\nAWSTemplateFormatVersion\n:\n \n\"2010-09-09\"\n\n\n\nDescription\n:\n \n>\n\n  \nHere are some\n\n  \ndetails about\n\n  \nthe template.\n\n\n\nMetadata\n:\n\n  \nInstances\n:\n\n    \nDescription\n:\n \n\"Information\n \nabout\n \nthe\n \ninstances\"\n\n  \nDatabases\n:\n \n    \nDescription\n:\n \n\"Information\n \nabout\n \nthe\n \ndatabases\"\n\n\n\nParameters\n:\n \n  \nInstanceTypeParameter\n:\n \n    \nType\n:\n \nString\n            \n# String, Number, List<Number>, CommaDelimitedList e.g. \"test,dev,prod\", or an AWS-specific types such as Amazon EC2 key pair names and VPC IDs.\n\n    \nDefault\n:\n \nt2.micro\n\n    \nAllowedValues\n:\n \n      \n-\n \nt2.micro\n\n      \n-\n \nm1.small\n\n    \nDescription\n:\n \nEnter t2.micro or m1.small. Default is t2.micro.\n\n    \n# AllowedPattern: \"[A-Za-z0-9]+\" # A regular expression that represents the patterns you want to allow for String types.\n\n    \n# ConstraintDescription: Malformed input-Parameter MyParameter must match pattern [A-Za-z0-9]+\n\n    \n# MinLength: 2  # for String\n\n    \n# MaxLength: 10\n\n    \n# MinValue: 0   # for Number types.\n\n    \n# MaxValue: 100 \n\n    \n# NoEcho: True\n\n\n\nMappings\n:\n \n  \nRegionMap\n:\n \n    \nus-east-1\n:\n \n      \n\"32\"\n:\n \n\"ami-6411e20d\"\n\n    \nus-west-1\n:\n \n      \n\"32\"\n:\n \n\"ami-c9c7978c\"\n\n    \neu-west-1\n:\n \n      \n\"32\"\n:\n \n\"ami-37c2f643\"\n\n    \nap-southeast-1\n:\n \n      \n\"32\"\n:\n \n\"ami-66f28c34\"\n\n    \nap-northeast-1\n:\n \n      \n\"32\"\n:\n \n\"ami-9c03a89d\"\n\n\n\nConditions\n:\n \n  \nCreateProdResources\n:\n \n!Equals\n \n[\n \n!Ref\n \nEnvType\n,\n \nprod\n \n]\n\n\n\nTransform\n:\n\n  \nset of transforms\n\n\n\nResources\n:\n\n  \nEc2Instance\n:\n\n      \nType\n:\n \nAWS::EC2::Instance\n\n      \nProperties\n:\n\n        \nInstanceType\n:\n\n          \nRef\n:\n \nInstanceTypeParameter\n  \n# reference to parameter above\n\n        \nImageId\n:\n \nami-2f726546\n\n\n\nOutputs\n:\n \n  \nVolumeId\n:\n \n    \nCondition\n:\n \nCreateProdResources\n\n    \nValue\n:\n \n      \n!Ref\n \nNewVolume\n\n\n\n\n\n\n\n\nThe Ref function can refer to input parameters that are specified at stack creation time.\n\n\n\n\nExamples\n\u00b6\n\n\nS3\n\u00b6\n\n\nResources\n:\n\n  \nHelloBucket\n:\n\n    \nType\n:\n \nAWS::S3::Bucket\n  \n# AWS::ProductIdentifier::ResourceType\n\n\n\n\n\n\nEC2\n\u00b6\n\n\nResources\n:\n\n  \nEc2Instance\n:\n\n    \nType\n:\n \nAWS::EC2::Instance\n\n    \nProperties\n:\n\n      \nSecurityGroups\n:\n\n      \n-\n \nRef\n:\n \nInstanceSecurityGroup\n\n      \nKeyName\n:\n \nmykey\n\n      \nImageId\n:\n \n''\n\n  \nInstanceSecurityGroup\n:\n\n    \nType\n:\n \nAWS::EC2::SecurityGroup\n\n    \nProperties\n:\n\n      \nGroupDescription\n:\n \nEnable SSH access via port 22\n\n      \nSecurityGroupIngress\n:\n\n      \n-\n \nIpProtocol\n:\n \ntcp\n\n        \nFromPort\n:\n \n'22'\n\n        \nToPort\n:\n \n'22'\n\n        \nCidrIp\n:\n \n0.0.0.0/0",
            "title": "CloudFormation Basics"
        },
        {
            "location": "/DevOps/CloudFormation/#devops-philosophy",
            "text": "Why we use Terraform and not Chef, Puppet, Ansible, SaltStack, or CloudFormation",
            "title": "DevOps Philosophy"
        },
        {
            "location": "/DevOps/CloudFormation/#tools",
            "text": "AWS CLI cloudformation  aws cloudformation validate-template  boto3 cloudformation",
            "title": "Tools"
        },
        {
            "location": "/DevOps/CloudFormation/#yaml",
            "text": "YAML Cheatsheet  YAML Cheatsheet 2   YAML notation for folded text:  >  data :   > \n    Wrapped text \n    will be folded \n    into a single \n    paragraph \n\n    Blank lines denote \n    paragraph breaks",
            "title": "YAML"
        },
        {
            "location": "/DevOps/CloudFormation/#sample-templates",
            "text": "Templates for the US East (Northern Virginia) Region  AWSlabs on GitHub \n-  Startup kit templates \n-  AWS CloudFormation Sample Templates  Cloudonaut Templates  Free Templates for AWS CloudFormation (Cloudonaut)  Deploying Microservices with Amazon ECS, AWS CloudFormation, and an Application Load Balancer",
            "title": "Sample Templates"
        },
        {
            "location": "/DevOps/CloudFormation/#template-basics",
            "text": "Template Basics  Template Anatomy  ---  AWSTemplateFormatVersion :   \"version   date\"  Description : \n   String  Metadata : \n   template metadata  Parameters : \n   set of parameters  Mappings : \n   set of mappings  Conditions : \n   set of conditions  Transform : \n   set of transforms  Resources : \n   set of resources  Outputs : \n   set of outputs   With examples:  ---  AWSTemplateFormatVersion :   \"2010-09-09\"  Description :   > \n   Here are some \n   details about \n   the template.  Metadata : \n   Instances : \n     Description :   \"Information   about   the   instances\" \n   Databases :  \n     Description :   \"Information   about   the   databases\"  Parameters :  \n   InstanceTypeParameter :  \n     Type :   String              # String, Number, List<Number>, CommaDelimitedList e.g. \"test,dev,prod\", or an AWS-specific types such as Amazon EC2 key pair names and VPC IDs. \n     Default :   t2.micro \n     AllowedValues :  \n       -   t2.micro \n       -   m1.small \n     Description :   Enter t2.micro or m1.small. Default is t2.micro. \n     # AllowedPattern: \"[A-Za-z0-9]+\" # A regular expression that represents the patterns you want to allow for String types. \n     # ConstraintDescription: Malformed input-Parameter MyParameter must match pattern [A-Za-z0-9]+ \n     # MinLength: 2  # for String \n     # MaxLength: 10 \n     # MinValue: 0   # for Number types. \n     # MaxValue: 100  \n     # NoEcho: True  Mappings :  \n   RegionMap :  \n     us-east-1 :  \n       \"32\" :   \"ami-6411e20d\" \n     us-west-1 :  \n       \"32\" :   \"ami-c9c7978c\" \n     eu-west-1 :  \n       \"32\" :   \"ami-37c2f643\" \n     ap-southeast-1 :  \n       \"32\" :   \"ami-66f28c34\" \n     ap-northeast-1 :  \n       \"32\" :   \"ami-9c03a89d\"  Conditions :  \n   CreateProdResources :   !Equals   [   !Ref   EnvType ,   prod   ]  Transform : \n   set of transforms  Resources : \n   Ec2Instance : \n       Type :   AWS::EC2::Instance \n       Properties : \n         InstanceType : \n           Ref :   InstanceTypeParameter    # reference to parameter above \n         ImageId :   ami-2f726546  Outputs :  \n   VolumeId :  \n     Condition :   CreateProdResources \n     Value :  \n       !Ref   NewVolume    The Ref function can refer to input parameters that are specified at stack creation time.",
            "title": "Template Basics"
        },
        {
            "location": "/DevOps/CloudFormation/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/DevOps/CloudFormation/#s3",
            "text": "Resources : \n   HelloBucket : \n     Type :   AWS::S3::Bucket    # AWS::ProductIdentifier::ResourceType",
            "title": "S3"
        },
        {
            "location": "/DevOps/CloudFormation/#ec2",
            "text": "Resources : \n   Ec2Instance : \n     Type :   AWS::EC2::Instance \n     Properties : \n       SecurityGroups : \n       -   Ref :   InstanceSecurityGroup \n       KeyName :   mykey \n       ImageId :   '' \n   InstanceSecurityGroup : \n     Type :   AWS::EC2::SecurityGroup \n     Properties : \n       GroupDescription :   Enable SSH access via port 22 \n       SecurityGroupIngress : \n       -   IpProtocol :   tcp \n         FromPort :   '22' \n         ToPort :   '22' \n         CidrIp :   0.0.0.0/0",
            "title": "EC2"
        },
        {
            "location": "/DevOps/Docker/",
            "text": "Docker Cheatsheet\n\u00b6\n\n\nUseful Links\n\u00b6\n\n\nDocker Cheat Sheet\n\n\nDocker Documentation\n\n\nDocker Tutorials and Labs\n\n\nDocker + Jenkins\n\n\nDocker Hub\n\n\nConcepts\n\u00b6\n\n\nA Docker image is a read-only template. For example, an image could contain an Ubuntu operating system with Apache and your web application installed. Images are used to create Docker containers. Docker provides a simple way to build new images or update existing images, or you can download Docker images that other people have already created. Docker images are the buildcomponent of Docker.\n\n\nDocker registries hold images.\n\n\nCheatsheet\n\u00b6\n\n\nTo show only running containers use:\n\n\n$ docker ps\n\n\n\n\n\nTo show all containers use:\n\n\n$ docker ps -a\n\n\n\n\n\nShow last started container:\n\n\n$ docker ps -l\n\n\n\n\n\nDownload an image:\n\n\n$ docker pull centos\n\n\n\n\n\nCreate then start a container: \ndocker run [OPTIONS] IMAGE [COMMAND] [ARG...]\n\n    * \nDocker run reference\n\n\n$ docker run hello-world\n\n\n\n\n\nRun with interactive terminal (i = interactive t = terminal):\n\n\n$ docker run -it ubuntu bash\n\n\n\n\n\nStart then detach the container (daemonize):\n\n\n$ docker run -d -p8088:80 --name webserver nginx\n\n\n\n\n\nIf you want a transient container, \ndocker run --rm\n will remove the container after it stops.\n\n\nLooks inside the container (use \n-f\n to act like \ntail -f\n):\n\n\n$ docker logs <container name>\n\n\n\n\n\nStop container:\n\n\n$ docker stop <container name>   \n# container ID or name\n\n\n\n\n\n\nDelete container:\n\n\n$ docker rm <container name>\n\n\n\n\n\nTo check the environment:\n\n\n$ docker run -it alpine env \n\n\n\n\n\nDocker version / info:\n\n\n$ docker version\n$ docker info\n\n\n\n\n\nPort Mapping\n\u00b6\n\n\n-p 80:5000\n  would map port 80 on our local host to port 5000 inside our container.\n\n\n$ docker run -d -p \n80\n:5000 training/webapp python app.py\n\n\n\n\n\nFull format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort\n\n\n$ docker run -d -p \n127\n.0.0.1:80:5000 training/webapp python app.py\n\n\n\n\n\nBoth hostPort and containerPort can be specified as a range of ports. When specifying ranges for both, the number of container ports in the range must match the number of host ports in the range, for example: \n-p1234-1236:1234-1236/tcp\n\n\nThe \n-P\n flag tells Docker to map any required network ports inside our container to our host (using random ports).\n\n\n$ docker run -d -P training/webapp python app.py\n\n\n\n\n\nLinking\n\u00b6\n\n\n--link <name or id>:alias\n where \nname\n is the name of the container we\u2019re linking to and \nalias\n is an alias for the link name.\nThe \n--link\n flag also takes the form: \n--link <name or id>\n\n\n$ docker run -d --name myES -p \n9200\n:9200 -p \n9300\n:9300 elasticsearch\n$ docker run --name myK --link myES:elasticsearch  -p \n5601\n:5601 -d docker-kibana-sense\n\n\n\n\n\nNetworks\n\u00b6\n\n\n$ docker network ls\n\n\n\n\n\nFind out the container\u2019s IP address:\n\n\n$ docker network inspect bridge\n\n\n\n\n\nData Volumes\n\u00b6\n\n\nCreate a new volume inside a container at /webapp:\n\n\n$ docker run -d -P --name web -v /webapp training/webapp python app.py\n$ docker inspect web\n\n\n\n\n\nYou can also use the VOLUME instruction in a Dockerfile to add one or more new volumes to any container created from that image.\n\n\nMount the host directory, \n/src/webapp\n, into the container at \n/opt/webapp\n.\n\n\n$ docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n\n\n\n\n\nOn Windows, use:  \ndocker run -v /c/Users/<path>:/<container path> ...\n\n\nExample Dockerfile\n\u00b6\n\n\n$ vim Dockerfile\n\n\n\n\n\nFROM\n docker/whalesay:latest\n\n\nRUN\n apt-get -y update \n&&\n apt-get install -y fortunes\n\nCMD\n /usr/games/fortune -a | cowsay\n\n\n\n\n\n\n$ docker build -t docker-whale .",
            "title": "Docker Cheatsheet"
        },
        {
            "location": "/DevOps/Docker/#docker-cheatsheet",
            "text": "",
            "title": "Docker Cheatsheet"
        },
        {
            "location": "/DevOps/Docker/#useful-links",
            "text": "Docker Cheat Sheet  Docker Documentation  Docker Tutorials and Labs  Docker + Jenkins  Docker Hub",
            "title": "Useful Links"
        },
        {
            "location": "/DevOps/Docker/#concepts",
            "text": "A Docker image is a read-only template. For example, an image could contain an Ubuntu operating system with Apache and your web application installed. Images are used to create Docker containers. Docker provides a simple way to build new images or update existing images, or you can download Docker images that other people have already created. Docker images are the buildcomponent of Docker.  Docker registries hold images.",
            "title": "Concepts"
        },
        {
            "location": "/DevOps/Docker/#cheatsheet",
            "text": "To show only running containers use:  $ docker ps  To show all containers use:  $ docker ps -a  Show last started container:  $ docker ps -l  Download an image:  $ docker pull centos  Create then start a container:  docker run [OPTIONS] IMAGE [COMMAND] [ARG...] \n    *  Docker run reference  $ docker run hello-world  Run with interactive terminal (i = interactive t = terminal):  $ docker run -it ubuntu bash  Start then detach the container (daemonize):  $ docker run -d -p8088:80 --name webserver nginx  If you want a transient container,  docker run --rm  will remove the container after it stops.  Looks inside the container (use  -f  to act like  tail -f ):  $ docker logs <container name>  Stop container:  $ docker stop <container name>    # container ID or name   Delete container:  $ docker rm <container name>  To check the environment:  $ docker run -it alpine env   Docker version / info:  $ docker version\n$ docker info",
            "title": "Cheatsheet"
        },
        {
            "location": "/DevOps/Docker/#port-mapping",
            "text": "-p 80:5000   would map port 80 on our local host to port 5000 inside our container.  $ docker run -d -p  80 :5000 training/webapp python app.py  Full format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort  $ docker run -d -p  127 .0.0.1:80:5000 training/webapp python app.py  Both hostPort and containerPort can be specified as a range of ports. When specifying ranges for both, the number of container ports in the range must match the number of host ports in the range, for example:  -p1234-1236:1234-1236/tcp  The  -P  flag tells Docker to map any required network ports inside our container to our host (using random ports).  $ docker run -d -P training/webapp python app.py",
            "title": "Port Mapping"
        },
        {
            "location": "/DevOps/Docker/#linking",
            "text": "--link <name or id>:alias  where  name  is the name of the container we\u2019re linking to and  alias  is an alias for the link name.\nThe  --link  flag also takes the form:  --link <name or id>  $ docker run -d --name myES -p  9200 :9200 -p  9300 :9300 elasticsearch\n$ docker run --name myK --link myES:elasticsearch  -p  5601 :5601 -d docker-kibana-sense",
            "title": "Linking"
        },
        {
            "location": "/DevOps/Docker/#networks",
            "text": "$ docker network ls  Find out the container\u2019s IP address:  $ docker network inspect bridge",
            "title": "Networks"
        },
        {
            "location": "/DevOps/Docker/#data-volumes",
            "text": "Create a new volume inside a container at /webapp:  $ docker run -d -P --name web -v /webapp training/webapp python app.py\n$ docker inspect web  You can also use the VOLUME instruction in a Dockerfile to add one or more new volumes to any container created from that image.  Mount the host directory,  /src/webapp , into the container at  /opt/webapp .  $ docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py  On Windows, use:   docker run -v /c/Users/<path>:/<container path> ...",
            "title": "Data Volumes"
        },
        {
            "location": "/DevOps/Docker/#example-dockerfile",
            "text": "$ vim Dockerfile  FROM  docker/whalesay:latest  RUN  apt-get -y update  &&  apt-get install -y fortunes CMD  /usr/games/fortune -a | cowsay   $ docker build -t docker-whale .",
            "title": "Example Dockerfile"
        },
        {
            "location": "/DevOps/Git/",
            "text": "Git Cheatsheets\n\u00b6\n\n\n\n\nGraphical git cheatsheet\n\n\nGit basic commands\n\n\nGit cheatsheet (visual)\n\n\nGit cheatsheet (interactive)\n\n\nGit full documentation\n\n\n\n\nRepo hosting:\n\n\n \nBitBucket\n\n\n \nGitHub\n \n\n\nCommon Commands\n\u00b6\n\n\n\n\nCreate a new Git repository in current directory:\n\n\n\n\ngit init\n\n\n\n\n\n\n\nOr create an empty Git repository in the specified directory:\n\n\n\n\ngit init <directory>\n\n\n\n\n\n\n\nOr copy an existing Git repository:\n\n\n\n\ngit clone <repo URL>\n\n\n\n\n\n\n\nClone the repository located at \n into the folder called \n on the local machine:\n\n\n\n\ngit clone <repo> <directory>\ngit clone username@host:/path/to/repository\n\n\n\n\n\n\n\nGlobal Configuration:\n\n\n\n\n$ git config --global user.name \n\"Firstname Lastname\"\n\n$ git config --global user.email \n\"your_email@youremail.com\"\n\n\n\n\n\n\n\n\nStage all changes in \n<file>\n for the next commit:\n\n\n\n\ngit add <file>\n\n\n\n\n\n\n\nOr stage all changes in \n<directory>\n for the next commit:\n\n\n\n\ngit add <directory>  \n# usually '.' for current directory\n\n\n\n\n\n\n\n\nCommit the staged snapshot to the project history:\n\n\n\n\ngit commit  \n# interactive   \n\ngit commit -m \n\"<message>\"\n\n\n\n\n\n\n\n\nOr add and commit all in one:\n\n\n\n\ngit commit -am \n\"message\"\n\n\n\n\n\n\n\n\nFix up the most recent commit (don't do that if shared history):\n\n\n\n\ngit commit --amend\n\n\n\n\n\n\n\nList which files are staged, unstaged, and untracked:\n\n\n\n\ngit status\ngit status -s  \n# short format\n\n\n\n\n\n\n\n\nShow file diff:\n\n\n\n\ngit diff           \n#  git diff by itself doesn\u2019t show all changes made since your last commit \u2013 only changes that are still unstaged.\n\ngit diff --staged  \n#  Shows file differences between staging and the last file version\n\n\n\n\n\n\n\n\nOpen GUI:\n\n\n\n\ngit gui\n\n\n\n\n\n\n\nDisplays committed snapshots:\n\n\n\n\ngit log -n <limit>\ngit log --graph --decorate --oneline\n\n\n\n\n\n\n\nChecking out commits, and checking out branches:\n\n\n\n\ngit checkout <commit>       \n#  Return to commit\n\ngit checkout master         \n#  Return to the master branch (or whatever branch we choose)\n\n\n\n\n\n\n\n\nCheck out a previous version of a file:\n\n\n\n\ngit checkout <commit> <file>    \n#  Check out the version of the file from the selected commit\n\ngit checkout HEAD hello.py      \n#  Check out the most recent version\n\n\n\n\n\n\nBranches\n\u00b6\n\n\nBranches are just pointers to commits.\n\n\n\n\nList all of the branches in your repository.  Also tell you what branch you're currently in ('*' branch):\n\n\n\n\ngit branch\n\n\n\n\n\n\n\nCreate a new branch called \n<branch>\n. \n\n\n\n\ngit branch <branch>\n\n\n\n\n\nThis does not check out the new branch. You need:\n\n\ngit checkout <existing-branch>\n\n\n\n\n\nOr direcly create-and-check out \n<new-branch>\n.\n\n\ngit checkout -b <new-branch>\n\n\n\n\n\n\n\nSafe delete the branch:\n\n\n\n\ngit branch -d <branch>\n\n\n\n\n\n\n\nMerge the specified branch into the current branch:\n\n\n\n\ngit merge <branch>\n\n\n\n\n\n\n\nUndo any undesired changes\n\n\n\n\nGenerate a new commit that undoes all of the changes introduced in \n<commit>\n, then apply it to the current branch\n\n\ngit revert <commit>\n\n\n\n\n\ngit revert\n undoes a single commit \u2014 it does not \u201crevert\u201d back to the previous state of a project by removing all subsequent commits.\n\n\n\n\nReset (dangerous method - erases history):\n\n\n\n\ngit reset\n\n\n\n\n\n\n\nList the remote connections you have to other repositories.\n\n\n\n\ngit remote -v\n\n\n\n\n\n\n\nCreate a new connection / delete a connection to a remote repository.\n\n\n\n\ngit remote add <name> <url>  \n# often \"origin\"\n\ngit remote rm <name>         \n# delete\n\n\n\n\n\n\n\n\nFetch the specified remote\u2019s copy of the current branch and immediately merge it into the local copy. This is the same as \ngit fetch <remote>\n followed by \ngit merge origin/<current-branch>\n.\n\n\n\n\ngit pull <remote>\n\n\n\n\n\n\n\nPut my changes on top of what everybody else has done. Ensure a linear history by preventing unnecessary merge commits.\n\n\n\n\ngit pull --rebase <remote>\n\n\n\n\n\n\n\nTransfer commits from your local repository to a remote repo.\n\n\n\n\ngit push <remote> <branch>\n\n\n\n\n\n\n\nPushes the current branch to the remote server and links the local branch to the remote so next time you can do \ngit pull\n or \ngit push\n.\n\n\n\n\ngit push -u origin <branch>\n\n\n\n\n\nTypical Workflows\n\u00b6\n\n\nClone a Repo\n\u00b6\n\n\n$ mkdir repos\n$ \ncd\n ~/repos\n$ git clone https://<url>\n$ ls -al  <repo dir>\n\n\n\n\n\nAdd a change in the working directory to the staging area\n\u00b6\n\n\n$ git status\n$ git add README\n\n\n\n\n\n-A\n, \n--all\n finds new files as well as staging modified content and removing files that are no longer in the working tree.\n\n\n$ git add -A\n$ git commit -m \n\"Add repo instructions\"\n\n$ git push -u origin master\n$ git pull\n$ ssh -p \n2222\n user@domain.com\n\n\n\n\n\nShort-lived topic branches\n\u00b6\n\n\n\n\nStart a new feature: \n\n\n\n\ngit checkout -b new-feature master\n\n\n\n\n\n\n\nEdit some files:\n\n\n\n\ngit add <file>\ngit commit -m \n\"Start a feature\"\n\n\n\n\n\n\n\n\nEdit some files\n\n\n\n\ngit add <file>\ngit commit -m \n\"Finish a feature\"\n\n\n\n\n\n\n\n\nMerge in the \nnew-feature\n branch\n\n\n\n\ngit checkout master\ngit merge new-feature\ngit branch -d new-feature\n\n\n\n\n\nPush and pull from a centralized repo\n\u00b6\n\n\n\n\nTo push the master branch to the central repo:\n\n\n\n\ngit push origin master\n\n\n\n\n\nIf local history has diverged from the central repository, Git will refuse the request.\n\n\ngit pull --rebase origin master\n\n\n\n\n\nSync my local repo with the remote repo\n\u00b6\n\n\ngit pull origin master\ngit add filename.xyz\ngit commit . -m \u201ccomment\u201d\ngit push origin master\n\n\n\n\n\nCreate a central Repo\n\u00b6\n\n\nThe \n--bare\n flag creates a repository that doesn\u2019t have a working directory, making it impossible to edit files and commit changes in that repository. Central repositories should always be created as bare repositories because pushing branches to a non-bare repository has the potential to overwrite changes.\n\n\n$ git init --bare foobar.git  \n$ git rev-parse --show-toplevel     \n# print top-level directory\n\n$ git rev-parse --git-dir           \n# print .git directory name",
            "title": "Git Cheatsheet"
        },
        {
            "location": "/DevOps/Git/#git-cheatsheets",
            "text": "Graphical git cheatsheet  Git basic commands  Git cheatsheet (visual)  Git cheatsheet (interactive)  Git full documentation   Repo hosting:    BitBucket    GitHub",
            "title": "Git Cheatsheets"
        },
        {
            "location": "/DevOps/Git/#common-commands",
            "text": "Create a new Git repository in current directory:   git init   Or create an empty Git repository in the specified directory:   git init <directory>   Or copy an existing Git repository:   git clone <repo URL>   Clone the repository located at   into the folder called   on the local machine:   git clone <repo> <directory>\ngit clone username@host:/path/to/repository   Global Configuration:   $ git config --global user.name  \"Firstname Lastname\" \n$ git config --global user.email  \"your_email@youremail.com\"    Stage all changes in  <file>  for the next commit:   git add <file>   Or stage all changes in  <directory>  for the next commit:   git add <directory>   # usually '.' for current directory    Commit the staged snapshot to the project history:   git commit   # interactive    \ngit commit -m  \"<message>\"    Or add and commit all in one:   git commit -am  \"message\"    Fix up the most recent commit (don't do that if shared history):   git commit --amend   List which files are staged, unstaged, and untracked:   git status\ngit status -s   # short format    Show file diff:   git diff            #  git diff by itself doesn\u2019t show all changes made since your last commit \u2013 only changes that are still unstaged. \ngit diff --staged   #  Shows file differences between staging and the last file version    Open GUI:   git gui   Displays committed snapshots:   git log -n <limit>\ngit log --graph --decorate --oneline   Checking out commits, and checking out branches:   git checkout <commit>        #  Return to commit \ngit checkout master          #  Return to the master branch (or whatever branch we choose)    Check out a previous version of a file:   git checkout <commit> <file>     #  Check out the version of the file from the selected commit \ngit checkout HEAD hello.py       #  Check out the most recent version",
            "title": "Common Commands"
        },
        {
            "location": "/DevOps/Git/#branches",
            "text": "Branches are just pointers to commits.   List all of the branches in your repository.  Also tell you what branch you're currently in ('*' branch):   git branch   Create a new branch called  <branch> .    git branch <branch>  This does not check out the new branch. You need:  git checkout <existing-branch>  Or direcly create-and-check out  <new-branch> .  git checkout -b <new-branch>   Safe delete the branch:   git branch -d <branch>   Merge the specified branch into the current branch:   git merge <branch>   Undo any undesired changes   Generate a new commit that undoes all of the changes introduced in  <commit> , then apply it to the current branch  git revert <commit>  git revert  undoes a single commit \u2014 it does not \u201crevert\u201d back to the previous state of a project by removing all subsequent commits.   Reset (dangerous method - erases history):   git reset   List the remote connections you have to other repositories.   git remote -v   Create a new connection / delete a connection to a remote repository.   git remote add <name> <url>   # often \"origin\" \ngit remote rm <name>          # delete    Fetch the specified remote\u2019s copy of the current branch and immediately merge it into the local copy. This is the same as  git fetch <remote>  followed by  git merge origin/<current-branch> .   git pull <remote>   Put my changes on top of what everybody else has done. Ensure a linear history by preventing unnecessary merge commits.   git pull --rebase <remote>   Transfer commits from your local repository to a remote repo.   git push <remote> <branch>   Pushes the current branch to the remote server and links the local branch to the remote so next time you can do  git pull  or  git push .   git push -u origin <branch>",
            "title": "Branches"
        },
        {
            "location": "/DevOps/Git/#typical-workflows",
            "text": "",
            "title": "Typical Workflows"
        },
        {
            "location": "/DevOps/Git/#clone-a-repo",
            "text": "$ mkdir repos\n$  cd  ~/repos\n$ git clone https://<url>\n$ ls -al  <repo dir>",
            "title": "Clone a Repo"
        },
        {
            "location": "/DevOps/Git/#add-a-change-in-the-working-directory-to-the-staging-area",
            "text": "$ git status\n$ git add README  -A ,  --all  finds new files as well as staging modified content and removing files that are no longer in the working tree.  $ git add -A\n$ git commit -m  \"Add repo instructions\" \n$ git push -u origin master\n$ git pull\n$ ssh -p  2222  user@domain.com",
            "title": "Add a change in the working directory to the staging area"
        },
        {
            "location": "/DevOps/Git/#short-lived-topic-branches",
            "text": "Start a new feature:    git checkout -b new-feature master   Edit some files:   git add <file>\ngit commit -m  \"Start a feature\"    Edit some files   git add <file>\ngit commit -m  \"Finish a feature\"    Merge in the  new-feature  branch   git checkout master\ngit merge new-feature\ngit branch -d new-feature",
            "title": "Short-lived topic branches"
        },
        {
            "location": "/DevOps/Git/#push-and-pull-from-a-centralized-repo",
            "text": "To push the master branch to the central repo:   git push origin master  If local history has diverged from the central repository, Git will refuse the request.  git pull --rebase origin master",
            "title": "Push and pull from a centralized repo"
        },
        {
            "location": "/DevOps/Git/#sync-my-local-repo-with-the-remote-repo",
            "text": "git pull origin master\ngit add filename.xyz\ngit commit . -m \u201ccomment\u201d\ngit push origin master",
            "title": "Sync my local repo with the remote repo"
        },
        {
            "location": "/DevOps/Git/#create-a-central-repo",
            "text": "The  --bare  flag creates a repository that doesn\u2019t have a working directory, making it impossible to edit files and commit changes in that repository. Central repositories should always be created as bare repositories because pushing branches to a non-bare repository has the potential to overwrite changes.  $ git init --bare foobar.git  \n$ git rev-parse --show-toplevel      # print top-level directory \n$ git rev-parse --git-dir            # print .git directory name",
            "title": "Create a central Repo"
        },
        {
            "location": "/DevOps/Minikube_Install/",
            "text": "Install Minikube\n\u00b6\n\n\nMinikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.\n\n\n\n\nFor Windows, install \nVirtualBox\n or Hyper-V first.\n\n\nMinikube is distributed in binary form: \n GitHub Repo \n. Download the minikube-installer.exe file, and execute the installer. This should automatically add minikube.exe to your path with an uninstaller available as well.\n\n\nIf needed, add \"C:\\Program Files (x86)\\Kubernetes\\minikube\" or similar to the PATH (in System Settings > Environment Variables)\n\n\nTest that minikube works\n\n\n\n\n$ minikube\n\n\n\n\n\nMore info at \nGetting Started\n\n\nInstall kubectl\n\u00b6\n\n\n\n\nInstall kubectl\n\n\n\n\nUse a version of kubectl that is the same version as your server or later. Using an older kubectl with a newer server might produce validation errors.\n\n\nOn Windows 10:\n\n\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/windows/amd64/kubectl.exe\n\n\n\n\n\nOR\n\n\nInstall Choco\n\n\nthen \n\n\nchoco install kubernetes-cli\n\n\n\n\n\nRun kubectl version to verify that the version you\u2019ve installed is sufficiently up-to-date.\n\n\nkubectl version\n\n\n\n\n\nConfigure kubectl\n\u00b6\n\n\nConfigure kubectl to use a remote Kubernetes cluster\n\n\n\n\nIf .kube config does not exist (should be created by minikube): \n\n\n\n\ncd\n C:\n\\u\nsers\n\\y\nourusername\nmkdir .kube\n\ncd\n .kube\nNew-Item config -type file\n\n\n\n\n\n\n\n\n\nEdit the config file with a text editor of your choice.\n\n\n\n\n\n\nCheck that kubectl is properly configured by getting the cluster state:\n\n\n\n\n\n\nkubectl cluster-info\n\n\n\n\n\n\n\nEnable auto-completion\n\n\n\n\necho\n \n\"source <(kubectl completion bash)\"\n >> ~/.bashrc\n\n\n\n\n\n\n\nYou must have appropriate permissions to list, create, edit and delete pods in your cluster. \nYou can verify that you can list these resources by running kubectl auth can-i \n pods.\n\n\n\n\nkubectl auth can-i list pods\nkubectl auth can-i create pods\nkubectl auth can-i edit pods\nkubectl auth can-i delete pods\n\n\n\n\n\nRun Minikube\n\u00b6\n\n\n\n\nInstall \ncurl\n\n\n\n\nchoco install curl\n\n\n# Test\n\ncurl http://google.com \n\n\n\n\n\n$ minikube start\n\n\n# List hosts\n\n$ kubectl get nodes\n\n\n# Test by deploying a container  (creates a deployment / pod automatically)\n\n$ kubectl run hello-minikube --image\n=\nk8s.gcr.io/echoserver:1.4 --port\n=\n8080\n\n\n\n# Provide a dynamic port to the container (creates a service)\n\n$ kubectl expose deployment hello-minikube --type\n=\nNodePort\n\n\n# We have now launched an echoserver pod but we have to wait until the pod is up before curling/accessing it\n\n\n# via the exposed service.\n\n\n# To check whether the pod is up and running we can use the following:\n\n\n$ kubectl get pod\n\n\n# We can see that the pod is now Running and we will now be able to curl it:\n\n$ curl \n$(\nminikube service hello-minikube --url\n)\n\n\n$ kubectl delete deployment hello-minikube\n\n$ kubectl delete service hello-minikube\n\n$ minikube stop\n\n\n\n\n\nInstall \nHelm\n\u00b6\n\n\nHelm is a package manager for Kubernetes\n\n\nDownload a binary release of the Helm client from \nhere\n\n\nOnce you have Helm ready, you can initialize the local CLI and also install Tiller into your Kubernetes cluster in one step:\n\n\n$ helm init\n\n\n\n\n\nThis will install Tiller (the helm server) into the Kubernetes cluster you saw with kubectl config current-context.\n\n\nInstall a test Helm chart\n\n\nhelm repo update              \n# Make sure we get the latest list of charts\n\nhelm install stable/mysql\nhelm ls\nhelm status <release name>\nhelm delete <release name>\n\n\n\n\n\nKubernetes Concepts\n\u00b6\n\n\n\n\nPods - This is the basic unit for all of the workloads you run on Kubernetes. These workloads, such as Deployments and Jobs, are composed of one or more Pods.\nA Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker or rkt), and some shared resources for those containers. Those resources include:\n\n\n\n\nShared storage, as Volumes\nNetworking, as a unique cluster IP address\nInformation about how to run each container, such as the container image version or specific ports to use\n\n\n\n\n\n\nNodes: A Pod always runs on a Node. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the Master. A Node can have multiple pods, and the Kubernetes master automatically handles scheduling the pods across the Nodes in the cluster. \n\n\n\n\n\n\nDeployment - The most common way of running X copies (Pods) of your application. Supports rolling updates to your container images.\n\n\n\n\n\n\nService - By itself, a Deployment can\u2019t receive traffic. Setting up a Service is one of the simplest ways to configure a Deployment to receive and loadbalance requests. Depending on the type of Service used, these requests can come from external client apps or be limited to apps within the same cluster. A Service is tied to a specific Deployment using label selection.\n\n\n\n\n\n\nLabels - Identifying metadata that you can use to sort and select sets of API objects. Labels have many applications, including the following:\n\n\n\n\n\n\nTo keep the right number of replicas (Pods) running in a Deployment. The specified label is used to stamp the Deployment\u2019s newly created Pods (as the value of the spec.template.labels configuration field), and to query which Pods it already manages (as the value of spec.selector.matchLabels).\n\n\n\n\nTo tie a Service to a Deployment using the selector field.\n\n\nTo look for specific subset of Kubernetes objects, when you are using kubectl. For instance, the command kubectl get deployments --selector=app=nginx only displays Deployments from the nginx app.\n\n\n\n\nMinikube / kubectl command examples\n\u00b6\n\n\n\n\nTo access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address:\n\n\n\n\nminikube dashboard\n\n\n\n\n\n\n\n\n\nThe minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the \nminikube ip\n command\n\n\n\n\n\n\nDeploy\n\n\n\n\n\n\nkubectl run hello-world --replicas\n=\n2\n --labels\n=\n\"run=load-balancer-example\"\n --image\n=\ngcr.io/google-samples/node-hello:1.0  --port\n=\n8080\n\n\n\n\n\n\n\n\n\n\nkubectl get\n - list resources. \nkubectl get deployment\n to get all deployments; \nkubectl get pods -l app=nginx\n to get pods with label \"app: nginx\"\n\n\n\n\n\n\nkubectl describe\n - show detailed information about a resource\n\n\n\n\nkubectl logs\n - print the logs from a container in a pod\n\n\nkubectl exec\n - execute a command on a container in a pod\n\n\n\n\nReusing the Docker daemon\n\u00b6\n\n\nWhen using a single VM of Kubernetes, it\u2019s really handy to reuse the minikube\u2019s built-in Docker daemon; as this means you don\u2019t have to build a docker registry on your host machine and push the image into it - you can just build inside the same docker daemon as minikube which speeds up local experiments. Just make sure you tag your Docker image with something other than \u2018latest\u2019 and use that tag while you pull the image. Otherwise, if you do not specify version of your image, it will be assumed as :latest, with pull image policy of Always correspondingly, which may eventually result in ErrImagePull as you may not have any versions of your Docker image out there in the default docker registry (usually DockerHub) yet.\n\n\nTo be able to work with the docker daemon on your mac/linux host use the docker-env command in your shell:\n\n\neval\n \n$(\nminikube docker-env\n)\n\ndocker ps\n\n\n\n\n\nA Docker client is required to publish built docker images to the Docker daemon running inside of minikube. \nSee \ninstalling Docker\n for instructions for your platform.",
            "title": "Install Minikube"
        },
        {
            "location": "/DevOps/Minikube_Install/#install-minikube",
            "text": "Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.   For Windows, install  VirtualBox  or Hyper-V first.  Minikube is distributed in binary form:   GitHub Repo  . Download the minikube-installer.exe file, and execute the installer. This should automatically add minikube.exe to your path with an uninstaller available as well.  If needed, add \"C:\\Program Files (x86)\\Kubernetes\\minikube\" or similar to the PATH (in System Settings > Environment Variables)  Test that minikube works   $ minikube  More info at  Getting Started",
            "title": "Install Minikube"
        },
        {
            "location": "/DevOps/Minikube_Install/#install-kubectl",
            "text": "Install kubectl   Use a version of kubectl that is the same version as your server or later. Using an older kubectl with a newer server might produce validation errors.  On Windows 10:  curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/windows/amd64/kubectl.exe  OR  Install Choco  then   choco install kubernetes-cli  Run kubectl version to verify that the version you\u2019ve installed is sufficiently up-to-date.  kubectl version",
            "title": "Install kubectl"
        },
        {
            "location": "/DevOps/Minikube_Install/#configure-kubectl",
            "text": "Configure kubectl to use a remote Kubernetes cluster   If .kube config does not exist (should be created by minikube):    cd  C: \\u sers \\y ourusername\nmkdir .kube cd  .kube\nNew-Item config -type file    Edit the config file with a text editor of your choice.    Check that kubectl is properly configured by getting the cluster state:    kubectl cluster-info   Enable auto-completion   echo   \"source <(kubectl completion bash)\"  >> ~/.bashrc   You must have appropriate permissions to list, create, edit and delete pods in your cluster. \nYou can verify that you can list these resources by running kubectl auth can-i   pods.   kubectl auth can-i list pods\nkubectl auth can-i create pods\nkubectl auth can-i edit pods\nkubectl auth can-i delete pods",
            "title": "Configure kubectl"
        },
        {
            "location": "/DevOps/Minikube_Install/#run-minikube",
            "text": "Install  curl   choco install curl # Test \ncurl http://google.com   $ minikube start # List hosts \n$ kubectl get nodes # Test by deploying a container  (creates a deployment / pod automatically) \n$ kubectl run hello-minikube --image = k8s.gcr.io/echoserver:1.4 --port = 8080  # Provide a dynamic port to the container (creates a service) \n$ kubectl expose deployment hello-minikube --type = NodePort # We have now launched an echoserver pod but we have to wait until the pod is up before curling/accessing it  # via the exposed service.  # To check whether the pod is up and running we can use the following: \n\n$ kubectl get pod # We can see that the pod is now Running and we will now be able to curl it: \n$ curl  $( minikube service hello-minikube --url ) \n\n$ kubectl delete deployment hello-minikube\n\n$ kubectl delete service hello-minikube\n\n$ minikube stop",
            "title": "Run Minikube"
        },
        {
            "location": "/DevOps/Minikube_Install/#install-helm",
            "text": "Helm is a package manager for Kubernetes  Download a binary release of the Helm client from  here  Once you have Helm ready, you can initialize the local CLI and also install Tiller into your Kubernetes cluster in one step:  $ helm init  This will install Tiller (the helm server) into the Kubernetes cluster you saw with kubectl config current-context.  Install a test Helm chart  helm repo update               # Make sure we get the latest list of charts \nhelm install stable/mysql\nhelm ls\nhelm status <release name>\nhelm delete <release name>",
            "title": "Install Helm"
        },
        {
            "location": "/DevOps/Minikube_Install/#kubernetes-concepts",
            "text": "Pods - This is the basic unit for all of the workloads you run on Kubernetes. These workloads, such as Deployments and Jobs, are composed of one or more Pods.\nA Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker or rkt), and some shared resources for those containers. Those resources include:   Shared storage, as Volumes\nNetworking, as a unique cluster IP address\nInformation about how to run each container, such as the container image version or specific ports to use    Nodes: A Pod always runs on a Node. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the Master. A Node can have multiple pods, and the Kubernetes master automatically handles scheduling the pods across the Nodes in the cluster.     Deployment - The most common way of running X copies (Pods) of your application. Supports rolling updates to your container images.    Service - By itself, a Deployment can\u2019t receive traffic. Setting up a Service is one of the simplest ways to configure a Deployment to receive and loadbalance requests. Depending on the type of Service used, these requests can come from external client apps or be limited to apps within the same cluster. A Service is tied to a specific Deployment using label selection.    Labels - Identifying metadata that you can use to sort and select sets of API objects. Labels have many applications, including the following:    To keep the right number of replicas (Pods) running in a Deployment. The specified label is used to stamp the Deployment\u2019s newly created Pods (as the value of the spec.template.labels configuration field), and to query which Pods it already manages (as the value of spec.selector.matchLabels).   To tie a Service to a Deployment using the selector field.  To look for specific subset of Kubernetes objects, when you are using kubectl. For instance, the command kubectl get deployments --selector=app=nginx only displays Deployments from the nginx app.",
            "title": "Kubernetes Concepts"
        },
        {
            "location": "/DevOps/Minikube_Install/#minikube-kubectl-command-examples",
            "text": "To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address:   minikube dashboard    The minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the  minikube ip  command    Deploy    kubectl run hello-world --replicas = 2  --labels = \"run=load-balancer-example\"  --image = gcr.io/google-samples/node-hello:1.0  --port = 8080     kubectl get  - list resources.  kubectl get deployment  to get all deployments;  kubectl get pods -l app=nginx  to get pods with label \"app: nginx\"    kubectl describe  - show detailed information about a resource   kubectl logs  - print the logs from a container in a pod  kubectl exec  - execute a command on a container in a pod",
            "title": "Minikube / kubectl command examples"
        },
        {
            "location": "/DevOps/Minikube_Install/#reusing-the-docker-daemon",
            "text": "When using a single VM of Kubernetes, it\u2019s really handy to reuse the minikube\u2019s built-in Docker daemon; as this means you don\u2019t have to build a docker registry on your host machine and push the image into it - you can just build inside the same docker daemon as minikube which speeds up local experiments. Just make sure you tag your Docker image with something other than \u2018latest\u2019 and use that tag while you pull the image. Otherwise, if you do not specify version of your image, it will be assumed as :latest, with pull image policy of Always correspondingly, which may eventually result in ErrImagePull as you may not have any versions of your Docker image out there in the default docker registry (usually DockerHub) yet.  To be able to work with the docker daemon on your mac/linux host use the docker-env command in your shell:  eval   $( minikube docker-env ) \ndocker ps  A Docker client is required to publish built docker images to the Docker daemon running inside of minikube. \nSee  installing Docker  for instructions for your platform.",
            "title": "Reusing the Docker daemon"
        },
        {
            "location": "/DevOps/Orchestrator_Scheduler/",
            "text": "Orchestrators / Schedulers\n\u00b6\n\n\nTools to build complex pipelines of batch jobs. They handle dependency resolution, workflow management, visualization.\n\n\nLinks\n\u00b6\n\n\nLuigi vs Airflow vs Pinball\n\n\nAirflow Documentation\n\n\nLuigi\n\n\nPetabyte-Scale Data Pipelines with Docker, Luigi and Elastic Spot Instances\n \n\n\nSnowplow",
            "title": "Orchestrators / Schedulers"
        },
        {
            "location": "/DevOps/Orchestrator_Scheduler/#orchestrators-schedulers",
            "text": "Tools to build complex pipelines of batch jobs. They handle dependency resolution, workflow management, visualization.",
            "title": "Orchestrators / Schedulers"
        },
        {
            "location": "/DevOps/Orchestrator_Scheduler/#links",
            "text": "Luigi vs Airflow vs Pinball  Airflow Documentation  Luigi  Petabyte-Scale Data Pipelines with Docker, Luigi and Elastic Spot Instances    Snowplow",
            "title": "Links"
        },
        {
            "location": "/Java/Java/",
            "text": "Install \nJava\n\u00b6\n\n\nJDK download\n\n\njava -version\n\n\n\n\n\nJava Tools\n\u00b6\n\n\nList 1\n\n\nList 2\n\n\n\n\nEclipse\n IDE\n\n\nMaven\n or Graddle build tool\n\n\nNexus\n private repository\n\n\nMaven public repository\n\n\nPhabrikator\n code review\n\n\nPhabrikator blog\n\n\n\n\n\n\nJenkins\n CI / CD automation server\n\n\nJProfiler\n\n\nFindBugs\n static analysis or \nChecker Framework\n\n\nCheckstyle\n coding standard checker\n\n\nStyle guidelines\n\n\n\n\n\n\n\n\nJava Libraries\n\u00b6\n\n\nLibraries\n\n\n\n\nLog4j\n\n\nSpring\n\n\nSpring Cloud for Amazon Web Services\n\n\nSpring boot code generator\n\n\n\n\n\n\nApache Commons\n\n\nGuava\n\n\nJackson JSON\n or \nGSON\n\n\nHibernate\n\non the JVM. \n\n\nPlay framework\n\n\nSpark web microframework\n\n\nAkka\n - actor model, to build highly concurrent, distributed, and resilient message-driven applications\n\n\n\n\nRandom Snippets\n\u00b6\n\n\nUUID\n.\nrandomUUID\n().\ntoString\n();",
            "title": "Java Pointers"
        },
        {
            "location": "/Java/Java/#install-java",
            "text": "JDK download  java -version",
            "title": "Install Java"
        },
        {
            "location": "/Java/Java/#java-tools",
            "text": "List 1  List 2   Eclipse  IDE  Maven  or Graddle build tool  Nexus  private repository  Maven public repository  Phabrikator  code review  Phabrikator blog    Jenkins  CI / CD automation server  JProfiler  FindBugs  static analysis or  Checker Framework  Checkstyle  coding standard checker  Style guidelines",
            "title": "Java Tools"
        },
        {
            "location": "/Java/Java/#java-libraries",
            "text": "Libraries   Log4j  Spring  Spring Cloud for Amazon Web Services  Spring boot code generator    Apache Commons  Guava  Jackson JSON  or  GSON  Hibernate \non the JVM.   Play framework  Spark web microframework  Akka  - actor model, to build highly concurrent, distributed, and resilient message-driven applications",
            "title": "Java Libraries"
        },
        {
            "location": "/Java/Java/#random-snippets",
            "text": "UUID . randomUUID (). toString ();",
            "title": "Random Snippets"
        },
        {
            "location": "/Java/Log4j/",
            "text": "Apache Log4j 2\n\u00b6\n\n\nhttp://www.tutorialspoint.com/log4j/log4j_quick_guide.htm\nhttp://www.tutorialspoint.com/log4j/index.htm\n\n\nKey Components\n\u00b6\n\n\n\n\nloggers: Responsible for capturing logging information.\n\n\nappenders: Responsible for publishing logging information to various preferred destinations.\n\n\nlayouts: Responsible for formatting logging information in different styles.\n\n\n\n\nThere are seven levels of logging defined within the API: OFF, DEBUG, INFO, ERROR, WARN, FATAL, and ALL.\n\n\nInstall\n\u00b6\n\n\nhttps://logging.apache.org/log4j/2.x/download.html\n\n\n$ gunzip apache-log4j-1.2.15.tar.gz\n$ tar -xvf apache-log4j-1.2.15.tar\n$ \npwd\n\n/usr/local/apache-log4j-1.2.15\n$ \nexport\n \nCLASSPATH\n=\n$CLASSPATH\n:/usr/local/apache-log4j-1.2.15/log4j-1.2.15.jar\n$ \nexport\n \nPATH\n=\n$PATH\n:/usr/local/apache-log4j-1.2.15/\n\n\n\n\n\nMaven Snippet\n\u00b6\n\n\n<dependencies>\n<dependency>\n<groupId>org.apache.logging.log4j</groupId>\n<artifactId>log4j-api</artifactId>\n<version>2.6.1</version>\n</dependency>\n<dependency>\n<groupId>org.apache.logging.log4j</groupId>\n<artifactId>log4j-core</artifactId>\n<version>2.6.1</version>\n</dependency>\n</dependencies>\n\n\n\n\n\nlog4j.properties\n\u00b6\n\n\nAll the libraries should be available in CLASSPATH and yourlog4j.properties file should be available in PATH.\n\n\n# Define the root logger with appender file\n\n\nlog\n \n=\n \n/usr/home/log4j\n\n\nlog4j.rootLogger\n \n=\n \nWARN, FILE\n\n\n\n# Define the file appender\n\n\nlog4j.appender.FILE\n=\norg.apache.log4j.FileAppender\n\n\nlog4j.appender.FILE.File\n=\n${log}/log.out\n\n\n\n# Define the layout for file appender\n\n\nlog4j.appender.FILE.layout\n=\norg.apache.log4j.PatternLayout\n\n\nlog4j.appender.FILE.layout.conversionPattern\n=\n%m%n\n\n\n\n\n\n\nSnippets\n\u00b6\n\n\nimport\n \norg.apache.logging.log4j.LogManager\n;\n\n\nimport\n \norg.apache.logging.log4j.Logger\n;\n\n\n\npublic\n \nclass\n \nMyTest\n \n{\n\n\n    \nprivate\n \nstatic\n \nfinal\n \nLogger\n \nlogger\n \n=\n \nLogManager\n.\ngetLogger\n();\n \n// equiv to  LogManager.getLogger(MyTest.class);\n\n    \nprivate\n \nstatic\n \nfinal\n \nLogger\n \nlogger\n \n=\n \nLogManager\n.\ngetLogger\n(\n\"HelloWorld\"\n);\n\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \n{\n\n         \nlogger\n.\nsetLevel\n(\nLevel\n.\nWARN\n);\n\n         \nlogger\n.\ninfo\n(\n\"Hello, World!\"\n);\n\n         \n// string interpolation\n\n         \nlogger\n.\ndebug\n(\n\"Logging in user {} with birthday {}\"\n,\n \nuser\n.\ngetName\n(),\n \nuser\n.\ngetBirthdayCalendar\n());\n\n\n         \n// pre-Java 8 style optimization: explicitly check the log level\n\n         \n// to make sure the expensiveOperation() method is only called if necessary\n\n         \nif\n \n(\nlogger\n.\nisTraceEnabled\n())\n \n{\n\n        \nlogger\n.\ntrace\n(\n\"Some long-running operation returned {}\"\n,\n \nexpensiveOperation\n());\n\n         \n}\n\n\n         \n// Java-8 style optimization: no need to explicitly check the log level:\n\n         \n// the lambda expression is not evaluated if the TRACE level is not enabledlogger.trace(\"Some long-running operation returned {}\", () -> expensiveOperation());\n\n         \n}\n\n\n}\n\n\n\n// FORMATTER LOGGER\n\n\npublic\n \nstatic\n \nLogger\n \nlogger\n \n=\n \nLogManager\n.\ngetFormatterLogger\n(\n\"Foo\"\n);\n\n\n\nlogger\n.\ndebug\n(\n\"Logging in user %s with birthday %s\"\n,\n \nuser\n.\ngetName\n(),\n \nuser\n.\ngetBirthdayCalendar\n());\n\n\nlogger\n.\ndebug\n(\n\"Logging in user %1$s with birthday %2$tm %2$te,%2$tY\"\n,\n \nuser\n.\ngetName\n(),\n \nuser\n.\ngetBirthdayCalendar\n());\n\n\n//\n\n\nlogger\n.\ndebug\n(\n\"Logging in user {} with birthday {}\"\n,\n \nuser\n.\ngetName\n(),\n \nuser\n.\ngetBirthdayCalendar\n());",
            "title": "Apache Log4j 2"
        },
        {
            "location": "/Java/Log4j/#apache-log4j-2",
            "text": "http://www.tutorialspoint.com/log4j/log4j_quick_guide.htm\nhttp://www.tutorialspoint.com/log4j/index.htm",
            "title": "Apache Log4j 2"
        },
        {
            "location": "/Java/Log4j/#key-components",
            "text": "loggers: Responsible for capturing logging information.  appenders: Responsible for publishing logging information to various preferred destinations.  layouts: Responsible for formatting logging information in different styles.   There are seven levels of logging defined within the API: OFF, DEBUG, INFO, ERROR, WARN, FATAL, and ALL.",
            "title": "Key Components"
        },
        {
            "location": "/Java/Log4j/#install",
            "text": "https://logging.apache.org/log4j/2.x/download.html  $ gunzip apache-log4j-1.2.15.tar.gz\n$ tar -xvf apache-log4j-1.2.15.tar\n$  pwd \n/usr/local/apache-log4j-1.2.15\n$  export   CLASSPATH = $CLASSPATH :/usr/local/apache-log4j-1.2.15/log4j-1.2.15.jar\n$  export   PATH = $PATH :/usr/local/apache-log4j-1.2.15/",
            "title": "Install"
        },
        {
            "location": "/Java/Log4j/#maven-snippet",
            "text": "<dependencies>\n<dependency>\n<groupId>org.apache.logging.log4j</groupId>\n<artifactId>log4j-api</artifactId>\n<version>2.6.1</version>\n</dependency>\n<dependency>\n<groupId>org.apache.logging.log4j</groupId>\n<artifactId>log4j-core</artifactId>\n<version>2.6.1</version>\n</dependency>\n</dependencies>",
            "title": "Maven Snippet"
        },
        {
            "location": "/Java/Log4j/#log4jproperties",
            "text": "All the libraries should be available in CLASSPATH and yourlog4j.properties file should be available in PATH.  # Define the root logger with appender file  log   =   /usr/home/log4j  log4j.rootLogger   =   WARN, FILE  # Define the file appender  log4j.appender.FILE = org.apache.log4j.FileAppender  log4j.appender.FILE.File = ${log}/log.out  # Define the layout for file appender  log4j.appender.FILE.layout = org.apache.log4j.PatternLayout  log4j.appender.FILE.layout.conversionPattern = %m%n",
            "title": "log4j.properties"
        },
        {
            "location": "/Java/Log4j/#snippets",
            "text": "import   org.apache.logging.log4j.LogManager ;  import   org.apache.logging.log4j.Logger ;  public   class   MyTest   { \n\n     private   static   final   Logger   logger   =   LogManager . getLogger ();   // equiv to  LogManager.getLogger(MyTest.class); \n     private   static   final   Logger   logger   =   LogManager . getLogger ( \"HelloWorld\" ); \n\n     public   static   void   main ( String []   args )   { \n          logger . setLevel ( Level . WARN ); \n          logger . info ( \"Hello, World!\" ); \n          // string interpolation \n          logger . debug ( \"Logging in user {} with birthday {}\" ,   user . getName (),   user . getBirthdayCalendar ()); \n\n          // pre-Java 8 style optimization: explicitly check the log level \n          // to make sure the expensiveOperation() method is only called if necessary \n          if   ( logger . isTraceEnabled ())   { \n         logger . trace ( \"Some long-running operation returned {}\" ,   expensiveOperation ()); \n          } \n\n          // Java-8 style optimization: no need to explicitly check the log level: \n          // the lambda expression is not evaluated if the TRACE level is not enabledlogger.trace(\"Some long-running operation returned {}\", () -> expensiveOperation()); \n          }  }  // FORMATTER LOGGER  public   static   Logger   logger   =   LogManager . getFormatterLogger ( \"Foo\" );  logger . debug ( \"Logging in user %s with birthday %s\" ,   user . getName (),   user . getBirthdayCalendar ());  logger . debug ( \"Logging in user %1$s with birthday %2$tm %2$te,%2$tY\" ,   user . getName (),   user . getBirthdayCalendar ());  //  logger . debug ( \"Logging in user {} with birthday {}\" ,   user . getName (),   user . getBirthdayCalendar ());",
            "title": "Snippets"
        },
        {
            "location": "/Linux/Linux/",
            "text": "Vim\n\u00b6\n\n\nVim Commands Cheat Sheet\n\n\n:q\n:q!\n:wq\n:wq {file}\n\n:e[dit] {file}\n\ni  insert\ndd  delete [count] lines\n\n\n\n\n\nBash\n\u00b6\n\n\n\n\nBASH Programming - Introduction\n\n\nBash CheatSheet for UNIX Systems\n\n\nBash Cheat Sheet\n\n\n\n\n#!/bin/bash\n\n\n\nvarname\n=\nvalue\n\necho\n \n$varname\n\n\n\n\n\n\nDon't forget chmod +x filename\n\n\nAmazon Linux\n\u00b6\n\n\nAmazon Linux Basics\n\n\nAdding Packages\n\u00b6\n\n\nsudo yum update -y                  \n# all packages\n\nsudo yum install -y package_name\nsudo yum install -y httpd24 php56 mysql55-server php56-mysqlnd\n\n\n\n\n\nStart a Service\n\u00b6\n\n\nsudo service docker start\nsudo service jenkins start\nsudo service nginx start\n\n\n\n\n\nAutostarting a service on Amazon Linux\n\u00b6\n\n\n\n\nTutorial on \"Chkconfig\" Command in Linux with Examples\n\n\nman page\n\n\n\n\n# check a service is configured for startup\n\nsudo chkconfig sshd\n\necho\n \n$?\n  \n# 0 = configured for startup\n\n\n# or\n\nsudo chkconfig --list mysqld\nsudo chkconfig --list         \n# all services\n\n\n\n# add a service\n\nsudo chkconfig --add vsftpd\nsudo chkconfig mysqld on\nsudo chkconfig --level \n3\n httpd on  \n# specific runlevel\n\n\n\n\n\n\nLinux Boot Process\n\u00b6\n\n\n\n\n Linux Boot Process\n\n\n Scripts in /etc/init.d \n\n\n\n\nYou can also use a \n/etc/rc.d/rc.local\n script.\n\n\nRunning Commands on your Linux Instance at Launch\n\u00b6\n\n\n\n\nPaste a user data script into the \nUser data\n field\n\n\n\n\n#!/bin/bash\n\nyum update -y\nyum install -y httpd24 php56 mysql55-server php56-mysqlnd\nservice httpd start\nchkconfig httpd on\ngroupadd www\nusermod -a -G www ec2-user\nchown -R root:www /var/www\nchmod \n2775\n /var/www\nfind /var/www -type d -exec chmod \n2775\n \n{}\n +\nfind /var/www -type f -exec chmod \n0664\n \n{}\n +\n\necho\n \n\"<?php phpinfo(); ?>\"\n > /var/www/html/phpinfo.php\n\n\n\n\n\n\n\nOr use \ncloud-init\n\n\ncloud-init for AWS EC2\n\n\ncloud-init docs\n\n\n\n\n\n\n\n\nFile location: \n/etc/sysconfig/cloudinit\n\n\nCloud-init output log file: \n/var/log/cloud-init-output.log\n\n\nInstall the SSM Agent on EC2 Instances at Start-Up\n\u00b6\n\n\n#!/bin/bash\n\n\ncd\n /tmp\ncurl https://amazon-ssm-region.s3.amazonaws.com/latest/linux_amd64/amazon-ssm-agent.rpm -o amazon-ssm-agent.rpm\nyum install -y amazon-ssm-agent.rpm\n\n\n\n\n\nLinux desktop\n\u00b6\n\n\nHow can I connect to an Amazon EC2 Linux instance with desktop functionality from Windows?",
            "title": "Linux Cheatsheet"
        },
        {
            "location": "/Linux/Linux/#vim",
            "text": "Vim Commands Cheat Sheet  :q\n:q!\n:wq\n:wq {file}\n\n:e[dit] {file}\n\ni  insert\ndd  delete [count] lines",
            "title": "Vim"
        },
        {
            "location": "/Linux/Linux/#bash",
            "text": "BASH Programming - Introduction  Bash CheatSheet for UNIX Systems  Bash Cheat Sheet   #!/bin/bash  varname = value echo   $varname   Don't forget chmod +x filename",
            "title": "Bash"
        },
        {
            "location": "/Linux/Linux/#amazon-linux",
            "text": "Amazon Linux Basics",
            "title": "Amazon Linux"
        },
        {
            "location": "/Linux/Linux/#adding-packages",
            "text": "sudo yum update -y                   # all packages \nsudo yum install -y package_name\nsudo yum install -y httpd24 php56 mysql55-server php56-mysqlnd",
            "title": "Adding Packages"
        },
        {
            "location": "/Linux/Linux/#start-a-service",
            "text": "sudo service docker start\nsudo service jenkins start\nsudo service nginx start",
            "title": "Start a Service"
        },
        {
            "location": "/Linux/Linux/#autostarting-a-service-on-amazon-linux",
            "text": "Tutorial on \"Chkconfig\" Command in Linux with Examples  man page   # check a service is configured for startup \nsudo chkconfig sshd echo   $?    # 0 = configured for startup  # or \nsudo chkconfig --list mysqld\nsudo chkconfig --list          # all services  # add a service \nsudo chkconfig --add vsftpd\nsudo chkconfig mysqld on\nsudo chkconfig --level  3  httpd on   # specific runlevel",
            "title": "Autostarting a service on Amazon Linux"
        },
        {
            "location": "/Linux/Linux/#linux-boot-process",
            "text": "Linux Boot Process   Scripts in /etc/init.d    You can also use a  /etc/rc.d/rc.local  script.",
            "title": "Linux Boot Process"
        },
        {
            "location": "/Linux/Linux/#running-commands-on-your-linux-instance-at-launch",
            "text": "Paste a user data script into the  User data  field   #!/bin/bash \nyum update -y\nyum install -y httpd24 php56 mysql55-server php56-mysqlnd\nservice httpd start\nchkconfig httpd on\ngroupadd www\nusermod -a -G www ec2-user\nchown -R root:www /var/www\nchmod  2775  /var/www\nfind /var/www -type d -exec chmod  2775   {}  +\nfind /var/www -type f -exec chmod  0664   {}  + echo   \"<?php phpinfo(); ?>\"  > /var/www/html/phpinfo.php   Or use  cloud-init  cloud-init for AWS EC2  cloud-init docs     File location:  /etc/sysconfig/cloudinit  Cloud-init output log file:  /var/log/cloud-init-output.log",
            "title": "Running Commands on your Linux Instance at Launch"
        },
        {
            "location": "/Linux/Linux/#install-the-ssm-agent-on-ec2-instances-at-start-up",
            "text": "#!/bin/bash  cd  /tmp\ncurl https://amazon-ssm-region.s3.amazonaws.com/latest/linux_amd64/amazon-ssm-agent.rpm -o amazon-ssm-agent.rpm\nyum install -y amazon-ssm-agent.rpm",
            "title": "Install the SSM Agent on EC2 Instances at Start-Up"
        },
        {
            "location": "/Linux/Linux/#linux-desktop",
            "text": "How can I connect to an Amazon EC2 Linux instance with desktop functionality from Windows?",
            "title": "Linux desktop"
        },
        {
            "location": "/Markup_and_Documentation/Jekyll/",
            "text": "Jekyll Basics\n\u00b6\n\n\nJekyll Home Page\n\n\nCheck out the \nJekyll docs\n for more info on how to get the most out of Jekyll. File all bugs/feature requests at \nJekyll\u2019s GitHub repo\n. If you have questions, you can ask them on \nJekyll Talk\n.\n\n\nJekyll source code\n\n\nGuide to basic Jekyll\n\n\nJekyll Install How-To\n\u00b6\n\n\nInstall Instructions\n\n\n\n\n\n\nInstall Ruby via \nRubyInstaller\n\n\n\n\n\n\nUpdate RubyGems\n\n\n\n\n\n\n$ gem update --system\n\n\n\n\n\n\n\nInstall Jekyll\n\n\n\n\n$ gem install jekyll\n\n\n\n\n\n\n\nTest Jekyll\n\n\n\n\n$ jekyll --version\n$ gem list jekyll\n\n\n\n\n\n\n\nInstall bundler\n\n\n\n\n$ gem install bundler\n\n\n\n\n\nBundler\n is a gem that manages other Ruby gems. It makes sure your gems and gem versions are compatible, and that you have all necessary dependencies each gem requires.\n\n\n\n\nCreate a new site\n\n\n\n\n# Create a new Jekyll site at ./myblog\n\n~ $ jekyll new myblog\n\n\n# Change into your new directory\n\n~ $ \ncd\n myblog\n\n\n\n\n\nJekyll installs a site that uses a gem-based theme called Minima.\n\n\nWith gem-based themes, some of the site\u2019s directories (such as the assets, _layouts, _includes, and _sass directories) are stored in the theme\u2019s gem, hidden from your immediate view. Yet all of the necessary directories will be read and processed during Jekyll\u2019s build process.\n\n\n\n\nBuild site locally\n\n\n\n\n# Build the site on the preview server\n\n~/myblog $ bundle \nexec\n jekyll serve\n\n\n\n\n\nNow browse to \nlocalhost:4000\n\n\nJekyll Quickstart\n\n\nWhen you run bundle exec jekyll serve, Bundler uses the gems and versions as specified in Gemfile.lock to ensure your Jekyll site builds with no compatibility or dependency conflicts.\n\n\nThe Gemfile and Gemfile.lock files inform Bundler about the gem requirements in your site. If your site doesn\u2019t have these Gemfiles, you can omit bundle exec and just run jekyll serve.\n\n\n$ jekyll build\n\n# => The current folder will be generated into ./_site\n\n\n$ jekyll serve\n\n# => A development server will run at http://localhost:4000/\n\n\n# Auto-regeneration: enabled. Use `--no-watch` to disable.\n\n\n\n\n\n\nPlugins\n\u00b6\n\n\n$ gem install jekyll-sitemap\n$ gem install jekyll-feed\netc...\n\n\n\n\n\nAdd to \n_config.yml\n\n\ngems:\n  - jekyll-paginate\n  - jekyll-feed\n  - jekyll-sitemap\n``\n\n\n# Custom Search\n\n[Adding a custom Google search](http://digitaldrummerj.me/blogging-on-github-part-7-adding-a-custom-google-search/)\n\n\n# Themes\n\n[Theme documentation](https://jekyllrb.com/docs/themes/)\n\nTo change theme, search for jekyll theme on [RubyGems](https://rubygems.org/search?utf8=%E2%9C%93&query=jekyll-theme) to find other gem-based themes.\n\nAdd the theme to your site\u2019s Gemfile:\n\n\n\n\n\ngem \"jekyll-theme-tactile\"\n\n\n```bash\n$ bundle install\n\n# check proper install\n$ bundle show jekyll-theme-tactile\n\n\n\n\n\nAdd the following to your site\u2019s _config.yml to activate the theme:\n\n\ntheme: jekyll-theme-tactile\n\n\n\n\n\nBuild your site:\n\n\n$ bundle \nexec\n jekyll serve\n\n\n\n\n\nYou can find out info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at \njekyllrb.com\n\n\nYou can find the source code for the Jekyll minima theme at:\n\nminima\n\n\n\n\nYou\u2019ll find this post in your \n_posts\n directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run \njekyll serve\n, which launches a web server and auto-regenerates your site when a file is updated.\n\n\nTo add new posts, simply add a file in the \n_posts\n directory that follows the convention \nYYYY-MM-DD-name-of-post.ext\n and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.\n\n\nJekyll also offers powerful support for code snippets:\n\n\n{% highlight ruby %}\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')\n\n\n=> prints 'Hi, Tom' to STDOUT.\n\u00b6\n\n\n{% endhighlight %}",
            "title": "\"Jekyll How-To\""
        },
        {
            "location": "/Markup_and_Documentation/Jekyll/#jekyll-basics",
            "text": "Jekyll Home Page  Check out the  Jekyll docs  for more info on how to get the most out of Jekyll. File all bugs/feature requests at  Jekyll\u2019s GitHub repo . If you have questions, you can ask them on  Jekyll Talk .  Jekyll source code  Guide to basic Jekyll",
            "title": "Jekyll Basics"
        },
        {
            "location": "/Markup_and_Documentation/Jekyll/#jekyll-install-how-to",
            "text": "Install Instructions    Install Ruby via  RubyInstaller    Update RubyGems    $ gem update --system   Install Jekyll   $ gem install jekyll   Test Jekyll   $ jekyll --version\n$ gem list jekyll   Install bundler   $ gem install bundler  Bundler  is a gem that manages other Ruby gems. It makes sure your gems and gem versions are compatible, and that you have all necessary dependencies each gem requires.   Create a new site   # Create a new Jekyll site at ./myblog \n~ $ jekyll new myblog # Change into your new directory \n~ $  cd  myblog  Jekyll installs a site that uses a gem-based theme called Minima.  With gem-based themes, some of the site\u2019s directories (such as the assets, _layouts, _includes, and _sass directories) are stored in the theme\u2019s gem, hidden from your immediate view. Yet all of the necessary directories will be read and processed during Jekyll\u2019s build process.   Build site locally   # Build the site on the preview server \n~/myblog $ bundle  exec  jekyll serve  Now browse to  localhost:4000  Jekyll Quickstart  When you run bundle exec jekyll serve, Bundler uses the gems and versions as specified in Gemfile.lock to ensure your Jekyll site builds with no compatibility or dependency conflicts.  The Gemfile and Gemfile.lock files inform Bundler about the gem requirements in your site. If your site doesn\u2019t have these Gemfiles, you can omit bundle exec and just run jekyll serve.  $ jekyll build # => The current folder will be generated into ./_site \n\n$ jekyll serve # => A development server will run at http://localhost:4000/  # Auto-regeneration: enabled. Use `--no-watch` to disable.",
            "title": "Jekyll Install How-To"
        },
        {
            "location": "/Markup_and_Documentation/Jekyll/#plugins",
            "text": "$ gem install jekyll-sitemap\n$ gem install jekyll-feed\netc...  Add to  _config.yml  gems:\n  - jekyll-paginate\n  - jekyll-feed\n  - jekyll-sitemap\n``\n\n\n# Custom Search\n\n[Adding a custom Google search](http://digitaldrummerj.me/blogging-on-github-part-7-adding-a-custom-google-search/)\n\n\n# Themes\n\n[Theme documentation](https://jekyllrb.com/docs/themes/)\n\nTo change theme, search for jekyll theme on [RubyGems](https://rubygems.org/search?utf8=%E2%9C%93&query=jekyll-theme) to find other gem-based themes.\n\nAdd the theme to your site\u2019s Gemfile:  gem \"jekyll-theme-tactile\"  ```bash\n$ bundle install\n\n# check proper install\n$ bundle show jekyll-theme-tactile  Add the following to your site\u2019s _config.yml to activate the theme:  theme: jekyll-theme-tactile  Build your site:  $ bundle  exec  jekyll serve  You can find out info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at  jekyllrb.com  You can find the source code for the Jekyll minima theme at: minima   You\u2019ll find this post in your  _posts  directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run  jekyll serve , which launches a web server and auto-regenerates your site when a file is updated.  To add new posts, simply add a file in the  _posts  directory that follows the convention  YYYY-MM-DD-name-of-post.ext  and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.  Jekyll also offers powerful support for code snippets:  {% highlight ruby %}\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')",
            "title": "Plugins"
        },
        {
            "location": "/Markup_and_Documentation/Jekyll/#prints-hi-tom-to-stdout",
            "text": "{% endhighlight %}",
            "title": "=&gt; prints 'Hi, Tom' to STDOUT."
        },
        {
            "location": "/Markup_and_Documentation/Markdown/",
            "text": "Markdown Essentials\n\u00b6\n\n\nMarkdown main site\n\n\nGitHub Flavored Markdown Guide\n\n\nBasics\n\u00b6\n\n\nA paragraph is one or more consecutive lines of text separated by one or more blank lines. A blank line contains nothing but spaces or tabs. \n\n\nDo not indent normal paragraphs with spaces or tabs. Indent at least 4 spaces or a tab for code blocks.\n\n\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\n\n\n\n\n\nEmphasis\n\u00b6\n\n\n*single asterisks*\n_single underscores_\n**double asterisks**\n__double underscores__\n\nEmphasis can be used in the mi\\*dd\\*le of a word.\n\n\n\n\n\nHeaders\n\u00b6\n\n\n# H1\n## H2\n### H3\n#### H4\n##### H5\n###### H6\n\nAlt-H1\n======\n\nAlt-H2\n------\n\n\n\n\n\nLinks and Images\n\u00b6\n\n\n[ Text for the link ](URL)\n\nThis is [an example][id] reference-style link.\n[id]: http://example.com/  \"Optional Title Here\"\n\n![Alt text](/path/to/img.jpg \"Optional title\")\n\n\n\n\n\nCode\n\u00b6\n\n\n`span of code`\n\n\n```python\n\ndef wiki_rocks(text):\n    formatter = lambda t: \"funky\"+t\n    return formatter(text)\n```\n\n\n\n\n\nwill be displayed as\n\n\ndef\n \nwiki_rocks\n(\ntext\n):\n\n    \nformatter\n \n=\n \nlambda\n \nt\n:\n \n\"funky\"\n+\nt\n\n    \nreturn\n \nformatter\n(\ntext\n)\n\n\n\n\n\n\nBlockquotes\n\u00b6\n\n\n> This is a blockquote with two paragraphs. \n> \n> Second paragraph.\n\n\n\n\n\nGitHub Pages\n\u00b6\n\n\nGitHub Pages documentation\n \n\n\nGitHub Pages site will use the layout and styles from the Jekyll theme you have selected in your \nrepository settings\n. The name of this theme is saved in the Jekyll \n_config.yml\n configuration file.\n\n\nBitbucket\n\u00b6\n\n\nBitbucket doesn't support arbitrary HTML in Markdown, it instead uses safe mode. \n\nSafe mode\n requires that you replace, remove, or escape HTML tags appropriately.\n\n\nCode highlighting to bitbucket README.md written in Python Markdown \n\n\n    \nfriends\n \n=\n \n[\n'john'\n,\n \n'pat'\n,\n \n'gary'\n,\n \n'michael'\n]\n\n    \nfor\n \ni\n,\n \nname\n \nin\n \nenumerate\n(\nfriends\n):\n\n        \nprint\n \n\"iteration {iteration} is {name}\"\n.\nformat\n(\niteration\n=\ni\n,\n \nname\n=\nname\n)\n\n\n\n\n\n\nPython markdown main site\n \n\n\nCloning your Bitbucket Wiki\n\u00b6\n\n\n$ git clone http://bitbucket.org/MY_USER/MY_REPO/wiki",
            "title": "Markdown Essentials"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#markdown-essentials",
            "text": "Markdown main site  GitHub Flavored Markdown Guide",
            "title": "Markdown Essentials"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#basics",
            "text": "A paragraph is one or more consecutive lines of text separated by one or more blank lines. A blank line contains nothing but spaces or tabs.   Do not indent normal paragraphs with spaces or tabs. Indent at least 4 spaces or a tab for code blocks.  Syntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)",
            "title": "Basics"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#emphasis",
            "text": "*single asterisks*\n_single underscores_\n**double asterisks**\n__double underscores__\n\nEmphasis can be used in the mi\\*dd\\*le of a word.",
            "title": "Emphasis"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#headers",
            "text": "# H1\n## H2\n### H3\n#### H4\n##### H5\n###### H6\n\nAlt-H1\n======\n\nAlt-H2\n------",
            "title": "Headers"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#links-and-images",
            "text": "[ Text for the link ](URL)\n\nThis is [an example][id] reference-style link.\n[id]: http://example.com/  \"Optional Title Here\"\n\n![Alt text](/path/to/img.jpg \"Optional title\")",
            "title": "Links and Images"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#code",
            "text": "`span of code`\n\n\n```python\n\ndef wiki_rocks(text):\n    formatter = lambda t: \"funky\"+t\n    return formatter(text)\n```  will be displayed as  def   wiki_rocks ( text ): \n     formatter   =   lambda   t :   \"funky\" + t \n     return   formatter ( text )",
            "title": "Code"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#blockquotes",
            "text": "> This is a blockquote with two paragraphs. \n> \n> Second paragraph.",
            "title": "Blockquotes"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#github-pages",
            "text": "GitHub Pages documentation    GitHub Pages site will use the layout and styles from the Jekyll theme you have selected in your  repository settings . The name of this theme is saved in the Jekyll  _config.yml  configuration file.",
            "title": "GitHub Pages"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#bitbucket",
            "text": "Bitbucket doesn't support arbitrary HTML in Markdown, it instead uses safe mode.  Safe mode  requires that you replace, remove, or escape HTML tags appropriately.  Code highlighting to bitbucket README.md written in Python Markdown        friends   =   [ 'john' ,   'pat' ,   'gary' ,   'michael' ] \n     for   i ,   name   in   enumerate ( friends ): \n         print   \"iteration {iteration} is {name}\" . format ( iteration = i ,   name = name )   Python markdown main site",
            "title": "Bitbucket"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#cloning-your-bitbucket-wiki",
            "text": "$ git clone http://bitbucket.org/MY_USER/MY_REPO/wiki",
            "title": "Cloning your Bitbucket Wiki"
        },
        {
            "location": "/Markup_and_Documentation/MkDocs/",
            "text": "This website is generated by \nmkdocs.org\n and the \nMaterial Theme\n.\n\n\nBasic MkDocs Commands\n\u00b6\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nInstall and documentation generation\n\u00b6\n\n\nmkdocs.org\n.\n\n\nTo install MkDocs / create a new documentation site:\n\n\n$ pip install mkdocs\n$ mkdocs new documentation\n\n\n\n\n\nTo build the documentation site:\n\n\n$ \ncd\n documentation\n$ mkdocs build\n\n\n\n\n\nTo start the live-reloading docs server - \nhttp://localhost:8000/\n\n\n$ mkdocs serve\n\n\n\n\n\nMkDocs can use the ghp-import tool to commit to the gh-pages branch and push the gh-pages branch to GitHub Pages:\n\n\n$ mkdocs gh-deploy\n\n\n\n\n\nMkDocs project layout\n\u00b6\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "MkDocs Basics"
        },
        {
            "location": "/Markup_and_Documentation/MkDocs/#basic-mkdocs-commands",
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.",
            "title": "Basic MkDocs Commands"
        },
        {
            "location": "/Markup_and_Documentation/MkDocs/#install-and-documentation-generation",
            "text": "mkdocs.org .  To install MkDocs / create a new documentation site:  $ pip install mkdocs\n$ mkdocs new documentation  To build the documentation site:  $  cd  documentation\n$ mkdocs build  To start the live-reloading docs server -  http://localhost:8000/  $ mkdocs serve  MkDocs can use the ghp-import tool to commit to the gh-pages branch and push the gh-pages branch to GitHub Pages:  $ mkdocs gh-deploy",
            "title": "Install and documentation generation"
        },
        {
            "location": "/Markup_and_Documentation/MkDocs/#mkdocs-project-layout",
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "MkDocs project layout"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/",
            "text": "reStructuredText\n\u00b6\n\n\nreStructuredText Quick Ref\n\n\nreStructuredText Cheat Sheet\n  (see below)\n\n\nreST Short Overview\n\u00b6\n\n\nAll reST files use an indentation of 3 spaces; no tabs are allowed. \nThe maximum line length is 80 characters for normal text, but tables, \ndeeply indented code samples and long links may extend beyond that. \nCode example bodies should use normal Python 4-space indentation.\nParagraphs are simply chunks of text separated by one or more blank lines. \nAs in Python, indentation is significant in reST, so all lines of the same\nparagraph must be left-aligned to the same level of indentation.\n\n\nSection headers are created by underlining (and optionally overlining)\nthe section title with a punctuation character, at least as long as the text:\n\n\n=================\nThis is a heading\n=================\n# with overline, for parts\n* with overline, for chapters\n= for sections\n- for subsections\n^ for subsubsections\n\" for paragraphs\n\none asterisk: *text* for emphasis (italics),\ntwo asterisks: **text** for strong emphasis (boldface), and\nbackquotes: ``text`` for code samples.\nescape with a backslash \\\n\n* This is a bulleted list.\n* It has two items, the second\n  item uses two lines.\n\n1. This is a numbered list.\n2. It has two items too.\n\n. This is a numbered list.\n. It has two items too.\n\n\n\n\n\nNested lists are possible, but be aware that they must be separated from the \nparent list items by blank lines\n\n\nSource Code Double Colon\n\u00b6\n\n\nThis is a normal text paragraph. The next paragraph is a code sample::\n\n       It is not processed in any way, except\n       that the indentation is removed.\n\n       It can span multiple lines.\n\nThis is a normal text paragraph again.\n\n\n\n\n\nLinks\n\u00b6\n\n\n`Link text <http://target>`_ for inline web links.\n\n\n\n\n\nDefinitions\n\u00b6\n\n\nterm (up to a line of text)\n   Definition of the term, which must be indented and \n   can even consist of multiple paragraphs\n\nnext term\n   Description.\n\n\n\n\n\nFootnotes\n\u00b6\n\n\nLorem ipsum [#]_ dolor sit amet ... [#]_\n\n\n\n\n\nUse of reStructuredText in Python docstrings\n\u00b6\n\n\nSee http://infinitemonkeycorps.net/docs/pph/\n\n\n    # Typical function documentation: \n\n    :param volume_id: The ID of the EBS volume to be attached.\n    :type volume_id: str\n\n    :param instance_id: The ID of the EC2 instance \n    :type instance_id: str\n\n    :return: `Reverse geocoder return value`_ dictionary giving closest\n        address(es) to `(lat, lng)`\n    :rtype: dict\n    :raises GoogleMapsError: If the coordinates could not be reverse geocoded.\n\n    Keyword arguments and return value are identical to those of :meth:`geocode()`.\n\n    .. _`Reverse geocoder return value`:\n        http://code.google.com/apis/maps/documentation/geocoding/index.html#ReverseGeocoding\n\n\n\n\n\n\n\nNormal docstring formatting conventions apply: see PEP 257.\n\n\nIdentifier references go in `backticks`.\n\n\n:param lat: some text\n  \ndocuments parameters\n\n\n:type lat: float\n \ndocuments parameter types\n\n\n:return:\n dictionary giving some info... \ndocuments return values\n\n\n:rtype: dict\n \ndocuments return type\n\n\n:raises SomeError:\n sometext...  \ndocuments exceptions raised\n\n\n>>>\n \nstarts a doctest and is automatically formatted as code\n\n\nCode can also be indicated by indenting four spaces or preceding with \n::\n and a blank line\n\n\nLink to other methods, functions, classes, modules with :meth:\nmymethod\n, \n\n\n:func:\nmyfunc\n, :class:\nmyclass\n, and :mod:\nmymodule\n.\n\n\nHyperlink names go in backticks with a trailing underscore: \nGoogle\n_\n\n\nTargets can be defined anywhere with: \n.. _Google: http://www.google.com/\n\n\n\n\nExplicit Markup\n\u00b6\n\n\nAn explicit markup block begins with a line starting with .. followed by\nwhitespace and is terminated by the next paragraph at the same level of \nindentation. (There needs to be a blank line between explicit markup\nand normal paragraphs.\n\n\n.. sectionauthor:: Guido van Rossum <guido@python.org>\n\n.. rubric:: Footnotes\n\n.. [#] Text of the first footnote.\n.. [#] Text of the second footnote.\n\n\n:mod:`parrot` -- Dead parrot access\n===================================\n\n.. module:: parrot\n   :platform: Unix, Windows\n   :synopsis: Analyze and reanimate dead parrots.\n.. moduleauthor:: Eric Cleese <eric@python.invalid>\n.. moduleauthor:: John Idle <john@python.invalid>\n\n.. function:: repeat([repeat=3[, number=1000000]])\n              repeat(y, z)\n   :bar: no\n\n   Return a line of text input from the user.\n\n\n.. class:: Spam\n\n      Description of the class.\n\n      .. data:: ham\n\n         Description of the attribute.\n\n\n\n\n\nInline markup\n\u00b6\n\n\n:rolename:`content`  or  :role:`title <target>`\n\n:meth:`~Queue.Queue.get` will refer to Queue.Queue.get but only display get as the link text.\n\n\n\n\n\nThe following roles refer to objects in modules and are possibly hyperlinked \nif a matching identifier is found:\n\n\nmod\n\n\nThe name of a module; a dotted name may be used. This should also be used for package names.\n\n\nfunc\n\n\nThe name of a Python function; dotted names may be used. The role text should not include trailing parentheses to enhance readability. The parentheses are stripped when searching for identifiers.\n\n\ndata\n\n\nThe name of a module-level variable or constant.\n\n\nconst\n\n\nThe name of a \u201cdefined\u201d constant. This may be a C-language #define or a Python variable that is not intended to be changed.\n\n\nclass\n\n\nA class name; a dotted name may be used.\n\n\nmeth\n\n\nThe name of a method of an object. The role text should include the type name and the method name. A dotted name may be used.\n\n\nattr\n\n\nThe name of a data attribute of an object.\n\n\nexc\n\n\nThe name of an exception. A dotted name may be used.\n\n\nOfficial reStructuredText Cheatsheet\n\u00b6\n\n\n=====================================================\n The reStructuredText_ Cheat Sheet: Syntax Reminders\n=====================================================\n:Info: See <http://docutils.sf.net/rst.html> for introductory docs.\n:Author: David Goodger <goodger@python.org>\n:Date: $Date: 2013-02-20 01:10:53 +0000 (Wed, 20 Feb 2013) $\n:Revision: $Revision: 7612 $\n:Description: This is a \"docinfo block\", or bibliographic field list\n\n.. NOTE:: If you are reading this as HTML, please read\n   `<cheatsheet.txt>`_ instead to see the input syntax examples!\n\nSection Structure\n=================\nSection titles are underlined or overlined & underlined.\n\nBody Elements\n=============\nGrid table:\n\n+--------------------------------+-----------------------------------+\n| Paragraphs are flush-left,     | Literal block, preceded by \"::\":: |\n| separated by blank lines.      |                                   |\n|                                |     Indented                      |\n|     Block quotes are indented. |                                   |\n+--------------------------------+ or::                              |\n| >>> print 'Doctest block'      |                                   |\n| Doctest block                  | > Quoted                          |\n+--------------------------------+-----------------------------------+\n| | Line blocks preserve line breaks & indents. [new in 0.3.6]       |\n| |     Useful for addresses, verse, and adornment-free lists; long  |\n|       lines can be wrapped with continuation lines.                |\n+--------------------------------------------------------------------+\n\nSimple tables:\n\n================  ============================================================\nList Type         Examples (syntax in the `text source <cheatsheet.txt>`_)\n================  ============================================================\nBullet list       * items begin with \"-\", \"+\", or \"*\"\nEnumerated list   1. items use any variation of \"1.\", \"A)\", and \"(i)\"\n                  #. also auto-enumerated\nDefinition list   Term is flush-left : optional classifier\n                      Definition is indented, no blank line between\nField list        :field name: field body\nOption list       -o  at least 2 spaces between option & description\n================  ============================================================\n\n================  ============================================================\nExplicit Markup   Examples (visible in the `text source`_)\n================  ============================================================\nFootnote          .. [1] Manually numbered or [#] auto-numbered\n                     (even [#labelled]) or [*] auto-symbol\nCitation          .. [CIT2002] A citation.\nHyperlink Target  .. _reStructuredText: http://docutils.sf.net/rst.html\n                  .. _indirect target: reStructuredText_\n                  .. _internal target:\nAnonymous Target  __ http://docutils.sf.net/docs/ref/rst/restructuredtext.html\nDirective (\"::\")  .. image:: images/biohazard.png\nSubstitution Def  .. |substitution| replace:: like an inline directive\nComment           .. is anything else\nEmpty Comment     (\"..\" on a line by itself, with blank lines before & after,\n                  used to separate indentation contexts)\n================  ============================================================\n\nInline Markup\n=============\n*emphasis*; **strong emphasis**; `interpreted text`; `interpreted text\nwith role`:emphasis:; ``inline literal text``; standalone hyperlink,\nhttp://docutils.sourceforge.net; named reference, reStructuredText_;\n`anonymous reference`__; footnote reference, [1]_; citation reference,\n[CIT2002]_; |substitution|; _`inline internal target`.\n\f\nDirective Quick Reference\n=========================\nSee <http://docutils.sf.net/docs/ref/rst/directives.html> for full info.\n\n================  ============================================================\nDirective Name    Description (Docutils version added to, in [brackets])\n================  ============================================================\nattention         Specific admonition; also \"caution\", \"danger\",\n                  \"error\", \"hint\", \"important\", \"note\", \"tip\", \"warning\"\nadmonition        Generic titled admonition: ``.. admonition:: By The Way``\nimage             ``.. image:: picture.png``; many options possible\nfigure            Like \"image\", but with optional caption and legend\ntopic             ``.. topic:: Title``; like a mini section\nsidebar           ``.. sidebar:: Title``; like a mini parallel document\nparsed-literal    A literal block with parsed inline markup\nrubric            ``.. rubric:: Informal Heading``\nepigraph          Block quote with class=\"epigraph\"\nhighlights        Block quote with class=\"highlights\"\npull-quote        Block quote with class=\"pull-quote\"\ncompound          Compound paragraphs [0.3.6]\ncontainer         Generic block-level container element [0.3.10]\ntable             Create a titled table [0.3.1]\nlist-table        Create a table from a uniform two-level bullet list [0.3.8]\ncsv-table         Create a table from CSV data [0.3.4]\ncontents          Generate a table of contents\nsectnum           Automatically number sections, subsections, etc.\nheader, footer    Create document decorations [0.3.8]\ntarget-notes      Create an explicit footnote for each external target\nmath              Mathematical notation (input in LaTeX format)\nmeta              HTML-specific metadata\ninclude           Read an external reST file as if it were inline\nraw               Non-reST data passed untouched to the Writer\nreplace           Replacement text for substitution definitions\nunicode           Unicode character code conversion for substitution defs\ndate              Generates today's date; for substitution defs\nclass             Set a \"class\" attribute on the next element\nrole              Create a custom interpreted text role [0.3.2]\ndefault-role      Set the default interpreted text role [0.3.10]\ntitle             Set the metadata document title [0.3.10]\n================  ============================================================\n\nInterpreted Text Role Quick Reference\n=====================================\nSee <http://docutils.sf.net/docs/ref/rst/roles.html> for full info.\n\n================  ============================================================\nRole Name         Description\n================  ============================================================\nemphasis          Equivalent to *emphasis*\nliteral           Equivalent to ``literal`` but processes backslash escapes\nmath              Mathematical notation (input in LaTeX format)\nPEP               Reference to a numbered Python Enhancement Proposal\nRFC               Reference to a numbered Internet Request For Comments\nraw               For non-reST data; cannot be used directly (see docs) [0.3.6]\nstrong            Equivalent to **strong**\nsub               Subscript\nsup               Superscript\ntitle             Title reference (book, etc.); standard default role\n================  ============================================================",
            "title": "reStructuredText Cheatsheet"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#restructuredtext",
            "text": "reStructuredText Quick Ref  reStructuredText Cheat Sheet   (see below)",
            "title": "reStructuredText"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#rest-short-overview",
            "text": "All reST files use an indentation of 3 spaces; no tabs are allowed. \nThe maximum line length is 80 characters for normal text, but tables, \ndeeply indented code samples and long links may extend beyond that. \nCode example bodies should use normal Python 4-space indentation.\nParagraphs are simply chunks of text separated by one or more blank lines. \nAs in Python, indentation is significant in reST, so all lines of the same\nparagraph must be left-aligned to the same level of indentation.  Section headers are created by underlining (and optionally overlining)\nthe section title with a punctuation character, at least as long as the text:  =================\nThis is a heading\n=================\n# with overline, for parts\n* with overline, for chapters\n= for sections\n- for subsections\n^ for subsubsections\n\" for paragraphs\n\none asterisk: *text* for emphasis (italics),\ntwo asterisks: **text** for strong emphasis (boldface), and\nbackquotes: ``text`` for code samples.\nescape with a backslash \\\n\n* This is a bulleted list.\n* It has two items, the second\n  item uses two lines.\n\n1. This is a numbered list.\n2. It has two items too.\n\n. This is a numbered list.\n. It has two items too.  Nested lists are possible, but be aware that they must be separated from the \nparent list items by blank lines",
            "title": "reST Short Overview"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#source-code-double-colon",
            "text": "This is a normal text paragraph. The next paragraph is a code sample::\n\n       It is not processed in any way, except\n       that the indentation is removed.\n\n       It can span multiple lines.\n\nThis is a normal text paragraph again.",
            "title": "Source Code Double Colon"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#links",
            "text": "`Link text <http://target>`_ for inline web links.",
            "title": "Links"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#definitions",
            "text": "term (up to a line of text)\n   Definition of the term, which must be indented and \n   can even consist of multiple paragraphs\n\nnext term\n   Description.",
            "title": "Definitions"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#footnotes",
            "text": "Lorem ipsum [#]_ dolor sit amet ... [#]_",
            "title": "Footnotes"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#use-of-restructuredtext-in-python-docstrings",
            "text": "See http://infinitemonkeycorps.net/docs/pph/      # Typical function documentation: \n\n    :param volume_id: The ID of the EBS volume to be attached.\n    :type volume_id: str\n\n    :param instance_id: The ID of the EC2 instance \n    :type instance_id: str\n\n    :return: `Reverse geocoder return value`_ dictionary giving closest\n        address(es) to `(lat, lng)`\n    :rtype: dict\n    :raises GoogleMapsError: If the coordinates could not be reverse geocoded.\n\n    Keyword arguments and return value are identical to those of :meth:`geocode()`.\n\n    .. _`Reverse geocoder return value`:\n        http://code.google.com/apis/maps/documentation/geocoding/index.html#ReverseGeocoding   Normal docstring formatting conventions apply: see PEP 257.  Identifier references go in `backticks`.  :param lat: some text    documents parameters  :type lat: float   documents parameter types  :return:  dictionary giving some info...  documents return values  :rtype: dict   documents return type  :raises SomeError:  sometext...   documents exceptions raised  >>>   starts a doctest and is automatically formatted as code  Code can also be indicated by indenting four spaces or preceding with  ::  and a blank line  Link to other methods, functions, classes, modules with :meth: mymethod ,   :func: myfunc , :class: myclass , and :mod: mymodule .  Hyperlink names go in backticks with a trailing underscore:  Google _  Targets can be defined anywhere with:  .. _Google: http://www.google.com/",
            "title": "Use of reStructuredText in Python docstrings"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#explicit-markup",
            "text": "An explicit markup block begins with a line starting with .. followed by\nwhitespace and is terminated by the next paragraph at the same level of \nindentation. (There needs to be a blank line between explicit markup\nand normal paragraphs.  .. sectionauthor:: Guido van Rossum <guido@python.org>\n\n.. rubric:: Footnotes\n\n.. [#] Text of the first footnote.\n.. [#] Text of the second footnote.\n\n\n:mod:`parrot` -- Dead parrot access\n===================================\n\n.. module:: parrot\n   :platform: Unix, Windows\n   :synopsis: Analyze and reanimate dead parrots.\n.. moduleauthor:: Eric Cleese <eric@python.invalid>\n.. moduleauthor:: John Idle <john@python.invalid>\n\n.. function:: repeat([repeat=3[, number=1000000]])\n              repeat(y, z)\n   :bar: no\n\n   Return a line of text input from the user.\n\n\n.. class:: Spam\n\n      Description of the class.\n\n      .. data:: ham\n\n         Description of the attribute.",
            "title": "Explicit Markup"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#inline-markup",
            "text": ":rolename:`content`  or  :role:`title <target>`\n\n:meth:`~Queue.Queue.get` will refer to Queue.Queue.get but only display get as the link text.  The following roles refer to objects in modules and are possibly hyperlinked \nif a matching identifier is found:  mod  The name of a module; a dotted name may be used. This should also be used for package names.  func  The name of a Python function; dotted names may be used. The role text should not include trailing parentheses to enhance readability. The parentheses are stripped when searching for identifiers.  data  The name of a module-level variable or constant.  const  The name of a \u201cdefined\u201d constant. This may be a C-language #define or a Python variable that is not intended to be changed.  class  A class name; a dotted name may be used.  meth  The name of a method of an object. The role text should include the type name and the method name. A dotted name may be used.  attr  The name of a data attribute of an object.  exc  The name of an exception. A dotted name may be used.",
            "title": "Inline markup"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#official-restructuredtext-cheatsheet",
            "text": "=====================================================\n The reStructuredText_ Cheat Sheet: Syntax Reminders\n=====================================================\n:Info: See <http://docutils.sf.net/rst.html> for introductory docs.\n:Author: David Goodger <goodger@python.org>\n:Date: $Date: 2013-02-20 01:10:53 +0000 (Wed, 20 Feb 2013) $\n:Revision: $Revision: 7612 $\n:Description: This is a \"docinfo block\", or bibliographic field list\n\n.. NOTE:: If you are reading this as HTML, please read\n   `<cheatsheet.txt>`_ instead to see the input syntax examples!\n\nSection Structure\n=================\nSection titles are underlined or overlined & underlined.\n\nBody Elements\n=============\nGrid table:\n\n+--------------------------------+-----------------------------------+\n| Paragraphs are flush-left,     | Literal block, preceded by \"::\":: |\n| separated by blank lines.      |                                   |\n|                                |     Indented                      |\n|     Block quotes are indented. |                                   |\n+--------------------------------+ or::                              |\n| >>> print 'Doctest block'      |                                   |\n| Doctest block                  | > Quoted                          |\n+--------------------------------+-----------------------------------+\n| | Line blocks preserve line breaks & indents. [new in 0.3.6]       |\n| |     Useful for addresses, verse, and adornment-free lists; long  |\n|       lines can be wrapped with continuation lines.                |\n+--------------------------------------------------------------------+\n\nSimple tables:\n\n================  ============================================================\nList Type         Examples (syntax in the `text source <cheatsheet.txt>`_)\n================  ============================================================\nBullet list       * items begin with \"-\", \"+\", or \"*\"\nEnumerated list   1. items use any variation of \"1.\", \"A)\", and \"(i)\"\n                  #. also auto-enumerated\nDefinition list   Term is flush-left : optional classifier\n                      Definition is indented, no blank line between\nField list        :field name: field body\nOption list       -o  at least 2 spaces between option & description\n================  ============================================================\n\n================  ============================================================\nExplicit Markup   Examples (visible in the `text source`_)\n================  ============================================================\nFootnote          .. [1] Manually numbered or [#] auto-numbered\n                     (even [#labelled]) or [*] auto-symbol\nCitation          .. [CIT2002] A citation.\nHyperlink Target  .. _reStructuredText: http://docutils.sf.net/rst.html\n                  .. _indirect target: reStructuredText_\n                  .. _internal target:\nAnonymous Target  __ http://docutils.sf.net/docs/ref/rst/restructuredtext.html\nDirective (\"::\")  .. image:: images/biohazard.png\nSubstitution Def  .. |substitution| replace:: like an inline directive\nComment           .. is anything else\nEmpty Comment     (\"..\" on a line by itself, with blank lines before & after,\n                  used to separate indentation contexts)\n================  ============================================================\n\nInline Markup\n=============\n*emphasis*; **strong emphasis**; `interpreted text`; `interpreted text\nwith role`:emphasis:; ``inline literal text``; standalone hyperlink,\nhttp://docutils.sourceforge.net; named reference, reStructuredText_;\n`anonymous reference`__; footnote reference, [1]_; citation reference,\n[CIT2002]_; |substitution|; _`inline internal target`.\n\f\nDirective Quick Reference\n=========================\nSee <http://docutils.sf.net/docs/ref/rst/directives.html> for full info.\n\n================  ============================================================\nDirective Name    Description (Docutils version added to, in [brackets])\n================  ============================================================\nattention         Specific admonition; also \"caution\", \"danger\",\n                  \"error\", \"hint\", \"important\", \"note\", \"tip\", \"warning\"\nadmonition        Generic titled admonition: ``.. admonition:: By The Way``\nimage             ``.. image:: picture.png``; many options possible\nfigure            Like \"image\", but with optional caption and legend\ntopic             ``.. topic:: Title``; like a mini section\nsidebar           ``.. sidebar:: Title``; like a mini parallel document\nparsed-literal    A literal block with parsed inline markup\nrubric            ``.. rubric:: Informal Heading``\nepigraph          Block quote with class=\"epigraph\"\nhighlights        Block quote with class=\"highlights\"\npull-quote        Block quote with class=\"pull-quote\"\ncompound          Compound paragraphs [0.3.6]\ncontainer         Generic block-level container element [0.3.10]\ntable             Create a titled table [0.3.1]\nlist-table        Create a table from a uniform two-level bullet list [0.3.8]\ncsv-table         Create a table from CSV data [0.3.4]\ncontents          Generate a table of contents\nsectnum           Automatically number sections, subsections, etc.\nheader, footer    Create document decorations [0.3.8]\ntarget-notes      Create an explicit footnote for each external target\nmath              Mathematical notation (input in LaTeX format)\nmeta              HTML-specific metadata\ninclude           Read an external reST file as if it were inline\nraw               Non-reST data passed untouched to the Writer\nreplace           Replacement text for substitution definitions\nunicode           Unicode character code conversion for substitution defs\ndate              Generates today's date; for substitution defs\nclass             Set a \"class\" attribute on the next element\nrole              Create a custom interpreted text role [0.3.2]\ndefault-role      Set the default interpreted text role [0.3.10]\ntitle             Set the metadata document title [0.3.10]\n================  ============================================================\n\nInterpreted Text Role Quick Reference\n=====================================\nSee <http://docutils.sf.net/docs/ref/rst/roles.html> for full info.\n\n================  ============================================================\nRole Name         Description\n================  ============================================================\nemphasis          Equivalent to *emphasis*\nliteral           Equivalent to ``literal`` but processes backslash escapes\nmath              Mathematical notation (input in LaTeX format)\nPEP               Reference to a numbered Python Enhancement Proposal\nRFC               Reference to a numbered Internet Request For Comments\nraw               For non-reST data; cannot be used directly (see docs) [0.3.6]\nstrong            Equivalent to **strong**\nsub               Subscript\nsup               Superscript\ntitle             Title reference (book, etc.); standard default role\n================  ============================================================",
            "title": "Official reStructuredText Cheatsheet"
        },
        {
            "location": "/Microservices/Microservices/",
            "text": "MicroServices\n\u00b6\n\n\n microservices.io \n\n\n Introduction to microservices \n\n\nScala\n\u00b6\n\n\nLagom\n \n\n\n.NET\n\u00b6\n\n\nMicroservices in C#",
            "title": "Microservices"
        },
        {
            "location": "/Microservices/Microservices/#microservices",
            "text": "microservices.io    Introduction to microservices",
            "title": "MicroServices"
        },
        {
            "location": "/Microservices/Microservices/#scala",
            "text": "Lagom",
            "title": "Scala"
        },
        {
            "location": "/Microservices/Microservices/#net",
            "text": "Microservices in C#",
            "title": ".NET"
        },
        {
            "location": "/Python/Jupyter/",
            "text": "IPython\n / \nJupyter\n\u00b6\n\n\n\n\nUsing IPython makes interactive work easy.\n\n\nBetter shell\n\n\nNotebook interface\n\n\nEmbeddable kernel\n\n\nParallel python\n\n\n\n\n\n\n\n\nIPython shell shortcuts\n\u00b6\n\n\n\n\nTAB expansion to complete python names and file paths\n\n\n~ and * directory / file expansion\n\n\nmany \"magic\" methods:\n\n\n\n\n%lsmagic                # list of all magic methods\n%quickref               # cheatsheet\n%magic\n\n\n\n\n\nHelp\n\u00b6\n\n\n?                       # overall help\nhelp                    # python help system\n?someobj or someobj?    # help\n??someobj or someobj??  # detailed help\n\n\n\n\n\n%pdoc\n \n%pdef\n \n%psource\n  for docstring, function definition, source code only.\n\n\nRun\n\u00b6\n\n\nTo run a program directly from the IPython console:\n\n\n%run somescript.py      # instead of execfile(\"somescript.py\") at the python prompt\n\n\n\n\n\n%run\n has special flags for timing the execution of your scripts (\n-t\n) or for running them under the control of either Python's pdb debugger (\n-d\n) or profiler (\n-p\n):\n\n\n%run -d myscript.py\n\n\n\n\n\nOther Commands\n\u00b6\n\n\n%edit %ed               # edit then execute\n%save\n%load example.py        # load local (example) file (or url) allowing modification\n%load http://matplotlib.org/plot_directive/mpl_examples/mplot3d/contour3d_demo.py\n%macro                  # define macro with range of history lines, filenames or string objects\n%recall\n\n%whos                   # list identifiers you have defined interactively\n%reset  -f -s           # remove objects -f for force -s for soft (leaves history).\n\n\n\n\n\n\n\n%reset\n is not a kernel restart\n\n\nRestart with \nCtrl+.\n in \"qtconsole\"\n\n\nimport module ; reload(module)\n to reload a module from disk\n\n\n\n\nDebugging\n\u00b6\n\n\n%debug                  # jump into the Python debugger (pdb)\n%pdb                    # start the debugger on any uncaught exception.\n\n%cd                     # change directory\n%pwd                    # print working directory\n%env                    # OS environment variables\n\n\n\n\n\nOS Commands\n\u00b6\n\n\n!OScommand\n!ping www.bbc.co.uk\n%alias                  # system command alias\n\n\n\n\n\nHistory\n\u00b6\n\n\n_ __ ___                # etc... for previous outputs.\n_i _ii _i4              # etc.. for previous input. _ih for list of previous inputs\n\n\n\n\n\nGUI integration\n\u00b6\n\n\nStart with \nipython --gui=qt\n or at the IPython prompt:\n\n\n%gui wx\n\n\n\n\n\nArguments can be \nwx\n, \nqt\n, \ngtk\n and \ntk\n.\n\n\nMatplotlib / pylab graphics in an iPython shell\n\u00b6\n\n\nStart with: \nipython --matplotlib\n ( or \n--matplotlib=qt\n etc...)\n\n\nAt the IPython prompt:\n\n\n%matplotlib             # set matplotlib to work interactively; does not import anythig\n%matplotlib  inline\n%matplotlib qt          # request a specific GUI backend\n%pylab inline\n\n\n\n\n\n%pylab\n makes the following imports:\n\n\nimport\n \nnumpy\n\n\nimport\n \nmatplotlib\n\n\nfrom\n \nmatplotlib\n \nimport\n \npylab\n,\n \nmlab\n,\n \npyplot\n\n\nnp\n \n=\n \nnumpy\n\n\nplt\n \n=\n \npyplot\n\n\nfrom\n \nIPython.display\n \nimport\n \ndisplay\n\n\nfrom\n \nIPython.core.pylabtools\n \nimport\n \nfigsize\n,\n \ngetfigs\n\n\nfrom\n \npylab\n \nimport\n \n*\n\n\nfrom\n \nnumpy\n \nimport\n \n*\n\n\n\n\n\n\nQtconsole - an improved console\n\u00b6\n\n\nAt the command prompt:\n\n\nipython.exe qtconsole --pylab=inline --ConsoleWidget.font_size=10\n\n\n\n\n\nalternative: --matplotlib inline\nor within IPython:\n\n\n%matplotlib  inline\n%pylab inline\n\n\n\n\n\nTo embed plots, SVG or HTML in qtconsole, call display:\n\n\nfrom IPython.core.display import display, display_html\nfrom IPython.core.display import display_png, display_svg\ndisplay(plt.gcf()) # embeds the current figure in the qtconsole\ndisplay(*getfigs()) # embeds all active figures in the qtconsole\n#or:\nf = plt.figure()\nplt.plot(np.rand(100))\ndisplay(f)\n\n\n\n\n\nipython and ipython notebook for matlab users\n\n\nIPython Notebook web-based interface\n\u00b6\n\n\n\n\nStart with: ipython notebook and switch to browser\n\n\nKeyboard shortcuts:\n\n\nEnter\n to edit a cell\n\n\nShift + Enter\n to evaluate\n\n\nCtrl + m\n or \nEsc\n for the \"command mode\"\n\n\n\n\n\n\n\n\nIn command mode:\n\n\n h list of keyboard shortcuts\n 1-6 to convert to heading cell\n m to convert to markdown cell\n y to convert to code\n c copy / v paste\n d d delete cell\n s save notebook\n . to restart kernel",
            "title": "IPython / Jupyter Cheatsheet"
        },
        {
            "location": "/Python/Jupyter/#ipython-jupyter",
            "text": "Using IPython makes interactive work easy.  Better shell  Notebook interface  Embeddable kernel  Parallel python",
            "title": "IPython / Jupyter"
        },
        {
            "location": "/Python/Jupyter/#ipython-shell-shortcuts",
            "text": "TAB expansion to complete python names and file paths  ~ and * directory / file expansion  many \"magic\" methods:   %lsmagic                # list of all magic methods\n%quickref               # cheatsheet\n%magic",
            "title": "IPython shell shortcuts"
        },
        {
            "location": "/Python/Jupyter/#help",
            "text": "?                       # overall help\nhelp                    # python help system\n?someobj or someobj?    # help\n??someobj or someobj??  # detailed help  %pdoc   %pdef   %psource   for docstring, function definition, source code only.",
            "title": "Help"
        },
        {
            "location": "/Python/Jupyter/#run",
            "text": "To run a program directly from the IPython console:  %run somescript.py      # instead of execfile(\"somescript.py\") at the python prompt  %run  has special flags for timing the execution of your scripts ( -t ) or for running them under the control of either Python's pdb debugger ( -d ) or profiler ( -p ):  %run -d myscript.py",
            "title": "Run"
        },
        {
            "location": "/Python/Jupyter/#other-commands",
            "text": "%edit %ed               # edit then execute\n%save\n%load example.py        # load local (example) file (or url) allowing modification\n%load http://matplotlib.org/plot_directive/mpl_examples/mplot3d/contour3d_demo.py\n%macro                  # define macro with range of history lines, filenames or string objects\n%recall\n\n%whos                   # list identifiers you have defined interactively\n%reset  -f -s           # remove objects -f for force -s for soft (leaves history).   %reset  is not a kernel restart  Restart with  Ctrl+.  in \"qtconsole\"  import module ; reload(module)  to reload a module from disk",
            "title": "Other Commands"
        },
        {
            "location": "/Python/Jupyter/#debugging",
            "text": "%debug                  # jump into the Python debugger (pdb)\n%pdb                    # start the debugger on any uncaught exception.\n\n%cd                     # change directory\n%pwd                    # print working directory\n%env                    # OS environment variables",
            "title": "Debugging"
        },
        {
            "location": "/Python/Jupyter/#os-commands",
            "text": "!OScommand\n!ping www.bbc.co.uk\n%alias                  # system command alias",
            "title": "OS Commands"
        },
        {
            "location": "/Python/Jupyter/#history",
            "text": "_ __ ___                # etc... for previous outputs.\n_i _ii _i4              # etc.. for previous input. _ih for list of previous inputs",
            "title": "History"
        },
        {
            "location": "/Python/Jupyter/#gui-integration",
            "text": "Start with  ipython --gui=qt  or at the IPython prompt:  %gui wx  Arguments can be  wx ,  qt ,  gtk  and  tk .",
            "title": "GUI integration"
        },
        {
            "location": "/Python/Jupyter/#matplotlib-pylab-graphics-in-an-ipython-shell",
            "text": "Start with:  ipython --matplotlib  ( or  --matplotlib=qt  etc...)  At the IPython prompt:  %matplotlib             # set matplotlib to work interactively; does not import anythig\n%matplotlib  inline\n%matplotlib qt          # request a specific GUI backend\n%pylab inline  %pylab  makes the following imports:  import   numpy  import   matplotlib  from   matplotlib   import   pylab ,   mlab ,   pyplot  np   =   numpy  plt   =   pyplot  from   IPython.display   import   display  from   IPython.core.pylabtools   import   figsize ,   getfigs  from   pylab   import   *  from   numpy   import   *",
            "title": "Matplotlib / pylab graphics in an iPython shell"
        },
        {
            "location": "/Python/Jupyter/#qtconsole-an-improved-console",
            "text": "At the command prompt:  ipython.exe qtconsole --pylab=inline --ConsoleWidget.font_size=10  alternative: --matplotlib inline\nor within IPython:  %matplotlib  inline\n%pylab inline  To embed plots, SVG or HTML in qtconsole, call display:  from IPython.core.display import display, display_html\nfrom IPython.core.display import display_png, display_svg\ndisplay(plt.gcf()) # embeds the current figure in the qtconsole\ndisplay(*getfigs()) # embeds all active figures in the qtconsole\n#or:\nf = plt.figure()\nplt.plot(np.rand(100))\ndisplay(f)  ipython and ipython notebook for matlab users",
            "title": "Qtconsole - an improved console"
        },
        {
            "location": "/Python/Jupyter/#ipython-notebook-web-based-interface",
            "text": "Start with: ipython notebook and switch to browser  Keyboard shortcuts:  Enter  to edit a cell  Shift + Enter  to evaluate  Ctrl + m  or  Esc  for the \"command mode\"     In command mode:   h list of keyboard shortcuts\n 1-6 to convert to heading cell\n m to convert to markdown cell\n y to convert to code\n c copy / v paste\n d d delete cell\n s save notebook\n . to restart kernel",
            "title": "IPython Notebook web-based interface"
        },
        {
            "location": "/Python/Matplotlib/",
            "text": "Matplotlib Cheatsheet\n\u00b6\n\n\nMatplotlib prepares 2D (and some 3D) graphics. \n\n\n\n\nMain page: http://www.matplotlib.org\n\n\nImage gallery: http://matplotlib.org/gallery.html \n\n\npyplot command summary: http://matplotlib.org/api/pyplot_summary.html \n\n\nExamples http://matplotlib.org/examples/index.html\n\n\nTutorial: http://www.loria.fr/~rougier/teaching/matplotlib/\n\n\nSee also: https://www.wakari.io/nb/url///wakari.io/static/notebooks/Lecture_4_Matplotlib.ipynb\n\n\n\n\nMatplotlib, pylab, and pyplot: how are they related?\n\u00b6\n\n\n\n\nMatplotlib is the whole package. Pylab and matplotlib.pyplot (pyplot in the following) are modules in matplotlib.\n\n\nPyplot makes matplotlib work like MATLAB. \n\n\nPyplot provides the state-machine interface to the underlying plotting library (the matplotlib API in the matplotlib module). \n\n\nEach pyplot function makes some change to a figure: eg, create a figure, create a plotting area in a figure, plot some lines in a plotting area, decorate the plot with labels, etc.... \n\n\nPyplot is \nstateful\n, in that it keeps track of the current figure and plotting area, and the plotting functions are directed to the current axes.\n\n\nPylab combines the pyplot functionality (for plotting) with the numpy functionality (mathematics / arrays) in a single namespace, making that namespace (or environment) even more MATLAB-like. \n\n\nThe pyplot interface is generally preferred for non-interactive plotting (i.e., scripting). \n\n\nThe pylab interface is convenient for interactive calculations and plotting, as it minimizes typing.\n\n\n\n\nExamples\n\u00b6\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n\n# Compute the x and y coordinates for points on sine and cosine curves\n\n\nx\n \n=\n \nnp\n.\narange\n(\n0\n,\n \n3\n \n*\n \nnp\n.\npi\n,\n \n0.1\n)\n\n\ny_sin\n \n=\n \nnp\n.\nsin\n(\nx\n)\n\n\ny_cos\n \n=\n \nnp\n.\ncos\n(\nx\n)\n\n\n\n# Set up a subplot grid that has height 2 and width 1,\n\n\n# and set the first such subplot as active.\n\n\nplt\n.\nsubplot\n(\n2\n,\n \n1\n,\n \n1\n)\n\n\n\n# Make the first plot\n\n\nplt\n.\nplot\n(\nx\n,\n \ny_sin\n)\n\n\nplt\n.\ntitle\n(\n'Sine'\n)\n\n\n\n# Set the second subplot as active, and make the second plot.\n\n\nplt\n.\nsubplot\n(\n2\n,\n \n1\n,\n \n2\n)\n\n\nplt\n.\nplot\n(\nx\n,\n \ny_cos\n)\n\n\nplt\n.\ntitle\n(\n'Cosine'\n)\n\n\n\n# Show the figure.\n\n\nplt\n.\nshow\n()",
            "title": "Matplotlib Cheatsheet"
        },
        {
            "location": "/Python/Matplotlib/#matplotlib-cheatsheet",
            "text": "Matplotlib prepares 2D (and some 3D) graphics.    Main page: http://www.matplotlib.org  Image gallery: http://matplotlib.org/gallery.html   pyplot command summary: http://matplotlib.org/api/pyplot_summary.html   Examples http://matplotlib.org/examples/index.html  Tutorial: http://www.loria.fr/~rougier/teaching/matplotlib/  See also: https://www.wakari.io/nb/url///wakari.io/static/notebooks/Lecture_4_Matplotlib.ipynb",
            "title": "Matplotlib Cheatsheet"
        },
        {
            "location": "/Python/Matplotlib/#matplotlib-pylab-and-pyplot-how-are-they-related",
            "text": "Matplotlib is the whole package. Pylab and matplotlib.pyplot (pyplot in the following) are modules in matplotlib.  Pyplot makes matplotlib work like MATLAB.   Pyplot provides the state-machine interface to the underlying plotting library (the matplotlib API in the matplotlib module).   Each pyplot function makes some change to a figure: eg, create a figure, create a plotting area in a figure, plot some lines in a plotting area, decorate the plot with labels, etc....   Pyplot is  stateful , in that it keeps track of the current figure and plotting area, and the plotting functions are directed to the current axes.  Pylab combines the pyplot functionality (for plotting) with the numpy functionality (mathematics / arrays) in a single namespace, making that namespace (or environment) even more MATLAB-like.   The pyplot interface is generally preferred for non-interactive plotting (i.e., scripting).   The pylab interface is convenient for interactive calculations and plotting, as it minimizes typing.",
            "title": "Matplotlib, pylab, and pyplot: how are they related?"
        },
        {
            "location": "/Python/Matplotlib/#examples",
            "text": "import   numpy   as   np  import   matplotlib.pyplot   as   plt  # Compute the x and y coordinates for points on sine and cosine curves  x   =   np . arange ( 0 ,   3   *   np . pi ,   0.1 )  y_sin   =   np . sin ( x )  y_cos   =   np . cos ( x )  # Set up a subplot grid that has height 2 and width 1,  # and set the first such subplot as active.  plt . subplot ( 2 ,   1 ,   1 )  # Make the first plot  plt . plot ( x ,   y_sin )  plt . title ( 'Sine' )  # Set the second subplot as active, and make the second plot.  plt . subplot ( 2 ,   1 ,   2 )  plt . plot ( x ,   y_cos )  plt . title ( 'Cosine' )  # Show the figure.  plt . show ()",
            "title": "Examples"
        },
        {
            "location": "/Python/Python3/",
            "text": "What's new in Python 3.x\n\u00b6\n\n\nWhat's really new in Python 3\n\n\nnonlocal / global\n\u00b6\n\n\nx\n \n=\n \n0\n\n\ndef\n \nouter\n():\n\n  \nx\n \n=\n \n1\n\n  \ndef\n \ninner\n():\n\n  \nnonlocal\n \nx\n\n  \nx\n \n=\n \n2\n\n  \nprint\n(\n\"inner:\"\n,\n \nx\n)\n\n\n  \ninner\n()\n\n  \nprint\n(\n\"outer:\"\n,\n \nx\n)\n\n\n\nouter\n()\n\n\nprint\n(\n\"global:\"\n,\n \nx\n)\n\n\n\n# inner: 2\n\n\n# outer: 2\n\n\n# global: 0\n\n\n\n## with global\n\n\nx\n \n=\n \n0\n\n\ndef\n \nouter\n():\n\n     \nx\n \n=\n \n1\n\n     \ndef\n \ninner\n():\n\n           \nglobal\n \nx\n\n           \nx\n \n=\n \n2\n\n           \nprint\n(\n\"inner:\"\n,\n \nx\n)\n\n  \ninner\n()\n\n  \nprint\n(\n\"outer:\"\n,\n \nx\n)\n\n\nouter\n()\n\n\nprint\n(\n\"global:\"\n,\n \nx\n)\n\n\n\n# inner: 2\n\n\n# outer: 1\n\n\n# global: 2\n\n\n\n\n\n\nString interpolation - new in 3.6\n\u00b6\n\n\nname\n=\n\"David\"\n\n\nf\n\"My name is {name}\"\n\n\nvalue\n \n=\n \ndecimal\n.\nDecimal\n(\n\"10.4507\"\n)\n\n\nprint\n(\nf\n\"result: {value:10.5}\"\n \n)\n  \n# width precision\n\n\n\n\n\n\nPEP 492 - Coroutines with async and await syntax\n\u00b6\n\n\nasync and await\n\n\nyield from iterator\n\n\nis equivalent\n\n\nfor\n \nx\n \nin\n \niterator\n:\n\n     \nyield\n \nx\n\n\n\n\n\n\nExample:\n\n\ndef\n \nlazy_range\n(\nup_to\n):\n\n     \n\"\"\"Generator to return the sequence of integers from 0 to up_to, exclusive.\"\"\"\n\n     \nindex\n \n=\n \n0\n\n     \ndef\n \ngratuitous_refactor\n():\n\n           \nnonlocal\n \nindex\n\n           \nwhile\n \nindex\n \n<\n \nup_to\n:\n\n               \nyield\n \nindex\n\n               \nindex\n \n+=\n \n1\n\n     \nyield from\n \ngratuitous_refactor\n()\n\n\n\n\n\n\nNew 3.6 syntax:\n\n\nasync\n \ndef\n \nfunc\n(\nparam1\n,\n \nparam2\n):\n\n    \ndo_stuff\n()\n\n    \nawait\n \nsome_coroutine\n()\n\n\n\nasync\n \ndef\n \nread_data\n(\ndb\n):\n\n  \ndata\n \n=\n \nawait\n \ndb\n.\nfetch\n(\n'SELECT ...'\n)\n\n\n\nasync\n \ndef\n \ndisplay_date\n(\nloop\n):\n\n  \nend_time\n \n=\n \nloop\n.\ntime\n()\n \n+\n \n5.0\n\n  \nwhile\n \nTrue\n:\n\n  \nprint\n(\ndatetime\n.\ndatetime\n.\nnow\n())\n\n  \nif\n \n(\nloop\n.\ntime\n()\n \n+\n \n1.0\n)\n \n>=\n \nend_time\n:\n\n  \nbreak\n\n  \nawait\n \nasyncio\n.\nsleep\n(\n1\n)\n\n\n\n\nloop\n \n=\n \nasyncio\n.\nget_event_loop\n()\n# Blocking call which returns when the display_date() coroutine is done\n\n\nloop\n.\nrun_until_complete\n(\ndisplay_date\n(\nloop\n))\n\n\nloop\n.\nclose\n()\n\n\n\n\n\n\nAsync for\n\u00b6\n\n\nasync\n \nfor\n \nTARGET\n \nin\n \nITER\n:\n\n  \nBLOCK\n\n\nelse\n:\n\n  \nBLOCK2\n\n\n\n\n\n\nAsync improvements - 3.6\n\u00b6\n\n\n\n\nset comprehension: \n{i async for i in agen()}\n\n\nlist comprehension: \n[i async for i in agen()]\n\n\ndict comprehension: \n{i: i ** 2 async for i in agen()}\n\n\ngenerator expression: \n(i ** 2 async for i in agen())\n\n\n\n\nType hinting\n\u00b6\n\n\nPEP 484\n\n\nmypy-lang.org\n\n\ndef\n \ngreet\n(\nname\n:\n \nstr\n)\n \n->\n \nstr\n\n    \nreturn\n \n'Hello there, {}'\n.\nformat\n(\nname\n)\n\n\n\n\n\n\nType aliases\n\u00b6\n\n\nUrl\n \n=\n \nstr\n\n\ndef\n \nretry\n(\nurl\n:\n \nUrl\n,\n \nretry_count\n:\n \nint\n)\n \n->\n \nNone\n:\n \n...\n\n\n\n\n\n\nfrom\n \ntyping\n \nimport\n \nTypeVar\n,\n \nIterable\n,\n \nTuple\n   \n\n\n\n\n\nOther common typings include: Any; Generic, Dict, List, Optional, Mapping, Set, Sequence - expressed as Sequence[int] \n\n\nT\n \n=\n \nTypeVar\n(\n'T'\n,\n \nint\n,\n \nfloat\n,\n \ncomplex\n)\n  \n# T is either or an int, a float or a complex \n\n\nVector\n \n=\n \nIterable\n[\nTuple\n[\nT\n,\n \nT\n]]\n          \n# \n\n\n\ndef\n \ninproduct\n(\nv\n:\n \nVector\n[\nT\n])\n \n->\n \nT\n:\n\n       \nreturn\n \nsum\n(\nx\n*\ny\n \nfor\n \nx\n,\n \ny\n \nin\n \nv\n)\n\n\n\ndef\n \ndilate\n(\nv\n:\n \nVector\n[\nT\n],\n \nscale\n:\n \nT\n)\n \n->\n \nVector\n[\nT\n]:\n\n       \nreturn\n \n((\nx\n \n*\n \nscale\n,\n \ny\n \n*\n \nscale\n)\n \nfor\n \nx\n,\n \ny\n \nin\n \nv\n)\n\n\nvec\n \n=\n \n[]\n \n# type: Vector[float]\n\n\n\n\n\n\nFor functions\n\u00b6\n\n\nCallable\n[[\nArg1Type\n,\n \nArg2Type\n],\n \nReturnType\n]\n\n\n\n\n\n\nType comments\n\u00b6\n\n\nx\n \n=\n \n[]\n   \n# type: List[Employee]\n\n\nx\n,\n \ny\n,\n \nz\n \n=\n \n[],\n \n[],\n \n[]\n  \n# type: List[int], List[int], List[str]",
            "title": "Python 3"
        },
        {
            "location": "/Python/Python3/#whats-new-in-python-3x",
            "text": "What's really new in Python 3",
            "title": "What's new in Python 3.x"
        },
        {
            "location": "/Python/Python3/#nonlocal-global",
            "text": "x   =   0  def   outer (): \n   x   =   1 \n   def   inner (): \n   nonlocal   x \n   x   =   2 \n   print ( \"inner:\" ,   x ) \n\n   inner () \n   print ( \"outer:\" ,   x )  outer ()  print ( \"global:\" ,   x )  # inner: 2  # outer: 2  # global: 0  ## with global  x   =   0  def   outer (): \n      x   =   1 \n      def   inner (): \n            global   x \n            x   =   2 \n            print ( \"inner:\" ,   x ) \n   inner () \n   print ( \"outer:\" ,   x )  outer ()  print ( \"global:\" ,   x )  # inner: 2  # outer: 1  # global: 2",
            "title": "nonlocal / global"
        },
        {
            "location": "/Python/Python3/#string-interpolation-new-in-36",
            "text": "name = \"David\"  f \"My name is {name}\"  value   =   decimal . Decimal ( \"10.4507\" )  print ( f \"result: {value:10.5}\"   )    # width precision",
            "title": "String interpolation - new in 3.6"
        },
        {
            "location": "/Python/Python3/#pep-492-coroutines-with-async-and-await-syntax",
            "text": "async and await  yield from iterator  is equivalent  for   x   in   iterator : \n      yield   x   Example:  def   lazy_range ( up_to ): \n      \"\"\"Generator to return the sequence of integers from 0 to up_to, exclusive.\"\"\" \n      index   =   0 \n      def   gratuitous_refactor (): \n            nonlocal   index \n            while   index   <   up_to : \n                yield   index \n                index   +=   1 \n      yield from   gratuitous_refactor ()   New 3.6 syntax:  async   def   func ( param1 ,   param2 ): \n     do_stuff () \n     await   some_coroutine ()  async   def   read_data ( db ): \n   data   =   await   db . fetch ( 'SELECT ...' )  async   def   display_date ( loop ): \n   end_time   =   loop . time ()   +   5.0 \n   while   True : \n   print ( datetime . datetime . now ()) \n   if   ( loop . time ()   +   1.0 )   >=   end_time : \n   break \n   await   asyncio . sleep ( 1 )  loop   =   asyncio . get_event_loop () # Blocking call which returns when the display_date() coroutine is done  loop . run_until_complete ( display_date ( loop ))  loop . close ()",
            "title": "PEP 492 - Coroutines with async and await syntax"
        },
        {
            "location": "/Python/Python3/#async-for",
            "text": "async   for   TARGET   in   ITER : \n   BLOCK  else : \n   BLOCK2",
            "title": "Async for"
        },
        {
            "location": "/Python/Python3/#async-improvements-36",
            "text": "set comprehension:  {i async for i in agen()}  list comprehension:  [i async for i in agen()]  dict comprehension:  {i: i ** 2 async for i in agen()}  generator expression:  (i ** 2 async for i in agen())",
            "title": "Async improvements - 3.6"
        },
        {
            "location": "/Python/Python3/#type-hinting",
            "text": "PEP 484  mypy-lang.org  def   greet ( name :   str )   ->   str \n     return   'Hello there, {}' . format ( name )",
            "title": "Type hinting"
        },
        {
            "location": "/Python/Python3/#type-aliases",
            "text": "Url   =   str  def   retry ( url :   Url ,   retry_count :   int )   ->   None :   ...   from   typing   import   TypeVar ,   Iterable ,   Tuple      Other common typings include: Any; Generic, Dict, List, Optional, Mapping, Set, Sequence - expressed as Sequence[int]   T   =   TypeVar ( 'T' ,   int ,   float ,   complex )    # T is either or an int, a float or a complex   Vector   =   Iterable [ Tuple [ T ,   T ]]            #   def   inproduct ( v :   Vector [ T ])   ->   T : \n        return   sum ( x * y   for   x ,   y   in   v )  def   dilate ( v :   Vector [ T ],   scale :   T )   ->   Vector [ T ]: \n        return   (( x   *   scale ,   y   *   scale )   for   x ,   y   in   v )  vec   =   []   # type: Vector[float]",
            "title": "Type aliases"
        },
        {
            "location": "/Python/Python3/#for-functions",
            "text": "Callable [[ Arg1Type ,   Arg2Type ],   ReturnType ]",
            "title": "For functions"
        },
        {
            "location": "/Python/Python3/#type-comments",
            "text": "x   =   []     # type: List[Employee]  x ,   y ,   z   =   [],   [],   []    # type: List[int], List[int], List[str]",
            "title": "Type comments"
        },
        {
            "location": "/Scala/Akka/",
            "text": "Akka\n\u00b6\n\n\nActors have:\n\n\n\n\nA mailbox (the queue where messages end up).\n\n\nA behavior (the state of the actor, internal variables etc.).\n\n\nMessages (pieces of data representing a signal, similar to method calls and their parameters).\n\n\nAn execution environment (the machinery that takes actors that have messages to react to and invokes their message handling code).\n\n\nAn address.\n\n\n\n\nsbt:\n\n\nlibraryDependencies\n \n++=\n \nSeq\n(\n\n  \n\"com.typesafe.akka\"\n \n%%\n \n\"akka-actor\"\n \n%\n \n\"2.5.6\"\n,\n\n  \n\"com.typesafe.akka\"\n \n%%\n \n\"akka-testkit\"\n \n%\n \n\"2.5.6\"\n \n%\n \nTest\n\n\n)\n\n\n\n\n\n\nBasic Example\n\u00b6\n\n\n//#full-example\n\n\npackage\n \ncom.lightbend.akka.sample\n\n\n\nimport\n \nakka.actor.\n{\n \nActor\n,\n \nActorLogging\n,\n \nActorRef\n,\n \nActorSystem\n,\n \nProps\n \n}\n\n\nimport\n \nscala.io.StdIn\n\n\n\n//#greeter-companion\n\n\n//#greeter-messages\n\n\nobject\n \nGreeter\n \n{\n\n  \n//#greeter-messages\n\n  \ndef\n \nprops\n(\nmessage\n:\n \nString\n,\n \nprinterActor\n:\n \nActorRef\n)\n:\n \nProps\n \n=\n \nProps\n(\nnew\n \nGreeter\n(\nmessage\n,\n \nprinterActor\n))\n\n  \n//#greeter-messages\n\n  \nfinal\n \ncase\n \nclass\n \nWhoToGreet\n(\nwho\n:\n \nString\n)\n\n  \ncase\n \nobject\n \nGreet\n\n\n}\n\n\n//#greeter-messages\n\n\n//#greeter-companion\n\n\n\n//#greeter-actor\n\n\nclass\n \nGreeter\n(\nmessage\n:\n \nString\n,\n \nprinterActor\n:\n \nActorRef\n)\n \nextends\n \nActor\n \n{\n\n  \nimport\n \nGreeter._\n\n  \nimport\n \nPrinter._\n\n\n  \nvar\n \ngreeting\n \n=\n \n\"\"\n\n\n  \ndef\n \nreceive\n \n=\n \n{\n\n    \ncase\n \nWhoToGreet\n(\nwho\n)\n \n=>\n\n      \ngreeting\n \n=\n \ns\"\n$message\n, \n$who\n\"\n\n    \ncase\n \nGreet\n           \n=>\n\n      \n//#greeter-send-message\n\n      \nprinterActor\n \n!\n \nGreeting\n(\ngreeting\n)\n\n      \n//#greeter-send-message\n\n  \n}\n\n\n}\n\n\n//#greeter-actor\n\n\n\n//#printer-companion\n\n\n//#printer-messages\n\n\nobject\n \nPrinter\n \n{\n\n  \n//#printer-messages\n\n  \ndef\n \nprops\n:\n \nProps\n \n=\n \nProps\n[\nPrinter\n]\n\n  \n//#printer-messages\n\n  \nfinal\n \ncase\n \nclass\n \nGreeting\n(\ngreeting\n:\n \nString\n)\n\n\n}\n\n\n//#printer-messages\n\n\n//#printer-companion\n\n\n\n//#printer-actor\n\n\nclass\n \nPrinter\n \nextends\n \nActor\n \nwith\n \nActorLogging\n \n{\n\n  \nimport\n \nPrinter._\n\n\n  \ndef\n \nreceive\n \n=\n \n{\n\n    \ncase\n \nGreeting\n(\ngreeting\n)\n \n=>\n\n      \nlog\n.\ninfo\n(\ns\"Greeting received (from \n${\nsender\n()\n}\n): \n$greeting\n\"\n)\n\n  \n}\n\n\n}\n\n\n//#printer-actor\n\n\n\n//#main-class\n\n\nobject\n \nAkkaQuickstart\n \nextends\n \nApp\n \n{\n\n  \nimport\n \nGreeter._\n\n\n  \n// Create the 'helloAkka' actor system\n\n  \nval\n \nsystem\n:\n \nActorSystem\n \n=\n \nActorSystem\n(\n\"helloAkka\"\n)\n\n\n  \ntry\n \n{\n\n    \n//#create-actors\n\n    \n// Create the printer actor\n\n    \nval\n \nprinter\n:\n \nActorRef\n \n=\n \nsystem\n.\nactorOf\n(\nPrinter\n.\nprops\n,\n \n\"printerActor\"\n)\n\n\n    \n// Create the 'greeter' actors\n\n    \nval\n \nhowdyGreeter\n:\n \nActorRef\n \n=\n\n      \nsystem\n.\nactorOf\n(\nGreeter\n.\nprops\n(\n\"Howdy\"\n,\n \nprinter\n),\n \n\"howdyGreeter\"\n)\n\n    \nval\n \nhelloGreeter\n:\n \nActorRef\n \n=\n\n      \nsystem\n.\nactorOf\n(\nGreeter\n.\nprops\n(\n\"Hello\"\n,\n \nprinter\n),\n \n\"helloGreeter\"\n)\n\n    \nval\n \ngoodDayGreeter\n:\n \nActorRef\n \n=\n\n      \nsystem\n.\nactorOf\n(\nGreeter\n.\nprops\n(\n\"Good day\"\n,\n \nprinter\n),\n \n\"goodDayGreeter\"\n)\n\n    \n//#create-actors\n\n\n    \n//#main-send-messages\n\n    \nhowdyGreeter\n \n!\n \nWhoToGreet\n(\n\"Akka\"\n)\n\n    \nhowdyGreeter\n \n!\n \nGreet\n\n\n    \nhowdyGreeter\n \n!\n \nWhoToGreet\n(\n\"Lightbend\"\n)\n\n    \nhowdyGreeter\n \n!\n \nGreet\n\n\n    \nhelloGreeter\n \n!\n \nWhoToGreet\n(\n\"Scala\"\n)\n\n    \nhelloGreeter\n \n!\n \nGreet\n\n\n    \ngoodDayGreeter\n \n!\n \nWhoToGreet\n(\n\"Play\"\n)\n\n    \ngoodDayGreeter\n \n!\n \nGreet\n\n    \n//#main-send-messages\n\n\n    \nprintln\n(\n\">>> Press ENTER to exit <<<\"\n)\n\n    \nStdIn\n.\nreadLine\n()\n\n  \n}\n \nfinally\n \n{\n\n    \nsystem\n.\nterminate\n()\n\n  \n}\n\n\n}\n\n\n//#main-class\n\n\n//#full-example",
            "title": "Akka Cheatsheet"
        },
        {
            "location": "/Scala/Akka/#akka",
            "text": "Actors have:   A mailbox (the queue where messages end up).  A behavior (the state of the actor, internal variables etc.).  Messages (pieces of data representing a signal, similar to method calls and their parameters).  An execution environment (the machinery that takes actors that have messages to react to and invokes their message handling code).  An address.   sbt:  libraryDependencies   ++=   Seq ( \n   \"com.typesafe.akka\"   %%   \"akka-actor\"   %   \"2.5.6\" , \n   \"com.typesafe.akka\"   %%   \"akka-testkit\"   %   \"2.5.6\"   %   Test  )",
            "title": "Akka"
        },
        {
            "location": "/Scala/Akka/#basic-example",
            "text": "//#full-example  package   com.lightbend.akka.sample  import   akka.actor. {   Actor ,   ActorLogging ,   ActorRef ,   ActorSystem ,   Props   }  import   scala.io.StdIn  //#greeter-companion  //#greeter-messages  object   Greeter   { \n   //#greeter-messages \n   def   props ( message :   String ,   printerActor :   ActorRef ) :   Props   =   Props ( new   Greeter ( message ,   printerActor )) \n   //#greeter-messages \n   final   case   class   WhoToGreet ( who :   String ) \n   case   object   Greet  }  //#greeter-messages  //#greeter-companion  //#greeter-actor  class   Greeter ( message :   String ,   printerActor :   ActorRef )   extends   Actor   { \n   import   Greeter._ \n   import   Printer._ \n\n   var   greeting   =   \"\" \n\n   def   receive   =   { \n     case   WhoToGreet ( who )   => \n       greeting   =   s\" $message ,  $who \" \n     case   Greet             => \n       //#greeter-send-message \n       printerActor   !   Greeting ( greeting ) \n       //#greeter-send-message \n   }  }  //#greeter-actor  //#printer-companion  //#printer-messages  object   Printer   { \n   //#printer-messages \n   def   props :   Props   =   Props [ Printer ] \n   //#printer-messages \n   final   case   class   Greeting ( greeting :   String )  }  //#printer-messages  //#printer-companion  //#printer-actor  class   Printer   extends   Actor   with   ActorLogging   { \n   import   Printer._ \n\n   def   receive   =   { \n     case   Greeting ( greeting )   => \n       log . info ( s\"Greeting received (from  ${ sender () } ):  $greeting \" ) \n   }  }  //#printer-actor  //#main-class  object   AkkaQuickstart   extends   App   { \n   import   Greeter._ \n\n   // Create the 'helloAkka' actor system \n   val   system :   ActorSystem   =   ActorSystem ( \"helloAkka\" ) \n\n   try   { \n     //#create-actors \n     // Create the printer actor \n     val   printer :   ActorRef   =   system . actorOf ( Printer . props ,   \"printerActor\" ) \n\n     // Create the 'greeter' actors \n     val   howdyGreeter :   ActorRef   = \n       system . actorOf ( Greeter . props ( \"Howdy\" ,   printer ),   \"howdyGreeter\" ) \n     val   helloGreeter :   ActorRef   = \n       system . actorOf ( Greeter . props ( \"Hello\" ,   printer ),   \"helloGreeter\" ) \n     val   goodDayGreeter :   ActorRef   = \n       system . actorOf ( Greeter . props ( \"Good day\" ,   printer ),   \"goodDayGreeter\" ) \n     //#create-actors \n\n     //#main-send-messages \n     howdyGreeter   !   WhoToGreet ( \"Akka\" ) \n     howdyGreeter   !   Greet \n\n     howdyGreeter   !   WhoToGreet ( \"Lightbend\" ) \n     howdyGreeter   !   Greet \n\n     helloGreeter   !   WhoToGreet ( \"Scala\" ) \n     helloGreeter   !   Greet \n\n     goodDayGreeter   !   WhoToGreet ( \"Play\" ) \n     goodDayGreeter   !   Greet \n     //#main-send-messages \n\n     println ( \">>> Press ENTER to exit <<<\" ) \n     StdIn . readLine () \n   }   finally   { \n     system . terminate () \n   }  }  //#main-class  //#full-example",
            "title": "Basic Example"
        },
        {
            "location": "/Scala/Play_Framework/",
            "text": "Links\n\u00b6\n\n\nPlay Framework\n\n\nThe Play application layout\n\u00b6\n\n\nThe layout of a Play application is standardized to keep things as simple as possible. After a first successful compile, a Play application looks like this:\n\n\napp                      \u2192 Application sources\n \u2514 assets                \u2192 Compiled asset sources\n    \u2514 stylesheets        \u2192 Typically LESS CSS sources\n    \u2514 javascripts        \u2192 Typically CoffeeScript sources\n \u2514 controllers           \u2192 Application controllers\n \u2514 models                \u2192 Application business layer\n \u2514 views                 \u2192 Templates\nbuild.sbt                \u2192 Application build script\nconf                     \u2192 Configurations files and other non-compiled resources (on classpath)\n \u2514 application.conf      \u2192 Main configuration file\n \u2514 routes                \u2192 Routes definition\ndist                     \u2192 Arbitrary files to be included in your projects distribution\npublic                   \u2192 Public assets\n \u2514 stylesheets           \u2192 CSS files\n \u2514 javascripts           \u2192 Javascript files\n \u2514 images                \u2192 Image files\nproject                  \u2192 sbt configuration files\n \u2514 build.properties      \u2192 Marker for sbt project\n \u2514 plugins.sbt           \u2192 sbt plugins including the declaration for Play itself\nlib                      \u2192 Unmanaged libraries dependencies\nlogs                     \u2192 Logs folder\n \u2514 application.log       \u2192 Default log file\ntarget                   \u2192 Generated stuff\n \u2514 resolution-cache      \u2192 Info about dependencies\n \u2514 scala-2.11\n    \u2514 api                \u2192 Generated API docs\n    \u2514 classes            \u2192 Compiled class files\n    \u2514 routes             \u2192 Sources generated from routes\n    \u2514 twirl              \u2192 Sources generated from templates\n \u2514 universal             \u2192 Application packaging\n \u2514 web                   \u2192 Compiled web assets\ntest                     \u2192 source folder for unit or functional tests",
            "title": "Play Framework"
        },
        {
            "location": "/Scala/Play_Framework/#links",
            "text": "Play Framework",
            "title": "Links"
        },
        {
            "location": "/Scala/Play_Framework/#the-play-application-layout",
            "text": "The layout of a Play application is standardized to keep things as simple as possible. After a first successful compile, a Play application looks like this:  app                      \u2192 Application sources\n \u2514 assets                \u2192 Compiled asset sources\n    \u2514 stylesheets        \u2192 Typically LESS CSS sources\n    \u2514 javascripts        \u2192 Typically CoffeeScript sources\n \u2514 controllers           \u2192 Application controllers\n \u2514 models                \u2192 Application business layer\n \u2514 views                 \u2192 Templates\nbuild.sbt                \u2192 Application build script\nconf                     \u2192 Configurations files and other non-compiled resources (on classpath)\n \u2514 application.conf      \u2192 Main configuration file\n \u2514 routes                \u2192 Routes definition\ndist                     \u2192 Arbitrary files to be included in your projects distribution\npublic                   \u2192 Public assets\n \u2514 stylesheets           \u2192 CSS files\n \u2514 javascripts           \u2192 Javascript files\n \u2514 images                \u2192 Image files\nproject                  \u2192 sbt configuration files\n \u2514 build.properties      \u2192 Marker for sbt project\n \u2514 plugins.sbt           \u2192 sbt plugins including the declaration for Play itself\nlib                      \u2192 Unmanaged libraries dependencies\nlogs                     \u2192 Logs folder\n \u2514 application.log       \u2192 Default log file\ntarget                   \u2192 Generated stuff\n \u2514 resolution-cache      \u2192 Info about dependencies\n \u2514 scala-2.11\n    \u2514 api                \u2192 Generated API docs\n    \u2514 classes            \u2192 Compiled class files\n    \u2514 routes             \u2192 Sources generated from routes\n    \u2514 twirl              \u2192 Sources generated from templates\n \u2514 universal             \u2192 Application packaging\n \u2514 web                   \u2192 Compiled web assets\ntest                     \u2192 source folder for unit or functional tests",
            "title": "The Play application layout"
        },
        {
            "location": "/Scala/Scala_Collections/",
            "text": "Examples from \nScala Koans\n.\n\n\nCore Packages\n\u00b6\n\n\nThe scala package contains core types like Int, Float, Array or Option which are accessible in all Scala compilation units without explicit qualification or imports.\n\n\nNotable packages include:\n\n\nscala.collection and its sub-packages contain Scala's collections framework\nscala.collection.immutable - Immutable, sequential data-structures such as Vector, List, Range, HashMap or HashSet\nscala.collection.mutable - Mutable, sequential data-structures such as ArrayBuffer, StringBuilder, HashMap or HashSet\nscala.collection.concurrent - Mutable, concurrent data-structures such as TrieMap\nscala.collection.parallel.immutable - Immutable, parallel data-structures such as ParVector, ParRange, ParHashMap or ParHashSet\nscala.collection.parallel.mutable - Mutable, parallel data-structures such as ParArray, ParHashMap, ParTrieMap or ParHashSet\n\nscala.concurrent - Primitives for concurrent programming such as Futures and Promises\nscala.io - Input and output operations\nscala.math - Basic math functions and additional numeric types like BigInt and BigDecimal\nscala.sys - Interaction with other processes and the operating system\nscala.util.matching - Regular expressions\n\n\n\n\n\nAdditional parts of the standard library are shipped as separate libraries. These include:\n\n\nscala.reflect - Scala's reflection API (scala-reflect.jar)\nscala.xml - XML parsing, manipulation, and serialization (scala-xml.jar)\nscala.swing - A convenient wrapper around Java's GUI framework called Swing (scala-swing.jar)\nscala.util.parsing - Parser combinators (scala-parser-combinators.jar)\nAutomatic imports\n\n\n\n\n\nIdentifiers in the scala package and the scala.Predef object are always in scope by default.\n\n\nSome of these identifiers are type aliases provided as shortcuts to commonly used classes. For example, List is an alias for scala.collection.immutable.List.\n\n\nOther aliases refer to classes provided by the underlying platform. For example, on the JVM, String is an alias for java.lang.String.\n\n\nTraversables\n\u00b6\n\n\nTraversables are the superclass of Lists, Arrays, Maps, Sets, Streams, and more.\nThe methods involved can be applied to each other in a different type.  \n\n\nval\n \nset\n \n=\n \nSet\n(\n1\n,\n \n9\n,\n \n10\n,\n \n22\n)\n\n\nval\n \nlist\n \n=\n \nList\n(\n3\n,\n \n4\n,\n \n5\n,\n \n10\n)\n\n\nval\n \nresult\n \n=\n \nset\n \n++\n \nlist\n        \n// ++ appends two Traversables together.\n\n\nresult\n.\nsize\n\n\nresult\n.\nisEmpty\n\n\nresult\n.\nhasDefiniteSize\n          \n// false if a Stream\n\n\n\n\n\n\n\n\nTake some\n\n\n\n\nlist\n.\nhead\n\n\nlist\n.\nheadOption\n\n\nlist\n.\ntail\n\n\nlist\n.\nlastOption\n\n\nresult\n.\nlast\n\n\nlist\n.\ninit\n                       \n// collection without the last element\n\n\nlist\n.\nslice\n(\n1\n,\n \n3\n)\n\n\nlist\n.\ntake\n(\n3\n)\n\n\nlist\n \ndrop\n \n6\n \ntake\n \n3\n\n\nlist\n.\ntakeWhile\n(\n_\n \n<\n \n100\n)\n\n\nlist\n.\ndropWhile\n(\n_\n \n<\n \n100\n)\n\n\n\n\n\n\n\n\nFilter, Map, Flatten\n\n\n\n\nlist\n.\nfilter\n(\n_\n \n<\n \n100\n)\n\n\nlist\n.\nfilterNot\n(\n_\n \n<\n \n100\n)\n\n\nlist\n.\nfind\n(\n_\n \n%\n \n2\n \n!=\n \n0\n)\n                       \n// get first element that matches\n\n\n\nlist\n.\nforeach\n(\nnum\n \n=>\n \nprintln\n(\nnum\n \n*\n \n4\n))\n       \n// side effect\n\n\n\nlist\n.\nmap\n(\n_\n \n*\n \n4\n)\n                             \n// map\n\n\n\nval\n \nlist\n \n=\n \nList\n(\nList\n(\n1\n),\n \nList\n(\n2\n,\n \n3\n,\n \n4\n),\n \nList\n(\n5\n,\n \n6\n,\n \n7\n),\n \nList\n(\n8\n,\n \n9\n,\n \n10\n))\n\n\nlist\n.\nflatten\n\n\nlist\n.\nflatMap\n(\n_\n.\nmap\n(\n_\n \n*\n \n4\n))\n                  \n// map then flatten\n\n\n\nval\n \nresult\n \n=\n \nlist\n.\ncollect\n \n{\n                 \n// apply a partial function to all elements of a Traversable and will return a different collection.\n\n      \ncase\n \nx\n:\n \nInt\n \nif\n \n(\nx\n \n%\n \n2\n \n==\n \n0\n)\n \n=>\n \nx\n \n*\n \n3\n\n    \n}\n\n\n// can use  orElse  or andThen\n\n\n\n\n\n\n\n\nSplit\n\n\n\n\nval\n \narray\n \n=\n \nArray\n(\n87\n,\n \n44\n,\n \n5\n,\n \n4\n,\n \n200\n,\n \n10\n,\n \n39\n,\n \n100\n)\n  \n// splitAt - will split a Traversable at a position, returning a tuple.\n\n\nval\n \nresult\n \n=\n \narray\n \nsplitAt\n \n3\n\n\nresult\n.\n_1\n\n\nresult\n.\n_2\n\n\n\nval\n \nresult\n \n=\n \narray\n \npartition\n \n(\n_\n \n<\n \n100\n)\n              \n// partition will split a Traversable according to predicate, return a 2 product Tuple. The left side are the elements satisfied by the predicate, the right side is not.  \n\n\n\n// groupBy returns a map e.g. Map( \"Odd\" -> ... , \"Even\" -> ...)\n\n\nval\n \nresult\n \n=\n \narray\n \ngroupBy\n \n{\n \ncase\n \nx\n:\n \nInt\n \nif\n \nx\n \n%\n \n2\n \n=\n=\n \n0\n \n=>\n \n\"Even\"\n;\n \ncase\n \nx\n:\n \nInt\n \nif\n \nx\n \n%\n \n2\n \n!=\n \n0\n \n=>\n \n\"Odd\"\n \n}\n \n\n\n\n\n\n\n\nAnalyze\n\n\n\n\nlist\n \nforall\n \n(\n_\n \n<\n \n100\n)\n                       \n// true if predicate true for all elements\n\n\nlist\n \nexists\n \n(\n_\n \n<\n \n100\n)\n                       \n// true if predicate true for any element\n\n\nlist\n \ncount\n \n(\n_\n \n<\n \n100\n)\n\n\n\n\n\n\n\n\nReduce and Fold\n\n\n\n\nlist\n.\nfoldLeft\n(\n0\n)(\n_\n \n-\n \n_\n)\n\n\n(\n0\n \n/:\n \nlist\n)(\n_\n \n-\n \n_\n)\n                          \n// Short hand\n\n\n\nlist\n.\nfoldRight\n(\n0\n)(\n_\n \n-\n \n_\n)\n                \n\n(\nlist\n \n:\\\n \n0\n)(\n_\n \n-\n \n_\n)\n                          \n// Short hand\n\n\n\nlist\n.\nreduceLeft\n \n{\n \n_\n \n+\n \n_\n \n}\n \n\nlist\n.\nreduceRight\n \n{\n \n_\n \n+\n \n_\n \n}\n\n\n\nlist\n.\nsum\n \n\nlist\n.\nproduct\n\n\nlist\n.\nmax\n\n\nlist\n.\nmin\n\n\n\nval\n \nlist\n \n=\n \nList\n(\nList\n(\n1\n,\n \n2\n,\n \n3\n),\n \nList\n(\n4\n,\n \n5\n,\n \n6\n),\n \nList\n(\n7\n,\n \n8\n,\n \n9\n))\n\n\nlist\n.\ntranspose\n\n\n\n\n\n\n\n\nConversions; toList, as well as other conversion methods like toSet, toArray will not convert if the collection type is the same.\n\n\n\n\nlist\n.\ntoArray\n\n\nlist\n.\ntoSet\n\n\nset\n.\ntoList\n\n\nset\n.\ntoIterable\n\n\nset\n.\ntoSeq\n\n\nset\n.\ntoIndexedSeq\n\n\nlist\n.\ntoStream\n\n\n\nval\n \nlist\n \n=\n \nList\n(\n\"Phoenix\"\n \n->\n \n\"Arizona\"\n,\n \n\"Austin\"\n \n->\n \n\"Texas\"\n)\n  \n// elements should be tuples\n\n\nval\n \nresult\n \n=\n \nlist\n.\ntoMap\n\n\n\n\n\n\n\n\nPrint\n\n\n\n\nresult\n.\nmkString\n(\n\",\"\n)\n\n\nlist\n.\nmkString\n(\n\">\"\n,\n \n\",\"\n,\n \n\"<\"\n)\n\n\nval\n \nlist\n \n=\n \nList\n(\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n,\n \n11\n,\n \n12\n,\n \n13\n,\n \n14\n,\n \n15\n)\n\n\nstringBuilder\n.\nappend\n(\n\"I want all numbers 6-12: \"\n)\n\n\nlist\n.\nfilter\n(\nit\n \n=>\n \nit\n \n>\n \n5\n \n&&\n \nit\n \n<\n \n13\n).\naddString\n(\nstringBuilder\n,\n \n\",\"\n)\n\n\nstringBuilder\n.\nmkString\n \n\n\n\n\n\nLists\n\u00b6\n\n\nval\n \na\n \n=\n \nList\n(\n1\n,\n \n2\n,\n \n3\n)\n       \n// immutable\n\n\nval\n \nb\n \n=\n \n1\n \n::\n \n2\n \n::\n \n3\n \n::\n \nNil\n  \n// cons notation \n\n\n(\na\n \n==\n \nb\n)\n    \n// true\n\n\na\n \neq\n \nb\n      \n// false\n\n\na\n.\nlength\n\n\na\n.\nhead\n\n\na\n.\ntail\n\n\na\n.\nreverse\n                   \n// reverse the list\n\n\na\n.\nmap\n \n{\nv\n \n=>\n \nv\n \n*\n \n2\n}\n          \n// or a.map {_ * 2} or a.map(_ * 2)\n\n\na\n.\nfilter\n \n{\nv\n \n=>\n \nv\n \n%\n \n3\n \n==\n \n0\n}\n\n\na\n.\nfilterNot\n(\nv\n \n=>\n \nv\n \n==\n \n5\n)\n    \n// remove where value is 5\n\n\na\n.\nreduceLeft\n(\n_\n \n+\n \n_\n)\n         \n// note the two _s below indicate the first and second args respectively\n\n\na\n.\nfoldLeft\n(\n10\n)(\n_\n \n+\n \n_\n)\n       \n// foldlLeft is like reduce, but with an explicit starting value\n\n\n(\n1\n \nto\n \n5\n).\ntoList\n             \n// from range\n\n\nval\n \na\n \n=\n \na\n.\ntoArray\n\n\n\n\n\n\nNil lists are identical, even of different types\n\n\nIterators\n\u00b6\n\n\nval\n \nlist\n \n=\n \nList\n(\n3\n,\n \n5\n,\n \n9\n,\n \n11\n,\n \n15\n,\n \n19\n,\n \n21\n)\n\n\nval\n \nit\n \n=\n \nlist\n.\niterator\n\n\nif\n \n(\nit\n.\nhasNext\n)\n \n{\n\n  \nit\n.\nnext\n \nshould\n \nbe\n(\n__\n)\n\n\n}\n\n\n\nval\n \nit\n \n=\n \nlist\n \ngrouped\n \n3\n         \n// `grouped` will return an fixed sized Iterable chucks of an Iterable\n\n\nval\n \nit\n \n=\n \nlist\n \nsliding\n \n3\n         \n// `sliding` will return an Iterable that shows a sliding window of an Iterable.    \n\n\nval\n \nit\n \n=\n \nlist\n \nsliding\n(\n3\n,\n \n3\n)\n     \n// `sliding` can take the size of the window as well the size of the step during each iteration\n\n\nlist\n \ntakeRight\n \n3\n\n\nlist\n \ndropRight\n \n3\n\n\n\nval\n \nxs\n \n=\n \nList\n(\n3\n,\n \n5\n,\n \n9\n)\n          \n// `zip` will stitch two iterables into an iterable of pairs (tuples) of corresponding elements from both iterables.\n\n\nval\n \nys\n \n=\n \nList\n(\n\"Bob\"\n,\n \n\"Ann\"\n,\n \n\"Stella\"\n)\n\n\nxs\n \nzip\n \nys\n\n\n\n// If two Iterables aren't the same size, then `zip` will only zip what can only be paired.\n\n\nxs\n \nzipAll\n(\nys\n,\n \n-\n1\n,\n \n\"?\"\n)\n  \n// if two Iterables aren't the same size, then `zipAll` can provide fillers\n\n\n\nxs\n.\nzipWithIndex\n\n\n\n\n\n\nArrays, Sequences\n\u00b6\n\n\nval\n \na\n \n=\n \nArray\n(\n1\n,\n \n2\n,\n \n3\n)\n\n\nval\n \ns\n \n=\n \na\n.\ntoSeq\n\n\nval\n \nl\n \n=\n \ns\n.\ntoList\n\n\n\n\n\n\nval\n \ns\n \n=\n \nSeq\n(\n\"hello\"\n,\n \n\"to\"\n,\n \n\"you\"\n)\n\n\nval\n \nfiltered\n \n=\n \ns\n.\nfilter\n(\n_\n.\nlength\n \n>\n \n2\n)\n\n\nval\n \nr\n \n=\n \ns\n \nmap\n \n{\n\n      \n_\n.\nreverse\n\n    \n}\n\n\nval\n \ns\n \n=\n \nfor\n \n(\nv\n \n<-\n \n1\n \nto\n \n10\n \nif\n \nv\n \n%\n \n3\n \n==\n \n0\n)\n \nyield\n \nv\n  \n// create a sequence from a for comprehension with an optional condition\n\n\ns\n.\ntoList\n\n\n\n\n\n\nLazy Collections and Streams\n\u00b6\n\n\nval\n \nstrictList\n \n=\n \nList\n(\n10\n,\n \n20\n,\n \n30\n)\n\n\nval\n \nlazyList\n \n=\n \nstrictList\n.\nview\n      \n// Strict collection always processes its elements but lazy collection does it on demand\n\n\n\nval\n \ninfinite\n \n=\n \nStream\n.\nfrom\n(\n1\n)\n\n\ninfinite\n.\ntake\n(\n4\n).\nsum\n\n\nStream\n.\ncontinually\n(\n1\n).\ntake\n(\n4\n).\nsum\n\n\n\n// Always remember tail of a lazy collection is never computed unless required\n\n\n\ndef\n \nmakeLazy\n(\nvalue\n:\n \nInt\n)\n:\n \nStream\n[\nInt\n]\n \n=\n \n{\n\n  \nStream\n.\ncons\n(\nvalue\n,\n \nmakeLazy\n(\nvalue\n \n+\n \n1\n))\n\n\n}\n\n\nval\n \nstream\n \n=\n \nmakeLazy\n(\n1\n)\n\n\nstream\n.\nhead\n\n\n\n\n\n\nMaps\n\u00b6\n\n\nval\n \nmyMap\n \n=\n \nMap\n(\n\"MI\"\n \n->\n \n\"Michigan\"\n,\n \n\"OH\"\n \n->\n \n\"Ohio\"\n,\n \n\"WI\"\n \n->\n \n\"Wisconsin\"\n,\n \n\"MI\"\n \n->\n \n\"Michigan\"\n)\n\n\n\n// access by key - Accessing a map by key results in an exception if key is not found\n\n\nmyMap\n(\n\"MI\"\n)\n                                 \n\nmyMap\n.\ncontains\n(\n\"IL\"\n)\n \n\n\nval\n \naNewMap\n \n=\n \nmyMap\n \n+\n \n(\n\"IL\"\n \n->\n \n\"Illinois\"\n)\n  \n// add - creates a new collection if immutable\n\n\nval\n \naNewMap\n \n=\n \nmyMap\n \n-\n \n\"MI\"\n                  \n// remove - Attempted removal of nonexistent elements from a map is handled gracefully\n\n\nval\n \naNewMap\n \n=\n \nmyMap\n \n--\n \nList\n(\n\"MI\"\n,\n \n\"OH\"\n)\n     \n// remove multiples\n\n\nval\n \naNewMap\n \n=\n \nmyMap\n \n-\n \n(\n\"MI\"\n,\n \n\"WI\"\n)\n \n// Notice: single '-' operator for tuples\n\n\n\nvar\n \nanotherMap\n \n+=\n \n(\n\"IL\"\n \n->\n \n\"Illinois\"\n)\n      \n// compiler trick - creates a new collection and reassigns; note the 'var' \n\n\n\n// Map values can be iterated\n\n\nval\n \nmapValues\n \n=\n \nmyMap\n.\nvalues\n\n\nmapValues\n.\nsize\n\n\nmapValues\n.\nhead\n\n\nmapValues\n.\nlast\n\n\n\nfor\n \n(\nmval\n \n<-\n \nmapValues\n)\n \nprintln\n(\nmval\n)\n\n\n// NOTE that the following will not compile, as iterators do not implement \"contains\"\n\n\n//mapValues.contains(\"Illinois\")\n\n\n\n// Map keys may be of mixed type\n\n\nval\n \nmyMap\n \n=\n \nMap\n(\n\"Ann Arbor\"\n \n->\n \n\"MI\"\n,\n \n49931\n \n->\n \n\"MI\"\n)\n\n\n\n// Mixed type values can be added to a map\n\n\nval\n \nmyMap\n \n=\n \nscala\n.\ncollection\n.\nmutable\n.\nMap\n.\nempty\n[\nString\n, \nAny\n]\n\n\nmyMap\n(\n\"Ann Arbor\"\n)\n \n=\n \n(\n48103\n,\n \n48104\n,\n \n48108\n)\n\n\nmyMap\n(\n\"Houghton\"\n)\n \n=\n \n49931\n\n\n\n// Map equivalency is independent of order\n\n\nval\n \nmyMap1\n \n=\n \nMap\n(\n\"MI\"\n \n->\n \n\"Michigan\"\n,\n \n\"OH\"\n \n->\n \n\"Ohio\"\n,\n \n\"WI\"\n \n->\n \n\"Wisconsin\"\n,\n \n\"IA\"\n \n->\n \n\"Iowa\"\n)\n\n\nval\n \nmyMap2\n \n=\n \nMap\n(\n\"WI\"\n \n->\n \n\"Wisconsin\"\n,\n \n\"MI\"\n \n->\n \n\"Michigan\"\n,\n \n\"IA\"\n \n->\n \n\"Iowa\"\n,\n \n\"OH\"\n \n->\n \n\"Ohio\"\n)\n\n\nmyMap1\n.\nequals\n(\nmyMap2\n)\n\n\n\n\n\n\nMaps insertion with duplicate key updates previous entry with subsequent value\n\n\n\n\nMutable Maps\n\n\n\n\nval\n \nmyMap\n \n=\n \nmutable\n.\nMap\n(\n\"MI\"\n \n->\n \n\"Michigan\"\n,\n \n\"OH\"\n \n->\n \n\"Ohio\"\n,\n \n\"WI\"\n \n->\n \n\"Wisconsin\"\n,\n \n\"IA\"\n \n->\n \n\"Iowa\"\n)\n\n\n// same methods than immutable maps work\n\n\nval\n \nmyMap\n \n+=\n \n(\n\"IL\"\n \n->\n \n\"Illinois\"\n)\n       \n// this is a method; note the difference from immutable += \n\n\nmyMap\n.\nclear\n()\n                           \n// Convention is to use parens if possible when method called changes state\n\n\n\n\n\n\nSets\n\u00b6\n\n\nval\n \nmySet\n \n=\n \nSet\n(\n1\n,\n \n3\n,\n \n4\n,\n \n9\n)\n  \n// immutable\n\n\nval\n \nmySet\n \n=\n \nmutable\n.\nSet\n(\n\"Michigan\"\n,\n \n\"Ohio\"\n,\n \n\"Wisconsin\"\n,\n \n\"Iowa\"\n)\n\n\nmySet\n.\nsize\n\n\nmySet\n \ncontains\n \n\"Ohio\"\n\n\nmySet\n \n+=\n \n\"Oregon\"\n\n\nmySet\n \n+=\n \n(\n\"Iowa\"\n,\n \n\"Ohio\"\n)\n\n\nmySet\n \n++=\n \nList\n(\n\"Iowa\"\n,\n \n\"Ohio\"\n)\n\n\nmySet\n \n-=\n \n\"Ohio\"\n\n\nmySet\n \n--=\n \nList\n(\n\"Iowa\"\n,\n \n\"Ohio\"\n)\n\n\nmySet\n.\nclear\n()\n  \n// mutable only\n\n\n\nvar\n \nsum\n \n=\n \n0\n\n\nfor\n \n(\ni\n \n<-\n \nmySet\n)\n    \n// for comprehension\n\n  \nsum\n \n=\n \nsum\n \n+\n \ni\n     \n// of course this is the same thing as mySet.reduce(_ + _)\n\n\n\nval\n \nmySet2\n \n=\n \nSet\n(\n\"Wisconsin\"\n,\n \n\"Michigan\"\n,\n \n\"Minnesota\"\n)\n\n\nmySet\n \nintersect\n \nmySet2\n  \n// or & operator\n\n\nmySet1\n \nunion\n \nmySet2\n    \n// or | operator\n\n\nmySet2\n \nsubsetOf\n \nmySet1\n\n\nmySet1\n \ndiff\n \nmySet2\n\n\nmySet1\n.\nequals\n(\nmySet2\n)\n  \n// independent of order\n\n\n\n\n\n\nOption[T]\n\u00b6\n\n\nval\n \nsomeValue\n:\n \nOption\n[\nString\n]\n \n=\n \nSome\n(\n\"I am wrapped in something\"\n)\n\n\nval\n \nnullValue\n:\n \nOption\n[\nString\n]\n \n=\n \nNone\n\n\nsomeValue\n.\nget\n                       \n// java.util.NoSuchElementException if None\n\n\nnullValue\n \ngetOrElse\n \n\"No value\"\n \n\nnullValue\n.\nisEmpty\n\n\n\nval\n \nvalue\n \n=\n \nsomeValue\n \nmatch\n \n{\n       \n// pattern matching\n\n      \ncase\n \nSome\n(\nv\n)\n \n=>\n \nv\n\n      \ncase\n \nNone\n \n=>\n \n0.0\n\n    \n}\n\n\n\n\n\n\n\n\nOption is more than just a replacement of null, its also a collection.\n\n\n\n\n    \nSome\n(\n10\n)\n \nfilter\n \n{\n \n_\n \n==\n \n10\n}\n\n    \nSome\n(\nSome\n(\n10\n))\n \nflatMap\n \n{\n \n_\n \nmap\n \n{\n \n_\n \n+\n \n10\n}}\n\n    \nvar\n \nnewValue1\n \n=\n \n0\n\n    \nSome\n(\n20\n)\n \nforeach\n \n{\n \nnewValue1\n \n=\n \n_\n}\n\n\n\n\n\n\n\n\nflatMap of Options will filter out all Nones and keep the Somes\n\n\n\n\nval\n \nlist\n \n=\n \nList\n(\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n)\n\n\nval\n \nresult\n \n=\n \nlist\n.\nflatMap\n(\nit\n \n=>\n \nif\n \n(\nit\n \n%\n \n2\n \n==\n \n0\n)\n \nSome\n(\nit\n)\n \nelse\n \nNone\n)\n\n\n\n\n\n\n\n\nUsing \"for comprehension\"\n\n\n\n\n    \nval\n \nvalues\n \n=\n \nList\n(\nSome\n(\n10\n),\n \nSome\n(\n20\n),\n \nNone\n,\n \nSome\n(\n15\n))\n\n    \nval\n \nnewValues\n \n=\n \nfor\n \n{\n\n      \nsomeValue\n \n<-\n \nvalues\n\n      \nvalue\n \n<-\n \nsomeValue\n\n    \n}\n \nyield\n \nvalue\n\n\n\n\n\n\nJava Interop\n\u00b6\n\n\nScala can implicitly convert from a Scala collection type into a Java collection type. \n\n\nimport\n \nscala.collection.JavaConversions._",
            "title": "Scala Collections"
        },
        {
            "location": "/Scala/Scala_Collections/#core-packages",
            "text": "The scala package contains core types like Int, Float, Array or Option which are accessible in all Scala compilation units without explicit qualification or imports.  Notable packages include:  scala.collection and its sub-packages contain Scala's collections framework\nscala.collection.immutable - Immutable, sequential data-structures such as Vector, List, Range, HashMap or HashSet\nscala.collection.mutable - Mutable, sequential data-structures such as ArrayBuffer, StringBuilder, HashMap or HashSet\nscala.collection.concurrent - Mutable, concurrent data-structures such as TrieMap\nscala.collection.parallel.immutable - Immutable, parallel data-structures such as ParVector, ParRange, ParHashMap or ParHashSet\nscala.collection.parallel.mutable - Mutable, parallel data-structures such as ParArray, ParHashMap, ParTrieMap or ParHashSet\n\nscala.concurrent - Primitives for concurrent programming such as Futures and Promises\nscala.io - Input and output operations\nscala.math - Basic math functions and additional numeric types like BigInt and BigDecimal\nscala.sys - Interaction with other processes and the operating system\nscala.util.matching - Regular expressions  Additional parts of the standard library are shipped as separate libraries. These include:  scala.reflect - Scala's reflection API (scala-reflect.jar)\nscala.xml - XML parsing, manipulation, and serialization (scala-xml.jar)\nscala.swing - A convenient wrapper around Java's GUI framework called Swing (scala-swing.jar)\nscala.util.parsing - Parser combinators (scala-parser-combinators.jar)\nAutomatic imports  Identifiers in the scala package and the scala.Predef object are always in scope by default.  Some of these identifiers are type aliases provided as shortcuts to commonly used classes. For example, List is an alias for scala.collection.immutable.List.  Other aliases refer to classes provided by the underlying platform. For example, on the JVM, String is an alias for java.lang.String.",
            "title": "Core Packages"
        },
        {
            "location": "/Scala/Scala_Collections/#traversables",
            "text": "Traversables are the superclass of Lists, Arrays, Maps, Sets, Streams, and more.\nThe methods involved can be applied to each other in a different type.    val   set   =   Set ( 1 ,   9 ,   10 ,   22 )  val   list   =   List ( 3 ,   4 ,   5 ,   10 )  val   result   =   set   ++   list          // ++ appends two Traversables together.  result . size  result . isEmpty  result . hasDefiniteSize            // false if a Stream    Take some   list . head  list . headOption  list . tail  list . lastOption  result . last  list . init                         // collection without the last element  list . slice ( 1 ,   3 )  list . take ( 3 )  list   drop   6   take   3  list . takeWhile ( _   <   100 )  list . dropWhile ( _   <   100 )    Filter, Map, Flatten   list . filter ( _   <   100 )  list . filterNot ( _   <   100 )  list . find ( _   %   2   !=   0 )                         // get first element that matches  list . foreach ( num   =>   println ( num   *   4 ))         // side effect  list . map ( _   *   4 )                               // map  val   list   =   List ( List ( 1 ),   List ( 2 ,   3 ,   4 ),   List ( 5 ,   6 ,   7 ),   List ( 8 ,   9 ,   10 ))  list . flatten  list . flatMap ( _ . map ( _   *   4 ))                    // map then flatten  val   result   =   list . collect   {                   // apply a partial function to all elements of a Traversable and will return a different collection. \n       case   x :   Int   if   ( x   %   2   ==   0 )   =>   x   *   3 \n     }  // can use  orElse  or andThen    Split   val   array   =   Array ( 87 ,   44 ,   5 ,   4 ,   200 ,   10 ,   39 ,   100 )    // splitAt - will split a Traversable at a position, returning a tuple.  val   result   =   array   splitAt   3  result . _1  result . _2  val   result   =   array   partition   ( _   <   100 )                // partition will split a Traversable according to predicate, return a 2 product Tuple. The left side are the elements satisfied by the predicate, the right side is not.    // groupBy returns a map e.g. Map( \"Odd\" -> ... , \"Even\" -> ...)  val   result   =   array   groupBy   {   case   x :   Int   if   x   %   2   = =   0   =>   \"Even\" ;   case   x :   Int   if   x   %   2   !=   0   =>   \"Odd\"   }     Analyze   list   forall   ( _   <   100 )                         // true if predicate true for all elements  list   exists   ( _   <   100 )                         // true if predicate true for any element  list   count   ( _   <   100 )    Reduce and Fold   list . foldLeft ( 0 )( _   -   _ )  ( 0   /:   list )( _   -   _ )                            // Short hand  list . foldRight ( 0 )( _   -   _ )                  ( list   :\\   0 )( _   -   _ )                            // Short hand  list . reduceLeft   {   _   +   _   }   list . reduceRight   {   _   +   _   }  list . sum   list . product  list . max  list . min  val   list   =   List ( List ( 1 ,   2 ,   3 ),   List ( 4 ,   5 ,   6 ),   List ( 7 ,   8 ,   9 ))  list . transpose    Conversions; toList, as well as other conversion methods like toSet, toArray will not convert if the collection type is the same.   list . toArray  list . toSet  set . toList  set . toIterable  set . toSeq  set . toIndexedSeq  list . toStream  val   list   =   List ( \"Phoenix\"   ->   \"Arizona\" ,   \"Austin\"   ->   \"Texas\" )    // elements should be tuples  val   result   =   list . toMap    Print   result . mkString ( \",\" )  list . mkString ( \">\" ,   \",\" ,   \"<\" )  val   list   =   List ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ,   11 ,   12 ,   13 ,   14 ,   15 )  stringBuilder . append ( \"I want all numbers 6-12: \" )  list . filter ( it   =>   it   >   5   &&   it   <   13 ). addString ( stringBuilder ,   \",\" )  stringBuilder . mkString",
            "title": "Traversables"
        },
        {
            "location": "/Scala/Scala_Collections/#lists",
            "text": "val   a   =   List ( 1 ,   2 ,   3 )         // immutable  val   b   =   1   ::   2   ::   3   ::   Nil    // cons notation   ( a   ==   b )      // true  a   eq   b        // false  a . length  a . head  a . tail  a . reverse                     // reverse the list  a . map   { v   =>   v   *   2 }            // or a.map {_ * 2} or a.map(_ * 2)  a . filter   { v   =>   v   %   3   ==   0 }  a . filterNot ( v   =>   v   ==   5 )      // remove where value is 5  a . reduceLeft ( _   +   _ )           // note the two _s below indicate the first and second args respectively  a . foldLeft ( 10 )( _   +   _ )         // foldlLeft is like reduce, but with an explicit starting value  ( 1   to   5 ). toList               // from range  val   a   =   a . toArray   Nil lists are identical, even of different types",
            "title": "Lists"
        },
        {
            "location": "/Scala/Scala_Collections/#iterators",
            "text": "val   list   =   List ( 3 ,   5 ,   9 ,   11 ,   15 ,   19 ,   21 )  val   it   =   list . iterator  if   ( it . hasNext )   { \n   it . next   should   be ( __ )  }  val   it   =   list   grouped   3           // `grouped` will return an fixed sized Iterable chucks of an Iterable  val   it   =   list   sliding   3           // `sliding` will return an Iterable that shows a sliding window of an Iterable.      val   it   =   list   sliding ( 3 ,   3 )       // `sliding` can take the size of the window as well the size of the step during each iteration  list   takeRight   3  list   dropRight   3  val   xs   =   List ( 3 ,   5 ,   9 )            // `zip` will stitch two iterables into an iterable of pairs (tuples) of corresponding elements from both iterables.  val   ys   =   List ( \"Bob\" ,   \"Ann\" ,   \"Stella\" )  xs   zip   ys  // If two Iterables aren't the same size, then `zip` will only zip what can only be paired.  xs   zipAll ( ys ,   - 1 ,   \"?\" )    // if two Iterables aren't the same size, then `zipAll` can provide fillers  xs . zipWithIndex",
            "title": "Iterators"
        },
        {
            "location": "/Scala/Scala_Collections/#arrays-sequences",
            "text": "val   a   =   Array ( 1 ,   2 ,   3 )  val   s   =   a . toSeq  val   l   =   s . toList   val   s   =   Seq ( \"hello\" ,   \"to\" ,   \"you\" )  val   filtered   =   s . filter ( _ . length   >   2 )  val   r   =   s   map   { \n       _ . reverse \n     }  val   s   =   for   ( v   <-   1   to   10   if   v   %   3   ==   0 )   yield   v    // create a sequence from a for comprehension with an optional condition  s . toList",
            "title": "Arrays, Sequences"
        },
        {
            "location": "/Scala/Scala_Collections/#lazy-collections-and-streams",
            "text": "val   strictList   =   List ( 10 ,   20 ,   30 )  val   lazyList   =   strictList . view        // Strict collection always processes its elements but lazy collection does it on demand  val   infinite   =   Stream . from ( 1 )  infinite . take ( 4 ). sum  Stream . continually ( 1 ). take ( 4 ). sum  // Always remember tail of a lazy collection is never computed unless required  def   makeLazy ( value :   Int ) :   Stream [ Int ]   =   { \n   Stream . cons ( value ,   makeLazy ( value   +   1 ))  }  val   stream   =   makeLazy ( 1 )  stream . head",
            "title": "Lazy Collections and Streams"
        },
        {
            "location": "/Scala/Scala_Collections/#maps",
            "text": "val   myMap   =   Map ( \"MI\"   ->   \"Michigan\" ,   \"OH\"   ->   \"Ohio\" ,   \"WI\"   ->   \"Wisconsin\" ,   \"MI\"   ->   \"Michigan\" )  // access by key - Accessing a map by key results in an exception if key is not found  myMap ( \"MI\" )                                   myMap . contains ( \"IL\" )   val   aNewMap   =   myMap   +   ( \"IL\"   ->   \"Illinois\" )    // add - creates a new collection if immutable  val   aNewMap   =   myMap   -   \"MI\"                    // remove - Attempted removal of nonexistent elements from a map is handled gracefully  val   aNewMap   =   myMap   --   List ( \"MI\" ,   \"OH\" )       // remove multiples  val   aNewMap   =   myMap   -   ( \"MI\" ,   \"WI\" )   // Notice: single '-' operator for tuples  var   anotherMap   +=   ( \"IL\"   ->   \"Illinois\" )        // compiler trick - creates a new collection and reassigns; note the 'var'   // Map values can be iterated  val   mapValues   =   myMap . values  mapValues . size  mapValues . head  mapValues . last  for   ( mval   <-   mapValues )   println ( mval )  // NOTE that the following will not compile, as iterators do not implement \"contains\"  //mapValues.contains(\"Illinois\")  // Map keys may be of mixed type  val   myMap   =   Map ( \"Ann Arbor\"   ->   \"MI\" ,   49931   ->   \"MI\" )  // Mixed type values can be added to a map  val   myMap   =   scala . collection . mutable . Map . empty [ String ,  Any ]  myMap ( \"Ann Arbor\" )   =   ( 48103 ,   48104 ,   48108 )  myMap ( \"Houghton\" )   =   49931  // Map equivalency is independent of order  val   myMap1   =   Map ( \"MI\"   ->   \"Michigan\" ,   \"OH\"   ->   \"Ohio\" ,   \"WI\"   ->   \"Wisconsin\" ,   \"IA\"   ->   \"Iowa\" )  val   myMap2   =   Map ( \"WI\"   ->   \"Wisconsin\" ,   \"MI\"   ->   \"Michigan\" ,   \"IA\"   ->   \"Iowa\" ,   \"OH\"   ->   \"Ohio\" )  myMap1 . equals ( myMap2 )   Maps insertion with duplicate key updates previous entry with subsequent value   Mutable Maps   val   myMap   =   mutable . Map ( \"MI\"   ->   \"Michigan\" ,   \"OH\"   ->   \"Ohio\" ,   \"WI\"   ->   \"Wisconsin\" ,   \"IA\"   ->   \"Iowa\" )  // same methods than immutable maps work  val   myMap   +=   ( \"IL\"   ->   \"Illinois\" )         // this is a method; note the difference from immutable +=   myMap . clear ()                             // Convention is to use parens if possible when method called changes state",
            "title": "Maps"
        },
        {
            "location": "/Scala/Scala_Collections/#sets",
            "text": "val   mySet   =   Set ( 1 ,   3 ,   4 ,   9 )    // immutable  val   mySet   =   mutable . Set ( \"Michigan\" ,   \"Ohio\" ,   \"Wisconsin\" ,   \"Iowa\" )  mySet . size  mySet   contains   \"Ohio\"  mySet   +=   \"Oregon\"  mySet   +=   ( \"Iowa\" ,   \"Ohio\" )  mySet   ++=   List ( \"Iowa\" ,   \"Ohio\" )  mySet   -=   \"Ohio\"  mySet   --=   List ( \"Iowa\" ,   \"Ohio\" )  mySet . clear ()    // mutable only  var   sum   =   0  for   ( i   <-   mySet )      // for comprehension \n   sum   =   sum   +   i       // of course this is the same thing as mySet.reduce(_ + _)  val   mySet2   =   Set ( \"Wisconsin\" ,   \"Michigan\" ,   \"Minnesota\" )  mySet   intersect   mySet2    // or & operator  mySet1   union   mySet2      // or | operator  mySet2   subsetOf   mySet1  mySet1   diff   mySet2  mySet1 . equals ( mySet2 )    // independent of order",
            "title": "Sets"
        },
        {
            "location": "/Scala/Scala_Collections/#optiont",
            "text": "val   someValue :   Option [ String ]   =   Some ( \"I am wrapped in something\" )  val   nullValue :   Option [ String ]   =   None  someValue . get                         // java.util.NoSuchElementException if None  nullValue   getOrElse   \"No value\"   nullValue . isEmpty  val   value   =   someValue   match   {         // pattern matching \n       case   Some ( v )   =>   v \n       case   None   =>   0.0 \n     }    Option is more than just a replacement of null, its also a collection.        Some ( 10 )   filter   {   _   ==   10 } \n     Some ( Some ( 10 ))   flatMap   {   _   map   {   _   +   10 }} \n     var   newValue1   =   0 \n     Some ( 20 )   foreach   {   newValue1   =   _ }    flatMap of Options will filter out all Nones and keep the Somes   val   list   =   List ( 1 ,   2 ,   3 ,   4 ,   5 )  val   result   =   list . flatMap ( it   =>   if   ( it   %   2   ==   0 )   Some ( it )   else   None )    Using \"for comprehension\"        val   values   =   List ( Some ( 10 ),   Some ( 20 ),   None ,   Some ( 15 )) \n     val   newValues   =   for   { \n       someValue   <-   values \n       value   <-   someValue \n     }   yield   value",
            "title": "Option[T]"
        },
        {
            "location": "/Scala/Scala_Collections/#java-interop",
            "text": "Scala can implicitly convert from a Scala collection type into a Java collection type.   import   scala.collection.JavaConversions._",
            "title": "Java Interop"
        },
        {
            "location": "/Scala/Scala_Database_Access/",
            "text": "Database Access / ORM Libraries\n\u00b6\n\n\nSlick\n\n\n\n\nManual\n\n\n\n\nJOOQ\n\n\nSqueryl\n\n\nSORM\n\n\nDoobie\n\n\nReactiveMongo\n\n\nComparison of multiple frameworks\n\n\nDatabase Initialization / Migration\n\u00b6\n\n\nFlyway",
            "title": "Scala Database Access"
        },
        {
            "location": "/Scala/Scala_Database_Access/#database-access-orm-libraries",
            "text": "Slick   Manual   JOOQ  Squeryl  SORM  Doobie  ReactiveMongo  Comparison of multiple frameworks",
            "title": "Database Access / ORM Libraries"
        },
        {
            "location": "/Scala/Scala_Database_Access/#database-initialization-migration",
            "text": "Flyway",
            "title": "Database Initialization / Migration"
        },
        {
            "location": "/Scala/Scala_Design_Patterns/",
            "text": "Create a new project from template\n\u00b6\n\n\nUse the \u201csbt new\u201d command, providing the name of the template. For example, \u201c$ sbt new akka/hello-akka.g8\u201d. \nYou can find a list of templates \nhere\n.\n\n\nOr download from \nScala Project Templates\n\n\nStatic Factory\n\u00b6\n\n\ntrait\n \nAnimal\n\n\nclass\n \nBird\n \nextends\n \nAnimal\n\n\nclass\n \nMammal\n \nextends\n \nAnimal\n\n\nclass\n \nFish\n \nextends\n \nAnimal\n\n\n\nobject\n \nAnimal\n \n{\n\n  \ndef\n \napply\n(\nanimal\n:\n \nString\n)\n:\n \nAnimal\n \n=\n \nanimal\n.\ntoLowerCase\n \nmatch\n \n{\n\n    \ncase\n \n\"bird\"\n \n=>\n \nnew\n \nBird\n\n    \ncase\n \n\"mammal\"\n \n=>\n \nnew\n \nMammal\n\n    \ncase\n \n\"fish\"\n \n=>\n \nnew\n \nFish\n\n    \ncase\n \nx\n:\n \nString\n \n=>\n \nthrow\n \nnew\n \nRuntimeException\n(\ns\"Unknown animal: \n$x\n\"\n)\n\n  \n}\n\n\n}\n\n\n\n\n\n\nAlgebraic Data Types and Pattern Matching\n\u00b6\n\n\n\n\nGoal: translate data descriptions into code\n\n\nModel data with logical ors and logical ands\n\n\nTwo patterns: product types (and) sum types (or)\n\n\nProduct type: A has a B and C\n\n\nSum type: A is a B or C\n\n\nSum and product together make algebraic data types\n\n\n\n\n// A has a B and C\n\n\ncase\n \nclass\n \nA\n(\nb\n:\n \nB\n,\n \nc\n:\n \nC\n)\n\n\n\n// A is a B or C\n\n\nsealed\n \ntrait\n \nA\n\n\ncase\n \nclass\n \nB\n()\n \nextends\n \nA\n\n\ncase\n \nclass\n \nC\n()\n \nextends\n \nA\n\n\n\n\n\n\nThey have only data and do not contain any functionality on top of this data as normal classes would. \n\n\nsealed\n \ntrait\n \nShape\n \n\ncase\n \nclass\n \nCircle\n(\nradius\n:\n \nDouble\n)\n \nextends\n \nShape\n \n\ncase\n \nclass\n \nRectangle\n(\nheight\n:\n \nDouble\n,\n \nwidth\n:\n \nDouble\n)\n \nextends\n \nShape\n\n\n\nobject\n \nShape\n \n{\n \n  \ndef\n \narea\n(\nshape\n:\n \nShape\n)\n:\n \nDouble\n \n=\n \n    \nshape\n \nmatch\n \n{\n \n      \ncase\n \nCircle\n(\nPoint\n(\nx\n,\n \ny\n),\n \nradius\n)\n \n=>\n \nMath\n.\nPI\n \n*\n \nMath\n.\npow\n(\nradius\n,\n \n2\n)\n   \n// use pattern matching to process\n\n      \ncase\n \nRectangle\n(\n_\n,\n \nh\n,\n \nw\n)\n \n=>\n \nh\n \n*\n \nw\n \n    \n}\n \n\n}\n\n\n\n\n\n\nStackable Traits\n\u00b6\n\n\nabstract\n \nclass\n \nStringWriter\n \n{\n\n  \ndef\n \nwrite\n(\ndata\n:\n \nString\n)\n:\n \nString\n\n\n}\n\n\n\nclass\n \nBasicStringWriter\n \nextends\n \nStringWriter\n \n{\n\n  \noverride\n \ndef\n \nwrite\n(\ndata\n:\n \nString\n)\n:\n \nString\n \n=\n\n    \ns\"Writing the following data: \n${\ndata\n}\n\"\n\n\n}\n\n\n\ntrait\n \nCapitalizingStringWriter\n \nextends\n \nStringWriter\n \n{\n\n  \nabstract\n \noverride\n \ndef\n \nwrite\n(\ndata\n:\n \nString\n)\n:\n \nString\n \n=\n \n{\n\n    \nsuper\n.\nwrite\n(\ndata\n.\nsplit\n(\n\"\\\\s+\"\n).\nmap\n(\n_\n.\ncapitalize\n).\nmkString\n(\n\" \"\n))\n\n  \n}\n\n\n}\n\n\n\ntrait\n \nUppercasingStringWriter\n \nextends\n \nStringWriter\n \n{\n\n  \nabstract\n \noverride\n \ndef\n \nwrite\n(\ndata\n:\n \nString\n)\n:\n \nString\n \n=\n \n{\n\n    \nsuper\n.\nwrite\n(\ndata\n.\ntoUpperCase\n)\n\n  \n}\n\n\n}\n\n\n\nobject\n \nExample\n \n{\n\n  \ndef\n \nmain\n(\nargs\n:\n \nArray\n[\nString\n])\n:\n \nUnit\n \n=\n \n{\n\n    \nval\n \nwriter1\n \n=\n \nnew\n \nBasicStringWriter\n \nwith\n \nUppercasingStringWriter\n \nwith\n \nCapitalizingStringWriter\n\n    \nSystem\n.\nout\n.\nprintln\n(\ns\"Writer 1: '\n${\nwriter1\n.\nwrite\n(\n\"we like learning scala!\"\n)\n}\n'\"\n)\n\n  \n}\n\n\n}\n\n\n\n\n\n\nStackable traits order of execution\n\n\nStackable traits are always executed from the right mixin to the left. \nSometimes, however, if we only get output and it doesn't depend on what is passed to the method, we simply end up with method calls on a stack, which then get evaluated and it will appear as if things are applied from left to right.\n\n\nComponents / Cake Pattern\n\u00b6\n\n\nhttp://jonasboner.com/real-world-scala-dependency-injection-di/\n\n\n// Service Interfaces and Component Definitions\n\n\n\ntrait\n \nOnOffDeviceComponent\n \n{\n\n  \nval\n \nonOff\n:\n \nOnOffDevice\n  \n// abstract val\n\n\n  \ntrait\n \nOnOffDevice\n \n{\n\n    \ndef\n \non\n:\n \nUnit\n\n    \ndef\n \noff\n:\n \nUnit\n\n  \n}\n\n\n}\n\n\n\ntrait\n \nSensorDeviceComponent\n \n{\n\n  \nval\n \nsensor\n:\n \nSensorDevice\n\n\n  \ntrait\n \nSensorDevice\n \n{\n\n    \ndef\n \nisCoffeePresent\n:\n \nBoolean\n\n  \n}\n\n\n}\n\n\n\n// =======================\n\n\n// Component / Service Implementations\n\n\n\ntrait\n \nOnOffDeviceComponentImpl\n \nextends\n \nOnOffDeviceComponent\n \n{\n\n  \nclass\n \nHeater\n \nextends\n \nOnOffDevice\n \n{\n\n    \ndef\n \non\n \n=\n \nprintln\n(\n\"heater.on\"\n)\n\n    \ndef\n \noff\n \n=\n \nprintln\n(\n\"heater.off\"\n)\n\n  \n}\n\n\n}\n\n\n\ntrait\n \nSensorDeviceComponentImpl\n \nextends\n \nSensorDeviceComponent\n \n{\n\n  \nclass\n \nPotSensor\n \nextends\n \nSensorDevice\n \n{\n\n    \ndef\n \nisCoffeePresent\n \n=\n \ntrue\n\n  \n}\n\n\n}\n\n\n\n// =======================\n\n\n// Component declaring two dependencies that it wants injected\n\n\ntrait\n \nWarmerComponentImpl\n \n{\n\n  \nthis:\n \nSensorDeviceComponent\n \nwith\n \nOnOffDeviceComponent\n \n=>\n     \n// Use of self-type for composition\n\n  \nclass\n \nWarmer\n \n{\n\n    \ndef\n \ntrigger\n \n=\n \n{\n\n      \nif\n \n(\nsensor\n.\nisCoffeePresent\n)\n \nonOff\n.\non\n\n      \nelse\n \nonOff\n.\noff\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n// =======================\n\n\n// Instantiation (and configuration) of the services in the ComponentRegistry module \n\n\n\nobject\n \nComponentRegistry\n \nextends\n\n  \nOnOffDeviceComponentImpl\n \nwith\n\n  \nSensorDeviceComponentImpl\n \nwith\n\n  \nWarmerComponentImpl\n \n{\n\n\n  \nval\n \nonOff\n \n=\n \nnew\n \nHeater\n      \n// all instantiations in one spot; can be easily be replaced by e.g. mocks \n\n  \nval\n \nsensor\n \n=\n \nnew\n \nPotSensor\n\n  \nval\n \nwarmer\n \n=\n \nnew\n \nWarmer\n\n\n}\n\n\n\n// =======================\n\n\nval\n \nwarmer\n \n=\n \nComponentRegistry\n.\nwarmer\n\n\nwarmer\n.\ntrigger\n\n\n\n\n\n\nType Classes (using context-bound type parameters)\n\u00b6\n\n\n\n\nAd-hoc polymorphism\n\n\nBreak free from your class oppressors!\n\n\nConcerns that cross class hierarchy e.g. serialize to JSON\n\n\nCommon behaviour without (useful) common type\n\n\nAbstract behaviour to a type class\n\n\nCan implement type class instances in ad-hoc manner \n\n\n\n\n// Define some behavior in terms of operations that a type must support in order to be considered a member of the type class.\n\n\ntrait\n \nNumber\n[\nT\n]\n \n{\n\n  \ndef\n \nplus\n(\nx\n:\n \nT\n,\n \ny\n:\n \nT\n)\n:\n \nT\n\n  \ndef\n \ndivide\n(\nx\n:\n \nT\n,\n \ny\n:\n \nInt\n)\n:\n \nT\n\n\n}\n\n\n\n// Define the default type class members in the companion object of the trait\n\n\nobject\n \nNumber\n \n{\n\n\n  \nimplicit\n \nobject\n \nDoubleNumber\n \nextends\n \nNumber\n[\nDouble\n]\n \n{\n             \n// note the implicit\n\n    \noverride\n \ndef\n \nplus\n(\nx\n:\n \nDouble\n,\n \ny\n:\n \nDouble\n)\n:\n \nDouble\n \n=\n \nx\n \n+\n \ny\n\n    \noverride\n \ndef\n \ndivide\n(\nx\n:\n \nDouble\n,\n \ny\n:\n \nInt\n)\n:\n \nDouble\n \n=\n \nx\n \n/\n \ny\n\n  \n}\n\n\n}\n \n\n\nobject\n \nStats\n \n{\n\n\n\n//  older pattern with implicit parameter\n\n\n//  def mean[T](xs: Vector[T])(implicit ev: Number[T]): T =         // note the implicit\n\n\n//    ev.divide(xs.reduce(ev.plus(_, _)), xs.size)\n\n\n  \ndef\n \nmean\n[\nT:\n \nNumber\n](\nxs\n:\n \nVector\n[\nT\n])\n:\n \nT\n \n=\n                           \n// note the context bound\n\n    \nimplicitly\n[\nNumber\n[\nT\n]].\ndivide\n(\n\n      \nxs\n.\nreduce\n(\nimplicitly\n[\nNumber\n[\nT\n]].\nplus\n(\n_\n,\n \n_\n)),\n                  \n// retrieve the evidence via implicitly[]\n\n      \nxs\n.\nsize\n\n    \n)\n\n\n}\n   \n\n\n\n\n\nVisitor Pattern\n\u00b6\n\n\nabstract\n \nclass\n \nElement\n(\ntext\n:\n \nString\n)\n \n{\n\n  \ndef\n \naccept\n(\nvisitor\n:\n \nVisitor\n)\n\n\n}\n\n\n\ncase\n \nclass\n \nTitle\n(\ntext\n:\n \nString\n)\n \nextends\n \nElement\n(\ntext\n)\n \n{\n\n  \noverride\n \ndef\n \naccept\n(\nvisitor\n:\n \nVisitor\n)\n:\n \nUnit\n \n=\n \n{\n\n    \nvisitor\n.\nvisit\n(\nthis\n)\n\n  \n}\n\n\n}\n\n\n\ncase\n \nclass\n \nText\n(\ntext\n:\n \nString\n)\n \nextends\n \nElement\n(\ntext\n)\n \n{\n\n  \noverride\n \ndef\n \naccept\n(\nvisitor\n:\n \nVisitor\n)\n:\n \nUnit\n \n=\n \n{\n\n    \nvisitor\n.\nvisit\n(\nthis\n)\n\n  \n}\n\n\n}\n\n\n\nclass\n \nDocument\n(\nparts\n:\n \nList\n[\nElement\n])\n \n{\n\n  \ndef\n \naccept\n(\nvisitor\n:\n \nVisitor\n)\n:\n \nUnit\n \n=\n \n{\n\n    \nparts\n.\nforeach\n(\np\n \n=>\n \np\n.\naccept\n(\nvisitor\n))\n\n  \n}\n\n\n}\n\n\n\ntrait\n \nVisitor\n \n{\n\n  \ndef\n \nvisit\n(\nelement\n:\n \nElement\n)\n\n\n}\n\n\n\nclass\n \nVisitorImpl1\n \nextends\n \nVisitor\n \n{\n \n  \noverride\n \ndef\n \nvisit\n(\nelement\n:\n \nElement\n)\n:\n \nUnit\n \n=\n \n{\n\n    \nelement\n \nmatch\n \n{\n\n      \ncase\n \nTitle\n(\ntext\n)\n \n=>\n \n???\n\n      \ncase\n \nText\n(\ntext\n)\n \n=>\n \n???\n \n      \n//...\n\n      \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nConfiguration\n\u00b6\n\n\nimport\n \ncom.typesafe.config.ConfigFactory\n\n\n\ntrait\n \nAppConfigComponent\n \n{\n\n\n  \nval\n \nappConfigService\n:\n \nAppConfigService\n\n\n  \nclass\n \nAppConfigService\n()\n \n{\n\n    \n//-Dconfig.resource=production.conf for overriding\n\n    \nprivate\n \nval\n \nconf\n \n=\n \nConfigFactory\n.\nload\n()\n\n    \nprivate\n \nval\n \nappConf\n \n=\n \nconf\n.\ngetConfig\n(\n\"job-scheduler\"\n)\n\n    \nprivate\n \nval\n \ndb\n \n=\n \nappConf\n.\ngetConfig\n(\n\"db\"\n)\n\n\n    \nval\n \nconfigPath\n \n=\n \nappConf\n.\ngetString\n(\n\"config-path\"\n)\n\n    \nval\n \nconfigExtension\n \n=\n \nappConf\n.\ngetString\n(\n\"config-extension\"\n)\n\n    \nval\n \nworkers\n \n=\n \nappConf\n.\ngetInt\n(\n\"workers\"\n)\n\n\n    \nval\n \ndbConnectionString\n \n=\n \ndb\n.\ngetString\n(\n\"connection-string\"\n)\n\n    \nval\n \ndbUsername\n \n=\n \ndb\n.\ngetString\n(\n\"username\"\n)\n\n    \nval\n \ndbPassword\n \n=\n \ndb\n.\ngetString\n(\n\"password\"\n)\n\n  \n}\n\n\n}\n\n\n\n\n\n\nMemoization\n\u00b6\n\n\nimport\n \nscala.collection.mutable.Map\n\n\n\ntrait\n \nMemoizer\n \n{\n\n\n  \ndef\n \nmemo\n[\nX\n, \nY\n](\nf\n:\n \nX\n \n=>\n \nY\n)\n:\n \n(\nX\n \n=>\n \nY\n)\n \n=\n \n{\n\n    \nval\n \ncache\n \n=\n \nMap\n[\nX\n, \nY\n]()\n\n    \n(\nx\n:\n \nX\n)\n \n=>\n \ncache\n.\ngetOrElseUpdate\n(\nx\n,\n \nf\n(\nx\n))\n\n  \n}\n\n\n}\n\n\n\n\n\n\nUsing scalaz:\n\n\nval\n \nmemoScalaz\n:\n \nString\n \n=>\n \nString\n \n=\n \nMemo\n.\nmutableHashMapMemo\n \n{\n\n  \nfunc\n\n\n}\n\n\n\n\n\n\nPimp my Library Pattern\n\u00b6\n\n\nThe pimp my library design pattern is really similar to extension methods in C#.\n\n\npackage\n \nobject\n \npimp\n \n{\n\n\n  \nimplicit\n \nclass\n \nStringExtensions\n(\nval\n \ns\n:\n \nString\n)\n \nextends\n \nAnyVal\n \n{\n\n\n    \ndef\n \nisAllUpperCase\n:\n \nBoolean\n \n=\n\n      \n(\n0\n \nto\n \ns\n.\nsize\n \n-\n \n1\n).\nfind\n \n{\n\n        \ncase\n \nindex\n \n=>\n\n          \n!\ns\n.\ncharAt\n(\nindex\n).\nisUpper\n\n      \n}.\nisEmpty\n\n\n  \n}\n\n\n}",
            "title": "Scala Design Patterns"
        },
        {
            "location": "/Scala/Scala_Design_Patterns/#create-a-new-project-from-template",
            "text": "Use the \u201csbt new\u201d command, providing the name of the template. For example, \u201c$ sbt new akka/hello-akka.g8\u201d. \nYou can find a list of templates  here .  Or download from  Scala Project Templates",
            "title": "Create a new project from template"
        },
        {
            "location": "/Scala/Scala_Design_Patterns/#static-factory",
            "text": "trait   Animal  class   Bird   extends   Animal  class   Mammal   extends   Animal  class   Fish   extends   Animal  object   Animal   { \n   def   apply ( animal :   String ) :   Animal   =   animal . toLowerCase   match   { \n     case   \"bird\"   =>   new   Bird \n     case   \"mammal\"   =>   new   Mammal \n     case   \"fish\"   =>   new   Fish \n     case   x :   String   =>   throw   new   RuntimeException ( s\"Unknown animal:  $x \" ) \n   }  }",
            "title": "Static Factory"
        },
        {
            "location": "/Scala/Scala_Design_Patterns/#algebraic-data-types-and-pattern-matching",
            "text": "Goal: translate data descriptions into code  Model data with logical ors and logical ands  Two patterns: product types (and) sum types (or)  Product type: A has a B and C  Sum type: A is a B or C  Sum and product together make algebraic data types   // A has a B and C  case   class   A ( b :   B ,   c :   C )  // A is a B or C  sealed   trait   A  case   class   B ()   extends   A  case   class   C ()   extends   A   They have only data and do not contain any functionality on top of this data as normal classes would.   sealed   trait   Shape   case   class   Circle ( radius :   Double )   extends   Shape   case   class   Rectangle ( height :   Double ,   width :   Double )   extends   Shape  object   Shape   {  \n   def   area ( shape :   Shape ) :   Double   =  \n     shape   match   {  \n       case   Circle ( Point ( x ,   y ),   radius )   =>   Math . PI   *   Math . pow ( radius ,   2 )     // use pattern matching to process \n       case   Rectangle ( _ ,   h ,   w )   =>   h   *   w  \n     }   }",
            "title": "Algebraic Data Types and Pattern Matching"
        },
        {
            "location": "/Scala/Scala_Design_Patterns/#stackable-traits",
            "text": "abstract   class   StringWriter   { \n   def   write ( data :   String ) :   String  }  class   BasicStringWriter   extends   StringWriter   { \n   override   def   write ( data :   String ) :   String   = \n     s\"Writing the following data:  ${ data } \"  }  trait   CapitalizingStringWriter   extends   StringWriter   { \n   abstract   override   def   write ( data :   String ) :   String   =   { \n     super . write ( data . split ( \"\\\\s+\" ). map ( _ . capitalize ). mkString ( \" \" )) \n   }  }  trait   UppercasingStringWriter   extends   StringWriter   { \n   abstract   override   def   write ( data :   String ) :   String   =   { \n     super . write ( data . toUpperCase ) \n   }  }  object   Example   { \n   def   main ( args :   Array [ String ]) :   Unit   =   { \n     val   writer1   =   new   BasicStringWriter   with   UppercasingStringWriter   with   CapitalizingStringWriter \n     System . out . println ( s\"Writer 1: ' ${ writer1 . write ( \"we like learning scala!\" ) } '\" ) \n   }  }   Stackable traits order of execution  Stackable traits are always executed from the right mixin to the left. \nSometimes, however, if we only get output and it doesn't depend on what is passed to the method, we simply end up with method calls on a stack, which then get evaluated and it will appear as if things are applied from left to right.",
            "title": "Stackable Traits"
        },
        {
            "location": "/Scala/Scala_Design_Patterns/#components-cake-pattern",
            "text": "http://jonasboner.com/real-world-scala-dependency-injection-di/  // Service Interfaces and Component Definitions  trait   OnOffDeviceComponent   { \n   val   onOff :   OnOffDevice    // abstract val \n\n   trait   OnOffDevice   { \n     def   on :   Unit \n     def   off :   Unit \n   }  }  trait   SensorDeviceComponent   { \n   val   sensor :   SensorDevice \n\n   trait   SensorDevice   { \n     def   isCoffeePresent :   Boolean \n   }  }  // =======================  // Component / Service Implementations  trait   OnOffDeviceComponentImpl   extends   OnOffDeviceComponent   { \n   class   Heater   extends   OnOffDevice   { \n     def   on   =   println ( \"heater.on\" ) \n     def   off   =   println ( \"heater.off\" ) \n   }  }  trait   SensorDeviceComponentImpl   extends   SensorDeviceComponent   { \n   class   PotSensor   extends   SensorDevice   { \n     def   isCoffeePresent   =   true \n   }  }  // =======================  // Component declaring two dependencies that it wants injected  trait   WarmerComponentImpl   { \n   this:   SensorDeviceComponent   with   OnOffDeviceComponent   =>       // Use of self-type for composition \n   class   Warmer   { \n     def   trigger   =   { \n       if   ( sensor . isCoffeePresent )   onOff . on \n       else   onOff . off \n     } \n   }  }  // =======================  // Instantiation (and configuration) of the services in the ComponentRegistry module   object   ComponentRegistry   extends \n   OnOffDeviceComponentImpl   with \n   SensorDeviceComponentImpl   with \n   WarmerComponentImpl   { \n\n   val   onOff   =   new   Heater        // all instantiations in one spot; can be easily be replaced by e.g. mocks  \n   val   sensor   =   new   PotSensor \n   val   warmer   =   new   Warmer  }  // =======================  val   warmer   =   ComponentRegistry . warmer  warmer . trigger",
            "title": "Components / Cake Pattern"
        },
        {
            "location": "/Scala/Scala_Design_Patterns/#type-classes-using-context-bound-type-parameters",
            "text": "Ad-hoc polymorphism  Break free from your class oppressors!  Concerns that cross class hierarchy e.g. serialize to JSON  Common behaviour without (useful) common type  Abstract behaviour to a type class  Can implement type class instances in ad-hoc manner    // Define some behavior in terms of operations that a type must support in order to be considered a member of the type class.  trait   Number [ T ]   { \n   def   plus ( x :   T ,   y :   T ) :   T \n   def   divide ( x :   T ,   y :   Int ) :   T  }  // Define the default type class members in the companion object of the trait  object   Number   { \n\n   implicit   object   DoubleNumber   extends   Number [ Double ]   {               // note the implicit \n     override   def   plus ( x :   Double ,   y :   Double ) :   Double   =   x   +   y \n     override   def   divide ( x :   Double ,   y :   Int ) :   Double   =   x   /   y \n   }  }   object   Stats   {  //  older pattern with implicit parameter  //  def mean[T](xs: Vector[T])(implicit ev: Number[T]): T =         // note the implicit  //    ev.divide(xs.reduce(ev.plus(_, _)), xs.size) \n\n   def   mean [ T:   Number ]( xs :   Vector [ T ]) :   T   =                             // note the context bound \n     implicitly [ Number [ T ]]. divide ( \n       xs . reduce ( implicitly [ Number [ T ]]. plus ( _ ,   _ )),                    // retrieve the evidence via implicitly[] \n       xs . size \n     )  }",
            "title": "Type Classes (using context-bound type parameters)"
        },
        {
            "location": "/Scala/Scala_Design_Patterns/#visitor-pattern",
            "text": "abstract   class   Element ( text :   String )   { \n   def   accept ( visitor :   Visitor )  }  case   class   Title ( text :   String )   extends   Element ( text )   { \n   override   def   accept ( visitor :   Visitor ) :   Unit   =   { \n     visitor . visit ( this ) \n   }  }  case   class   Text ( text :   String )   extends   Element ( text )   { \n   override   def   accept ( visitor :   Visitor ) :   Unit   =   { \n     visitor . visit ( this ) \n   }  }  class   Document ( parts :   List [ Element ])   { \n   def   accept ( visitor :   Visitor ) :   Unit   =   { \n     parts . foreach ( p   =>   p . accept ( visitor )) \n   }  }  trait   Visitor   { \n   def   visit ( element :   Element )  }  class   VisitorImpl1   extends   Visitor   {  \n   override   def   visit ( element :   Element ) :   Unit   =   { \n     element   match   { \n       case   Title ( text )   =>   ??? \n       case   Text ( text )   =>   ???  \n       //... \n       } \n   }  }",
            "title": "Visitor Pattern"
        },
        {
            "location": "/Scala/Scala_Design_Patterns/#configuration",
            "text": "import   com.typesafe.config.ConfigFactory  trait   AppConfigComponent   { \n\n   val   appConfigService :   AppConfigService \n\n   class   AppConfigService ()   { \n     //-Dconfig.resource=production.conf for overriding \n     private   val   conf   =   ConfigFactory . load () \n     private   val   appConf   =   conf . getConfig ( \"job-scheduler\" ) \n     private   val   db   =   appConf . getConfig ( \"db\" ) \n\n     val   configPath   =   appConf . getString ( \"config-path\" ) \n     val   configExtension   =   appConf . getString ( \"config-extension\" ) \n     val   workers   =   appConf . getInt ( \"workers\" ) \n\n     val   dbConnectionString   =   db . getString ( \"connection-string\" ) \n     val   dbUsername   =   db . getString ( \"username\" ) \n     val   dbPassword   =   db . getString ( \"password\" ) \n   }  }",
            "title": "Configuration"
        },
        {
            "location": "/Scala/Scala_Design_Patterns/#memoization",
            "text": "import   scala.collection.mutable.Map  trait   Memoizer   { \n\n   def   memo [ X ,  Y ]( f :   X   =>   Y ) :   ( X   =>   Y )   =   { \n     val   cache   =   Map [ X ,  Y ]() \n     ( x :   X )   =>   cache . getOrElseUpdate ( x ,   f ( x )) \n   }  }   Using scalaz:  val   memoScalaz :   String   =>   String   =   Memo . mutableHashMapMemo   { \n   func  }",
            "title": "Memoization"
        },
        {
            "location": "/Scala/Scala_Design_Patterns/#pimp-my-library-pattern",
            "text": "The pimp my library design pattern is really similar to extension methods in C#.  package   object   pimp   { \n\n   implicit   class   StringExtensions ( val   s :   String )   extends   AnyVal   { \n\n     def   isAllUpperCase :   Boolean   = \n       ( 0   to   s . size   -   1 ). find   { \n         case   index   => \n           ! s . charAt ( index ). isUpper \n       }. isEmpty \n\n   }  }",
            "title": "Pimp my Library Pattern"
        },
        {
            "location": "/Scala/Scala_Generalities/",
            "text": "Main Features of Scala\n\u00b6\n\n\n\n\nAll types are objects\n\n\nType inference\n\n\nNested Functions\n\n\nFunctions are objects\n\n\nDomain specific language (DSL) support\n\n\nTraits\n\n\nClosures\n\n\nConcurrency support inspired by Erlang\n\n\n\n\nTools and Frameworks\n\u00b6\n\n\nREPL \nhttp://ammonite.io/\n\n\nhttps://scalafiddle.io/\n\n\n\n\nThe Lift Framework\n\n\nThe Play framework\n\n\nThe Bowler framework\n\n\nAkka\n\n\n\n\nhttps://typelevel.org/\n\n\nInstall\n\u00b6\n\n\n\n\nNeed to have Java Software Development Kit (SDK) installed \n\n\n\n\njava -version\n\n\n\n\n\nexport\n \nJAVA_HOME\n=\n/usr/local/java-current\n\nexport\n \nPATH\n=\n$PATH\n:\n$JAVA_HOME\n/bin/\n\n\n\n\n\nhttp://www.scala-lang.org/download/\n\n\nCompilation\n\u00b6\n\n\nscalac HelloWorld.scala  // produces HelloWorld.class\nscala -classpath . HelloWorld",
            "title": "Scala (Generalities)"
        },
        {
            "location": "/Scala/Scala_Generalities/#main-features-of-scala",
            "text": "All types are objects  Type inference  Nested Functions  Functions are objects  Domain specific language (DSL) support  Traits  Closures  Concurrency support inspired by Erlang",
            "title": "Main Features of Scala"
        },
        {
            "location": "/Scala/Scala_Generalities/#tools-and-frameworks",
            "text": "REPL  http://ammonite.io/  https://scalafiddle.io/   The Lift Framework  The Play framework  The Bowler framework  Akka   https://typelevel.org/",
            "title": "Tools and Frameworks"
        },
        {
            "location": "/Scala/Scala_Generalities/#install",
            "text": "Need to have Java Software Development Kit (SDK) installed    java -version  export   JAVA_HOME = /usr/local/java-current export   PATH = $PATH : $JAVA_HOME /bin/  http://www.scala-lang.org/download/",
            "title": "Install"
        },
        {
            "location": "/Scala/Scala_Generalities/#compilation",
            "text": "scalac HelloWorld.scala  // produces HelloWorld.class\nscala -classpath . HelloWorld",
            "title": "Compilation"
        },
        {
            "location": "/Scala/Scala_Language/",
            "text": "Links\n\u00b6\n\n\n\n\nScala Cheatsheet\n\n\nScala @ TutorialPoint\n\n\nScala Tutorial (PDF)\n\n\n\n\nSome examples are derived from \nScala Koans\n.\n\n\nBasics\n\u00b6\n\n\nStyle\n\u00b6\n\n\nClass Names - For all class names, the first letter should be in Upper Case. If several words are used to form a name of the class, each inner word's first letter should be in Upper Case.\n\n\nclass MyFirstScalaClass\n\n\nMethod Names - All method names should start with a Lower Case letter. If multiple words are used to form the name of the method, then each inner word's first letter should be in Upper Case.\n\n\ndef myMethodName()\n\n\nProgram File Name - Name of the program file should exactly match the object name. When saving the file you should save it using the object name (Remember Scala is case-sensitive) and append \".scala\" to the end of the name. If the file name and the object name do not match your program will not compile.\n\n\nAssume 'HelloWorld' is the object name: the file should be saved as 'HelloWorld.scala'.\n\n\nPackages\n\u00b6\n\n\npackage\n \npkg\n         \n// at start of file\n\n\npackage\n \npkg\n \n{\n \n...\n \n}\n \n// bracket style\n\n\n\n\n\n\nImports\n\u00b6\n\n\nimport\n \nscala.collection._\n                   \n// wildcard import. When importing all the names of a package or class, one uses the underscore character (_) instead of the asterisk (*).\n\n\nimport\n \nscala.collection.Vector\n              \n// one class import\n\n\nimport\n \nscala.collection.\n{\nVector\n,\n \nSequence\n}\n  \n// selective import. Multiple classes can be imported from the same package by enclosing them in curly braces\n\n\nimport\n \nscala.collection.\n{\nVector\n \n=>\n \nVec28\n}\n   \n// renaming import.\n\n\nimport\n \njava.util.\n{\nDate\n \n=>\n \n_\n,\n \n_\n}\n             \n// import all from java.util except Date.\n\n\n\n\n\n\nAll classes from the java.lang package are imported by default. The Predef object provides definitions that are accessible in all Scala compilation units without explicit qualification:\n- immutable Map, Set, List, ::, Nil, print, println, assert, assume, require, ensuring\n\n\nimport\n \nscala.collection.mutable.HashMap\n               \n// Mutable collections must be imported.\n\n\nimport\n \nscala.collection.immutable.\n{\nTreeMap\n,\n \nTreeSet\n}\n  \n// So are specialized collections.\n\n\n\n\n\n\nApplication Entry Point\n\u00b6\n\n\nobject\n \nHelloWorld\n \n{\n\n    \ndef\n \nmain\n(\nargs\n:\n \nArray\n[\nString\n])\n \n{\n\n        \nprintln\n(\n\"Hello, world!\"\n)\n\n    \n}\n\n\n}\n\n\n\n\n\n\nBlocks\n\u00b6\n\n\nYou can combine expressions by surrounding them with {}. We call this a block.\nThe result of the last expression in the block is the result of the overall block, too.\n\n\nprintln\n({\n\n  \nval\n \nx\n \n=\n \n1\n \n+\n \n1\n\n  \nx\n \n+\n \n1\n\n\n})\n \n// 3\n\n\n\n\n\n\nVariables and Values\n\u00b6\n\n\nvar\n \nx\n \n=\n \n5\n \n// variable\n\n\nval\n \nx\n \n=\n \n5\n \n// immutable value / \"const\"\n\n\nvar\n \nx\n:\n \nDouble\n \n=\n \n5\n \n// explicit type\n\n\nprintln\n(\nx\n)\n\n\n\n\n\n\nA lazy val is assignment that will not evaluated until it is called. Note there is no lazy var\n\n\nlazy\n \nval\n \na\n \n=\n \n{\nheavymath\n();\n \n19\n}\n\n\n\n\n\n\nLiterals\n\u00b6\n\n\nval\n \na\n \n=\n \n2\n       \n// int\n\n\nval\n \nb\n \n=\n \n31L\n     \n// long\n\n\nval\n \nc\n \n=\n \n0x30B\n   \n// hexadecimal\n\n\nval\n \nd\n \n=\n \n3\nf\n      \n// float\n\n\nval\n \ne\n \n=\n \n3.22d\n   \n// double\n\n\nval\n \nf\n \n=\n \n93\ne\n-\n9\n\n\nval\n \ng\n \n=\n \n'a'\n         \n// character\n\n\nval\n \nh\n \n=\n \n'\\u0061'\n    \n// unicode for a\n\n\nval\n \ni\n \n=\n \n'\n\\\n141\n'\n      \n// octal for a\n\n\nval\n \nj\n \n=\n \n'\\\"'\n        \n// escape sequences\n\n\nval\n \nk\n \n=\n \n'\\\\'\n\n\nval\n \ns\n \n=\n \n\"To be or not to be\"\n    \n// string\n\n\ns\n.\ncharAt\n(\n0\n)\n\n\nval\n \ns2\n \n=\n \n\"\"\"An apple a day\n\n\nkeeps the doctor away\"\"\"\n        \n// multi-lines string\n\n\ns2\n.\nsplit\n(\n'\\n'\n)\n\n\nval\n \ns3\n \n=\n \n\"\"\"An apple a day\n\n\n           |keeps the doctor away\"\"\"\n  \n// Multiline String literals can use | to specify the starting position of subsequent lines, then use stripMargin to remove the surplus indentation.\n\n\ns3\n.\nstripMargin\n\n\n\n\n\n\nEnumerations\n\u00b6\n\n\nobject\n \nPlanets\n \nextends\n \nEnumeration\n \n{\n\n  \nval\n \nMercury\n \n=\n \nValue\n\n  \nval\n \nVenus\n \n=\n \nValue\n\n  \nval\n \nEarth\n \n=\n \nValue\n\n  \nval\n \nMars\n \n=\n \nValue\n\n  \nval\n \nJupiter\n \n=\n \nValue\n\n  \nval\n \nSaturn\n \n=\n \nValue\n\n  \nval\n \nUranus\n \n=\n \nValue\n\n  \nval\n \nNeptune\n \n=\n \nValue\n\n  \nval\n \nPluto\n \n=\n \nValue\n\n\n}\n\n\n\nPlanets\n.\nMercury\n.\nid\n\n\nPlanets\n.\nMercury\n.\ntoString\n \n//How does it get the name? by Reflection.\n\n\n\nobject\n \nGreekPlanets\n \nextends\n \nEnumeration\n \n{\n\n  \nval\n \nMercury\n \n=\n \nValue\n(\n1\n,\n \n\"Hermes\"\n)\n   \n// enumeration with your own index and/or your own Strings\n\n  \nval\n \nVenus\n \n=\n \nValue\n(\n2\n,\n \n\"Aphrodite\"\n)\n\n  \n//Fun Fact: Tellus is Roman for (Mother) Earth\n\n  \nval\n \nEarth\n \n=\n \nValue\n(\n3\n,\n \n\"Gaia\"\n)\n\n  \nval\n \nMars\n \n=\n \nValue\n(\n4\n,\n \n\"Ares\"\n)\n\n  \nval\n \nJupiter\n \n=\n \nValue\n(\n5\n,\n \n\"Zeus\"\n)\n\n  \nval\n \nSaturn\n \n=\n \nValue\n(\n6\n,\n \n\"Cronus\"\n)\n\n  \nval\n \nUranus\n \n=\n \nValue\n(\n7\n,\n \n\"Ouranus\"\n)\n\n  \nval\n \nNeptune\n \n=\n \nValue\n(\n8\n,\n \n\"Poseidon\"\n)\n\n  \nval\n \nPluto\n \n=\n \nValue\n(\n9\n,\n \n\"Hades\"\n)\n\n\n}\n\n\n\n\n\n\nCommon Data Structures\n\u00b6\n\n\n(\n1\n,\n2\n,\n3\n)\n                     \n// tuple literal. (Tuple3)\n\n\nvar\n \n(\nx\n,\ny\n,\nz\n)\n \n=\n \n(\n1\n,\n2\n,\n3\n)\n       \n// destructuring bind: tuple unpacking via pattern matching.\n\n\n// BAD var x,y,z = (1,2,3)  // hidden error: each assigned to the entire tuple.\n\n\n\nval\n \ntuple\n \n=\n \n(\n\"apple\"\n,\n \n3\n)\n    \n// mixed type tuple\n\n\ntuple\n.\n_1\n\n\ntuple\n.\n_2\n\n\ntuple\n.\nswap\n\n\n\n\n\n\nvar\n \nxs\n \n=\n \nList\n(\n1\n,\n2\n,\n3\n)\n        \n// list (immutable).\n\n\nxs\n(\n2\n)\n                       \n// paren indexing\n\n\n1\n \n::\n \nList\n(\n2\n,\n3\n)\n              \n// cons (create a new list by prepending the element).\n\n\n\n1\n \nto\n \n5\n                      \n// Range sugar. Same as `1 until 6`\n\n\n1\n \nto\n \n10\n \nby\n \n2\n\n\nRange\n(\n1\n,\n \n10\n,\n \n2\n)\n             \n// Range does not include the last item, even in a step increment\n\n\nRange\n(\n1\n,\n \n9\n,\n \n2\n).\ninclusive\n\n\n\n\n\n\n()\n                          \n// (empty parens)   sole member of the Unit type (like C/Java void).\n\n\n\n\n\n\nControl Constructs\n\u00b6\n\n\nif\n \n(\ncheck\n)\n \nhappy\n \nelse\n \nsad\n   \n// conditional.\n\n\nif\n \n(\ncheck\n)\n \nhappy\n            \n//\n\n\nif\n \n(\ncheck\n)\n \nhappy\n \nelse\n \n()\n    \n// same as above\n\n\nwhile\n \n(\nx\n \n<\n \n5\n)\n \n{\n \nprintln\n(\nx\n);\n \nx\n \n+=\n \n1\n}\n \n// while loop.\n\n\ndo\n \n{\n \nprintln\n(\nx\n);\n \nx\n \n+=\n \n1\n}\n \nwhile\n \n(\nx\n \n<\n \n5\n)\n  \n// do while loop.\n\n\n\nfor\n \n(\nx\n \n<-\n \nxs\n \nif\n \nx\n%\n2\n \n==\n \n0\n)\n \nyield\n \nx\n*\n10\n    \n// for comprehension with guard\n\n\nxs\n.\nfilter\n(\n_\n%\n2\n \n==\n \n0\n).\nmap\n(\n_\n*\n10\n)\n           \n// same as filter/map\n\n\nfor\n \n((\nx\n,\ny\n)\n \n<-\n \nxs\n \nzip\n \nys\n)\n \nyield\n \nx\n*\ny\n      \n// for comprehension: destructuring bind\n\n\n(\nxs\n \nzip\n \nys\n)\n \nmap\n \n{\n \ncase\n \n(\nx\n,\ny\n)\n \n=>\n \nx\n*\ny\n \n}\n   \n// same as\n\n\nfor\n \n(\nx\n \n<-\n \nxs\n;\n \ny\n \n<-\n \nys\n)\n \nyield\n \nx\n*\ny\n        \n// for comprehension: cross product. Later generators varying more rapidly than earlier ones\n\n\nxs\n \nflatMap\n \n{\nx\n \n=>\n \nys\n \nmap\n \n{\ny\n \n=>\n \nx\n*\ny\n}}\n     \n// same as\n\n\nfor\n \n(\nx\n \n<-\n \nxs\n;\n \ny\n \n<-\n \nys\n)\n \n{\n\n  \nprintln\n(\n\"%d/%d = %.1f\"\n.\nformat\n(\nx\n,\n \ny\n,\n \nx\n/\ny\n.\ntoFloat\n))\n     \n// for comprehension: imperative-ish\n\n\n}\n\n\nfor\n \n(\ni\n \n<-\n \n1\n \nto\n \n5\n)\n \n{\n                     \n// for comprehension: iterate including the upper bound\n\n  \nprintln\n(\ni\n)\n\n\n}\n\n\nfor\n \n(\ni\n \n<-\n \n1\n \nuntil\n \n5\n)\n \n{\n                  \n// for comprehension: iterate omitting the upper bound\n\n  \nprintln\n(\ni\n)\n\n\n}\n\n\n\nimport\n \nscala.util.control.Breaks._\n      \n// break\n\n\nbreakable\n \n{\n\n  \nfor\n \n(\nx\n \n<-\n \nxs\n)\n \n{\n\n    \nif\n \n(\nMath\n.\nrandom\n \n<\n \n0.1\n)\n \nbreak\n\n  \n}\n\n\n}\n\n\n\n\n\n\nFormatting and Interpolation\n\u00b6\n\n\nval\n \nhelloMessage\n \n=\n \n\"Hello World\"\n\n\ns\"Application \n$helloMessage\n\"\n  \n// string interpolation; can include expressions which can include numbers and strings\n\n\n// use `f` prefix before the string instead of an `s` for sprintf formatting\n\n\n\n\n\n\nFunctions\n\u00b6\n\n\nScala is a functional language in the sense that every function is a value and every value is an object so ultimately every function is an object.\nScala provides a lightweight syntax for defining anonymous functions, it supports higher-order functions, it allows functions to be nested, and supports currying.\n\n\ndef\n \nadd\n(\nx\n:\n \nInt\n,\n \ny\n:\n \nInt\n)\n:\n \nInt\n \n=\n \nx\n \n+\n \ny\n    \n// the return type is declared after the parameter list and a colon\n\n\n\n// GOOD def f(x: Any) = println(x)\n\n\n// BAD  def f(x) = println(x)           // syntax error: need types for every arg.\n\n\n\ndef\n \nf\n(\nx\n:\n \nInt\n)\n \n=\n \n{\n                       \n// inferred return type\n\n  \nval\n \nsquare\n \n=\n \nx\n*\nx\n\n  \nsquare\n.\ntoString\n\n  \n}\n \n// The last expression in the body is the method\u2019s return value. (Scala does have a return keyword, but it\u2019s rarely used.)\n\n\n\n// BAD def f(x: Int) { x*x }  hidden error: without = it\u2019s a Unit-returning procedure; causes havoc\n\n\n\n// When performing recursion, the return type on the method is mandatory!\n\n\n\n\n\n\n\n\nBackticks for reserved keywords and identifiers with a space (rare)\n\n\n\n\ndef\n \n`put employee on probation`\n(\nemployee\n:\n \nEmployee\n)\n \n=\n \n{\n\n       \nnew\n \nEmployee\n(\nemployee\n.\n`first name`\n,\n \nemployee\n.\n`last name`\n,\n \n\"Probation\"\n)\n\n    \n}\n\n\n\n\n\n\nMultiple parameter lists or none at all\n\u00b6\n\n\ndef\n \naddThenMultiply\n(\nx\n:\n \nInt\n,\n \ny\n:\n \nInt\n)(\nmultiplier\n:\n \nInt\n)\n:\n \nInt\n \n=\n \n(\nx\n \n+\n \ny\n)\n \n*\n \nmultiplier\n\n\ndef\n \nname\n:\n \nString\n \n=\n \nSystem\n.\ngetProperty\n(\n\"name\"\n)\n\n\n\n\n\n\nProcedures\n\u00b6\n\n\ndef\n \nfoo\n(\nx\n:\n \nInt\n)\n \n{\n \n//Note: No `=`; returns Unit\n\n      \nprint\n(\nx\n.\ntoString\n)\n\n    \n}\n\n\ndef\n \nfoo\n(\nx\n:\n \nInt\n)\n:\n \nUnit\n \n=\n  \nprint\n(\nx\n.\ntoString\n)\n  \n// or\n\n\n\n\n\n\nConvention (not required for the compiler) states that if you a call a method that returns a Unit / has a side effect, invoke that method with empty parenthesis, other leave the parenthesis out\n\n\ndef\n \nperformSideEffect\n()\n:\nUnit\n \n=\n \nSystem\n.\ncurrentTimeMillis\n\n\nperformSideEffect\n()\n\n\n\n\n\n\nDefault and named parameters\n\u00b6\n\n\ndef\n \naddColorsWithDefaults\n(\nred\n:\n \nInt\n \n=\n \n0\n,\n \ngreen\n:\n \nInt\n \n=\n \n0\n,\n \nblue\n:\n \nInt\n \n=\n \n0\n)\n \n=\n \n{\n\n  \n(\nred\n,\n \ngreen\n,\n \nblue\n)\n\n\n}\n\n\n\nme\n.\naddColors\n(\nblue\n \n=\n \n40\n)\n\n\n\n\n\n\nVariable Length Arguments\n\u00b6\n\n\ndef\n \nsum\n(\nargs\n:\n \nInt*\n)\n \n=\n \nargs\n.\nreduceLeft\n(\n_\n+\n_\n)\n              \n// varargs. must be last arg\n\n\n\ndef\n \ncapitalizeAll\n(\nargs\n:\n \nString*\n)\n \n=\n \n{\n\n      \nargs\n.\nmap\n \n{\n \narg\n \n=>\n\n        \narg\n.\ncapitalize\n\n      \n}\n\n    \n}\n\n\n\ncapitalizeAll\n(\n\"rarity\"\n,\n \n\"applejack\"\n)\n\n\n\n\n\n\nIf you want a collection expanded into a vararg, add \n:_*\n\n\ndef\n \nrepeatedParameterMethod\n(\nx\n:\n \nInt\n,\n \ny\n:\n \nString\n,\n \nz\n:\n \nAny*\n)\n \n=\n \n{\n\n    \n\"%d %ss can give you %s\"\n.\nformat\n(\nx\n,\n \ny\n,\n \nz\n.\nmkString\n(\n\", \"\n))\n\n  \n}\n\n\n\nrepeatedParameterMethod\n(\n3\n,\n \n\"egg\"\n,\n \nList\n(\n\"a delicious sandwich\"\n,\n \n\"protein\"\n,\n \n\"high cholesterol\"\n)\n:_\n*\n)\n \nshould\n \nbe\n(\n__\n)\n\n\n\n\n\n\nTail recursion\n\u00b6\n\n\nAs a precaution, the helpful @tailrec annotation will throw a compile time if a method is not tail recursive,\nmeaning that the last call and only call of the method is the recursive method. Scala optimizes recursive calls\nto a loop from a stack\n\n\nimport\n \nscala.annotation.tailrec\n \n//  importing annotation!\n\n\n@tailrec\n                        \n//  compiler will check that the function is tail recursive\n\n\ndef\n \nfactorial\n(\ni\n:\n \nBigInt\n)\n:\n \nBigInt\n \n=\n \n{\n\n      \n@tailrec\n\n      \ndef\n \nfact\n(\ni\n:\n \nBigInt\n,\n \naccumulator\n:\n \nBigInt\n)\n:\n \nBigInt\n \n=\n \n{\n  \n// methods can be placed inside in methods; return type is obligatory\n\n        \nif\n \n(\ni\n \n<=\n \n1\n)\n\n          \naccumulator\n\n        \nelse\n\n          \nfact\n(\ni\n \n-\n \n1\n,\n \ni\n \n*\n \naccumulator\n)\n\n      \n}\n\n      \nfact\n(\ni\n,\n \n1\n)\n\n    \n}\n\n\n\nfactorial\n(\n3\n)\n\n\n\n\n\n\nInfix, Postfix and Prefix Notations; Operators\n\u00b6\n\n\nobject\n \nFrenchDate\n \n{\n\n    \ndef\n \nmain\n(\nargs\n:\n \nArray\n[\nString\n])\n \n{\n\n        \nval\n \nnow\n \n=\n \nnew\n \nDate\n\n        \nval\n \ndf\n \n=\n \ngetDateInstance\n(\nLONG\n,\n \nLocale\n.\nFRANCE\n)\n\n    \nprintln\n(\ndf\n \nformat\n \nnow\n)\n       \n// Methods taking one argument can be used with an infix syntax. Equivalent to df.format(now)\n\n    \n}\n\n\n}\n\n\n\n\n\n\n1 + 2 * 3 / x\n consists exclusively of method calls, because it is equivalent to the following expression: \n(1).+(((2).*(3))./(x))\n\nThis also means that +, *, etc. are valid identifiers in Scala.\n\n\nInfix Operators do NOT work if an object has a method that takes two parameters.\n\n\n \nval\n \ng\n:\n \nInt\n \n=\n \n31\n\n \nval\n \ns\n:\n \nString\n \n=\n \ng\n \ntoHexString\n  \n// Postfix operators work if an object has a method that takes no parameters\n\n\n\n\n\n\nPrefix operators work if an object has a method name that starts with unary_\n\n\nclass\n \nStereo\n \n{\n\n      \ndef\n \nunary_+\n \n=\n \n\"on\"\n\n      \ndef\n \nunary_-\n \n=\n \n\"off\"\n\n    \n}\n\n\n\nval\n \nstereo\n \n=\n \nnew\n \nStereo\n\n\n+\nstereo\n   \n// it is on\n\n\n\n\n\n\nMethods with colons are right-associative, that means the object that a method is on will be on the \nright\n and the method parameter will be on the \nleft\n\n\nclass\n \nFoo\n \n(\ny\n:\nInt\n)\n \n{\n\n      \ndef\n \n~:(\nn\n:\nInt\n)\n \n=\n \nn\n \n+\n \ny\n \n+\n \n3\n\n    \n}\n\n\n\nval\n \nfoo\n \n=\n \nnew\n \nFoo\n(\n9\n)\n\n\n10\n \n~:\n \nfoo\n\n\nfoo\n.~:(\n10\n)\n  \n// same as\n\n\n\n\n\n\nAnonymous Functions\n\u00b6\n\n\ndef\n \nlambda\n \n=\n \n(\nx\n:\n \nInt\n)\n \n=>\n \nx\n \n+\n \n1\n\n\n\n// other variants\n\n\ndef\n \nlambda2\n \n=\n \n{\n \nx\n:\n \nInt\n \n=>\n \nx\n \n+\n \n1\n \n}\n\n\nval\n \nlambda3\n \n=\n \nnew\n \nFunction1\n[\nInt\n, \nInt\n]\n \n{\n\n      \ndef\n \napply\n(\nv1\n:\n \nInt\n)\n:\n \nInt\n \n=\n \nv1\n \n+\n \n1\n\n    \n}\n\n\n\nval\n \neverything\n \n=\n \n()\n \n=>\n \n42\n                       \n// without parameter\n\n\nval\n \nadd\n \n=\n \n(\nx\n:\n \nInt\n,\n \ny\n:\n \nInt\n)\n \n=>\n \nx\n \n+\n \ny\n             \n// multiple parameters\n\n\n\n(\n1\n \nto\n \n5\n).\nmap\n(\n_\n*\n2\n)\n                               \n// underscore notation.\n\n\n(\n1\n \nto\n \n5\n)\n \nmap\n \n(\n_\n*\n2\n)\n                              \n// same with infix sugar.\n\n\n(\n1\n \nto\n \n5\n).\nreduceLeft\n(\n \n_\n+\n_\n \n)\n                      \n// underscores are positionally matched 1st and 2nd args.\n\n\n(\n1\n \nto\n \n5\n).\nmap\n(\n \nx\n \n=>\n \nx\n*\nx\n \n)\n                        \n// to use an arg twice, have to name it.\n\n\n(\n1\n \nto\n \n5\n).\nmap\n \n{\n \nx\n \n=>\n \nval\n \ny\n \n=\n \nx\n*\n2\n;\n \nprintln\n(\ny\n);\n \ny\n \n}\n    \n// block style returns last expression.\n\n\n(\n1\n \nto\n \n5\n)\n \nfilter\n \n{\n_\n%\n2\n \n==\n \n0\n}\n \nmap\n \n{\n_\n*\n2\n}\n            \n// pipeline style (works with parens too).\n\n\n\n// GOOD (1 to 5).map(2*)\n\n\n// BAD (1 to 5).map(*2)                         // anonymous function: bound infix method. Use 2*_ for sanity\u2019s sake instead.\n\n\n\ndef\n \ncompose\n(\ng\n:\n \nR\n \n=>\n \nR\n,\n \nh\n:\n \nR\n \n=>\n \nR\n)\n \n=\n \n(\nx\n:\nR\n)\n \n=>\n \ng\n(\nh\n(\nx\n))\n\n\nval\n \nf\n \n=\n \ncompose\n({\n_\n*\n2\n},\n \n{\n_\n-\n1\n})\n                   \n// anonymous functions: to pass in multiple blocks, need outer parens.\n\n\n\n\n\n\nPassing anonymous functions as parameter:\n\n\ndef\n \nmakeWhatEverYouLike\n(\nxs\n:\n \nList\n[\nString\n],\n \nfunc\n:\n \nString\n \n=>\n \nString\n)\n \n=\n \n{\n\n      \nxs\n \nmap\n \nfunc\n\n    \n}\n\n\n\n\n\n\nFunction returning another function using an anonymous function:\n\n\ndef\n \nadd\n(\nx\n:\n \nInt\n)\n \n=\n \n(\ny\n:\nInt\n)\n \n=>\n \nx\n \n+\n \ny\n\n\n\n\n\n\nFunction Values:\n\n\nobject\n \nTimer\n \n{\n\n    \ndef\n \noncePerSecond\n(\ncallback\n:\n \n()\n \n=>\n \nUnit\n)\n \n{\n        \n// () => T is a Function type that takes a Unit type. Unit is known as 'void' to a Java programmer.\n\n        \nwhile\n \n(\ntrue\n)\n \n{\n \ncallback\n();\n \nThread\n \nsleep\n \n1000\n \n}\n\n    \n}\n\n\n    \ndef\n \ntimeFlies\n()\n \n{\n\n        \nprintln\n(\n\"time flies like an arrow...\"\n)\n\n    \n}\n\n\n    \ndef\n \nmain\n(\nargs\n:\n \nArray\n[\nString\n])\n \n{\n\n        \noncePerSecond\n(\ntimeFlies\n)\n       \n// function value; could also be () => timeFlies()\n\n    \n}\n\n\n}\n\n\n\n\n\n\nBy-name parameter\n\u00b6\n\n\nThis is used extensively in scala to create blocks.\n\n\n    \ndef\n \ncalc\n(\nx\n:\n \n=>\n \nInt\n)\n:\n \nEither\n[\nThrowable\n, \nInt\n]\n \n=\n \n{\n   \n//x is a call by name parameter; delayed execution of x\n\n      \ntry\n \n{\n\n        \nRight\n(\nx\n)\n\n      \n}\n \ncatch\n \n{\n\n        \ncase\n \nb\n:\n \nThrowable\n \n=>\n \nLeft\n(\nb\n)\n\n      \n}\n\n    \n}\n\n\n    \nval\n \ny\n \n=\n \ncalc\n \n{\n                                    \n//This looks like a natural block\n\n      \nprintln\n(\n\"Here we go!\"\n)\n                          \n//Some superfluous call\n\n      \n49\n \n+\n \n20\n\n    \n}\n\n\n\n\n\n\nBy name parameters can also be used with an Object and apply to make interesting block-like calls\n\n\nobject\n \nPigLatinizer\n \n{\n\n      \ndef\n \napply\n(\nx\n:\n \n=>\n \nString\n)\n \n=\n \nx\n.\ntail\n \n+\n \nx\n.\nhead\n \n+\n \n\"ay\"\n\n    \n}\n\n\n\nval\n \nresult\n \n=\n \nPigLatinizer\n \n{\n\n      \nval\n \nx\n \n=\n \n\"pret\"\n\n      \nval\n \nz\n \n=\n \n\"zel\"\n\n      \nx\n \n++\n \nz\n \n//concatenate the strings\n\n    \n}\n\n\n\n\n\n\nClosures\n\u00b6\n\n\nvar\n \nincrementer\n \n=\n \n1\n\n\n\ndef\n \nclosure\n \n=\n \n{\n\n  \nx\n:\n \nInt\n \n=>\n \nx\n \n+\n \nincrementer\n\n\n}\n\n\n\n\n\n\nCurrying\n\u00b6\n\n\nval\n \nzscore\n \n=\n \n(\nmean\n:\n \nR\n,\n \nsd\n:\n \nR\n)\n \n=>\n \n(\nx\n:\nR\n)\n \n=>\n \n(\nx\n-\nmean\n)/\nsd\n       \n// currying, obvious syntax.\n\n\ndef\n \nzscore\n(\nmean\n:\n \nR\n,\n \nsd\n:\n \nR\n)\n \n=\n \n(\nx\n:\n \nR\n)\n \n=>\n \n(\nx\n-\nmean\n)/\nsd\n          \n// currying, obvious syntax\n\n\ndef\n \nzscore\n(\nmean\n:\n \nR\n,\n \nsd\n:\n \nR\n)(\nx\n:\n \nR\n)\n \n=\n \n(\nx\n-\nmean\n)/\nsd\n              \n// currying, sugar syntax. but then:\n\n\nval\n \nnormer\n \n=\n \nzscore\n(\n7\n,\n \n0.4\n)\n \n_\n                               \n// need trailing underscore to get the partial, only for the sugar version.\n\n\ndef\n \nmapmake\n[\nT\n](\ng\n:\n \nT\n \n=>\n \nT\n)(\nseq\n:\n \nList\n[\nT\n])\n \n=\n \nseq\n.\nmap\n(\ng\n)\n        \n// generic type.\n\n\n\ndef\n \nmultiply\n(\nx\n:\n \nInt\n,\n \ny\n:\n \nInt\n)\n \n=\n \nx\n \n*\n \ny\n\n\nval\n \nmultiplyCurried\n \n=\n \n(\nmultiply\n \n_\n).\ncurried\n                  \n\nmultiply\n(\n4\n,\n \n5\n)\n\n\nmultiplyCurried\n(\n3\n)(\n2\n)\n\n\n\n\n\n\nPartial Applications\n\u00b6\n\n\ndef\n \nadder\n(\nm\n:\n \nInt\n,\n \nn\n:\n \nInt\n)\n \n=\n \nm\n \n+\n \nn\n\n\nval\n \nadd2\n \n=\n \nadder\n(\n2\n,\n \n_:\nInt\n)\n  \n// You can partially apply any argument in the argument list, not just the last one.\n\n\nadd2\n(\n3\n)\n    \n// which is 5\n\n\n\nval\n \nadd3\n \n=\n \nadder\n \n_\n              \n// underscore to convert from a function to a lambda\n\n\nadder\n(\n1\n,\n \n9\n)\n\n\nadd3\n(\n1\n,\n \n9\n)\n\n\n\n\n\n\nPartial Functions\n\u00b6\n\n\nval\n \ndoubleEvens\n:\n \nPartialFunction\n[\nInt\n, \nInt\n]\n \n=\n \nnew\n \nPartialFunction\n[\nInt\n, \nInt\n]\n \n{\n   \n// full declaration\n\n      \n//States that this partial function will take on the task\n\n      \ndef\n \nisDefinedAt\n(\nx\n:\n \nInt\n)\n \n=\n \nx\n \n%\n \n2\n \n==\n \n0\n\n\n      \n//What we do if this does partial function matches\n\n      \ndef\n \napply\n(\nv1\n:\n \nInt\n)\n \n=\n \nv1\n \n*\n \n2\n\n    \n}\n\n\n\nval\n \ntripleOdds\n:\n \nPartialFunction\n[\nInt\n, \nInt\n]\n \n=\n \n{\n\n      \ncase\n \nx\n:\n \nInt\n \nif\n \n(\nx\n \n%\n \n2\n)\n \n!=\n \n0\n \n=>\n \nx\n \n*\n \n3\n              \n// syntaxic sugar (usual way)\n\n    \n}\n\n\n\nval\n \nwhatToDo\n \n=\n \ndoubleEvens\n \norElse\n \ntripleOdds\n            \n//  combine the partial functions together: OrElse\n\n\n\nval\n \naddFive\n \n=\n \n(\nx\n:\n \nInt\n)\n \n=>\n \nx\n \n+\n \n5\n\n\nval\n \nwhatToDo\n \n=\n \ndoubleEvens\n \norElse\n \ntripleOdds\n \nandThen\n \naddFive\n  \n// chain (partial) functions together: andThen\n\n\n\n\n\n\nClasses, Objects, and Traits\n\u00b6\n\n\nclass\n \nC\n(\nx\n:\n \nR\n)\n                       \n// constructor params - x is only available in class body\n\n\nclass\n \nC\n(\nval\n \nx\n:\n \nR\n)\n                   \n// c.x  constructor params - automatic public (immutable) member defined\n\n\nclass\n \nD\n(\nvar\n \nx\n:\n \nR\n)\n                   \n// you can define class with var or val parameters\n\n\n\nclass\n \nC\n(\nvar\n \nx\n:\n \nR\n)\n \n{\n\n  \nassert\n(\nx\n \n>\n \n0\n,\n \n\"positive please\"\n)\n  \n// constructor is class body\n\n  \nvar\n \ny\n \n=\n \nx\n                         \n// declare a public member\n\n  \nval\n \nreadonly\n \n=\n \n5\n                  \n// declare a gettable but not settable member\n\n  \nprivate\n \nvar\n \nsecret\n \n=\n \n1\n            \n// declare a private member\n\n  \ndef\n \nthis\n \n=\n \nthis\n(\n42\n)\n               \n// alternative constructor\n\n\n}\n\n\n\nnew\n{\n \n...\n \n}\n  \n// anonymous class\n\n\nabstract\n \nclass\n \nD\n \n{\n \n...\n \n}\n    \n// define an abstract(non-createable) class.\n\n\nclass\n \nC\n \nextends\n \nD\n \n{\n \n...\n \n}\n   \n// define an inherited class. Class hierarchy is linear, a class can only extend from one parent class\n\n\nclass\n \nC\n(\nx\n:\n \nR\n)\n \nextends\n \nD\n(\nx\n)\n  \n// inheritance and constructor params. (wishlist: automatically pass-up params by default)\n\n\n// A class can be placed inside another class\n\n\nobject\n \nO\n \nextends\n \nD\n \n{\n \n...\n \n}\n  \n// define a singleton.\n\n\n\ntrait\n \nT\n \n{\n \n...\n \n}\n             \n// traits. See below.\n\n\nclass\n \nC\n \nextends\n \nT\n \n{\n \n...\n \n}\n\n\nclass\n \nC\n \nextends\n \nD\n \nwith\n \nT\n \n{\n \n...\n \n}\n\n\n\n// interfaces-with-implementation. no constructor params. mixin-able.\n\n\ntrait\n \nT1\n;\n \ntrait\n \nT2\n\n\nclass\n \nC\n \nextends\n \nT1\n \nwith\n \nT2\n          \n// multiple traits.\n\n\nclass\n \nC\n \nextends\n \nD\n \nwith\n \nT1\n \nwith\n \nT2\n   \n// parent class and (multiple) trait(s).\n\n\nclass\n \nC\n \nextends\n \nD\n \n{\n \noverride\n \ndef\n \nf\n \n=\n \n...}\n   \n// must declare method overrides.\n\n\n\nvar\n \nc\n \n=\n \nnew\n \nC\n(\n4\n)\n        \n// Instantiation\n\n\n//BAD new List[Int]\n\n\n//GOOD List(1,2,3)      // Instead, convention: callable factory shadowing the type\n\n\n\nclassOf\n[\nString\n]\n \n// class literal.\n\n\nclassOf\n[\nString\n].\ngetCanonicalName\n\n\nclassOf\n[\nString\n].\ngetSimpleName\n\n\nval\n \nzoom\n \n=\n \n\"zoom\"\n\n\nzoom\n.\ngetClass\n \n==\n \nclassOf\n[\nString\n]\n\n\n\nx\n.\nisInstanceOf\n[\nString\n]\n  \n// type check (runtime)\n\n\nx\n.\nasInstanceOf\n[\nString\n]\n  \n// type cast (runtime)\n\n\nx\n:\n \nString\n               \n// compare to parameter ascription (compile time)\n\n\n\n\n\n\nMethods\n\u00b6\n\n\nclass\n \nComplex\n(\nreal\n:\n \nDouble\n,\n \nimaginary\n:\n \nDouble\n)\n \n{\n\n    \ndef\n \nre\n \n=\n \nreal\n       \n// return type inferred automatically by the compiler\n\n    \ndef\n \nim\n \n=\n \nimaginary\n  \n// methods without arguments\n\n    \ndef\n \nprint\n()\n:\n \nUnit\n \n=\n \nprintln\n(\ns\"\n$real\n + i * \n$imaginary\n\"\n)\n\n    \noverride\n \ndef\n \ntoString\n()\n \n=\n \n\"\"\n \n+\n \nre\n \n+\n \n(\nif\n \n(\nim\n \n<\n \n0\n)\n \n\"\"\n \nelse\n \n\"+\"\n)\n \n+\n \nim\n \n+\n \n\"i\"\n  \n// override methods inherited from a super-class\n\n\n}\n\n\n\n\n\n\nAsserts and Contracts\n\u00b6\n\n\nAsserts take a boolean argument and can take a message.\n\n\nassert\n(\ntrue\n)\n \n// should be true\n\n\nassert\n(\ntrue\n,\n \n\"This should be true\"\n)\n\n\n\n\n\n\ndef\n \naddNaturals\n(\nnats\n:\n \nList\n[\nInt\n])\n:\n \nInt\n \n=\n \n{\n\n  \nrequire\n(\nnats\n \nforall\n \n(\n_\n \n>=\n \n0\n),\n \n\"List contains negative numbers\"\n)\n\n  \nnats\n.\nfoldLeft\n(\n0\n)(\n_\n \n+\n \n_\n)\n\n\n}\n \nensuring\n(\n_\n \n>=\n \n0\n)\n\n\n\n\n\n\nPath-dependent Classes\n\u00b6\n\n\nWhen a class is instantiated inside of another object, it belongs to the instance.  This is a path dependent type. Once established, it cannot be placed inside of another object\n\n\ncase\n \nclass\n \nBoard\n(\nlength\n:\n \nInt\n,\n \nheight\n:\n \nInt\n)\n \n{\n\n  \ncase\n \nclass\n \nCoordinate\n(\nx\n:\n \nInt\n,\n \ny\n:\n \nInt\n)\n\n\n}\n\n\n\nval\n \nb1\n \n=\n \nBoard\n(\n20\n,\n \n20\n)\n\n\nval\n \nb2\n \n=\n \nBoard\n(\n30\n,\n \n30\n)\n\n\nval\n \nc1\n \n=\n \nb1\n.\nCoordinate\n(\n15\n,\n \n15\n)\n\n\nval\n \nc2\n \n=\n \nb2\n.\nCoordinate\n(\n25\n,\n \n25\n)\n\n\n// val c1 = c2  won't work\n\n\n\n\n\n\nUse \nA#B\n for a Java-style inner class:\n\n\nclass\n \nGraph\n \n{\n\n  \nclass\n \nNode\n \n{\n\n    \nvar\n \nconnectedNodes\n:\n \nList\n[\nGraph\n#\nNode\n]\n \n=\n \nNil\n          \n// accepts Nodes from any Graph\n\n    \ndef\n \nconnectTo\n(\nnode\n:\n \nGraph\n#\nNode\n)\n \n{\n\n      \nif\n \n(\nconnectedNodes\n.\nfind\n(\nnode\n.\nequals\n).\nisEmpty\n)\n \n{\n\n        \nconnectedNodes\n \n=\n \nnode\n \n::\n \nconnectedNodes\n\n      \n}\n\n    \n}\n\n  \n}\n\n  \nvar\n \nnodes\n:\n \nList\n[\nNode\n]\n \n=\n \nNil\n\n  \ndef\n \nnewNode\n:\n \nNode\n \n=\n \n{\n\n    \nval\n \nres\n \n=\n \nnew\n \nNode\n\n    \nnodes\n \n=\n \nres\n \n::\n \nnodes\n\n    \nres\n\n  \n}\n\n\n}\n\n\n\n\n\n\nCompanion Objects\n\u00b6\n\n\nStatic members (methods or fields) do not exist in Scala. Rather than defining static members, the Scala programmer declares these members in singleton objects, that is a class with a single instance.\n\n\nobject\n \nTimerAnonymous\n \n{\n\n    \ndef\n \noncePerSecond\n(\ncallback\n:\n \n()\n \n=>\n \nUnit\n)\n \n{\n\n        \nwhile\n \n(\ntrue\n)\n \n{\n \ncallback\n();\n \nThread\n \nsleep\n \n1000\n \n}\n\n    \n}\n\n    \ndef\n \nmain\n(\nargs\n:\n \nArray\n[\nString\n])\n \n{\n\n        \noncePerSecond\n(()\n \n=>\n \nprintln\n(\n\"time flies like an arrow...\"\n))\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\n\nAn object that has the same name as class is called a companion object, it is used to contain factories for the class that it complements.\n\n\nA companion object can also store shared variables and values for every instantiated class to share.\n\n\nA companion object can see private values and variables of the instantiated object\n\n\n\n\nApply Method\n\u00b6\n\n\nThe apply method is a magical method in Scala.\n\n\nclass\n \nEmployee\n \n(\nval\n \nfirstName\n:\nString\n,\n \nval\n \nlastName\n:\nString\n)\n\n\n\nobject\n \nEmployee\n \n{\n\n    \ndef\n \napply\n(\nfirstName\n:\nString\n,\n \nlastName\n:\nString\n)\n \n=\n \nnew\n \nEmployee\n(\nfirstName\n,\n \nlastName\n)\n  \n// would also work in a class, but rarer\n\n\n}\n\n\n\nval\n \na\n \n=\n \nEmployee\n(\n\"John\"\n,\n \n\"Doe\"\n)\n\n\n// is equivalent to\n\n\nvar\n \nb\n \n=\n \nEmployee\n.\napply\n(\n\"John\"\n,\n \n\"Doe\"\n)\n\n\n\n\n\n\nCase Classes\n\u00b6\n\n\n\n\nThe \nnew\n keyword is not mandatory to create instances of these classes (i.e. one can write Const(5) instead of new Const(5)),\n\n\nGetter functions are automatically defined for the constructor parameters (i.e. it is possible to get the value of the v constructor parameter of some instance c of class Const just by writing c.v),\n\n\nDefault definitions for methods equals and hashCode are provided, which work on the structure of the instances and not on their identity,\n\n\nA default definition for method toString is provided, and prints the value in a source form (e.g. the tree for expression x+1 prints as Sum(Var(x),Const(1))),\n\n\nInstances of these classes can be decomposed through pattern matching\n\n\n\n\ncase\n \nclass\n \nPerson\n(\nfirst\n:\n \nString\n,\n \nlast\n:\n \nString\n,\n \nage\n:\n \nInt\n \n=\n \n0\n)\n  \n// Case classes can have default and named parameters\n\n\nval\n \np1\n \n=\n \nPerson\n(\n\"Fred\"\n,\n \n\"Jones\"\n)\n    \n// new is optional\n\n\nval\n \np2\n \n=\n \nnew\n \nPerson\n(\n\"Fred\"\n,\n \n\"Jones\"\n)\n\n\np1\n \n==\n \np2\n                            \n// true\n\n\np1\n.\nhashCode\n \n==\n \np2\n.\nhashCode\n          \n// true\n\n\np1\n \neq\n \np2\n                            \n// false\n\n\nval\n \np3\n \n=\n \np2\n.\ncopy\n(\nfirst\n \n=\n \n\"Jane\"\n)\n    \n// copy the case class but change the name in the copy\n\n\n\n\n\n\ncase\n \nclass\n \nDog\n(\nvar\n \nname\n:\n \nString\n,\n \nbreed\n:\n \nString\n)\n     \n// Case classes can have mutable properties - potentially unsafe\n\n\n\n\n\n\nCase classes can be disassembled to their constituent parts as a tuple:\n\n\nval\n \nparts\n \n=\n \nPerson\n.\nunapply\n(\np1\n).\nget\n \n// returns Option[T]\n\n\nparts\n.\n_1\n\n\nparts\n.\n_2\n\n\n\n\n\n\nAlgebraic data type\n\u00b6\n\n\nsealed\n \ntrait\n \nTree\n    \n// or abstract class\n\n\nfinal\n \ncase\n \nclass\n \nSum\n(\nl\n:\n \nTree\n,\n \nr\n:\n \nTree\n)\n \nextends\n \nTree\n\n\nfinal\n \ncase\n \nclass\n \nVar\n(\nn\n:\n \nString\n)\n \nextends\n \nTree\n\n\nfinal\n \ncase\n \nclass\n \nConst\n(\nv\n:\n \nInt\n)\n \nextends\n \nTree\n\n\n\n\n\n\nPattern Matching\n\u00b6\n\n\n{ case \"x\" => 5 }\n defines a partial function which, when given the string \"x\" as argument, returns the integer 5, and fails with an exception otherwise.\n\n\ntype\n \nEnvironment\n \n=\n \nString\n \n=>\n \nInt\n  \n// the type Environment can be used as an alias of the type of functions from String to Int\n\n\n\ndef\n \neval\n(\nt\n:\n \nTree\n,\n \nenv\n:\n \nEnvironment\n)\n:\n \nInt\n \n=\n \nt\n \nmatch\n \n{\n\n    \ncase\n \nSum\n(\nl\n,\n \nr\n)\n \n=>\n \neval\n(\nl\n,\n \nenv\n)\n \n+\n \neval\n(\nr\n,\n \nenv\n)\n\n    \ncase\n \nVar\n(\nn\n)\n \n=>\n \nenv\n(\nn\n)\n\n    \ncase\n \nConst\n(\nv\n)\n \n=>\n \nv\n\n\n}\n\n\n\ndef\n \nderive\n(\nt\n:\n \nTree\n,\n \nv\n:\n \nString\n)\n:\n \nTree\n \n=\n \nt\n \nmatch\n \n{\n\n    \ncase\n \nSum\n(\nl\n,\n \nr\n)\n \n=>\n \nSum\n(\nderive\n(\nl\n,\n \nv\n),\n \nderive\n(\nr\n,\n \nv\n))\n\n    \ncase\n \nVar\n(\nn\n)\n \nif\n \n(\nv\n \n==\n \nn\n)\n \n=>\n \nConst\n(\n1\n)\n                  \n// guard, an expression following the if keyword.\n\n    \ncase\n \n_\n \n=>\n \nConst\n(\n0\n)\n                                   \n// wild-card, written _, which is a pattern matching any value, without giving it a name.\n\n\n}\n\n\n\n\n\n\n// GOOD (xs zip ys) map { case (x,y) => x*y }\n\n\n// BAD (xs zip ys) map( (x,y) => x*y )  // use case in function args for pattern matching.\n\n\n// BAD\n\n\nval\n \nv42\n \n=\n \n42\n\n\nSome\n(\n3\n)\n \nmatch\n \n{\n\n  \ncase\n \nSome\n(\nv42\n)\n \n=>\n \nprintln\n(\n\"42\"\n)\n\n  \ncase\n \n_\n \n=>\n \nprintln\n(\n\"Not 42\"\n)\n\n\n}\n   \n// \u201cv42\u201d is interpreted as a name matching any Int value, and \u201c42\u201d is printed.\n\n\n// GOOD\n\n\nval\n \nv42\n \n=\n \n42\n\n\nSome\n(\n3\n)\n \nmatch\n \n{\n\n  \ncase\n \nSome\n(\n`v42`\n)\n \n=>\n \nprintln\n(\n\"42\"\n)\n\n  \ncase\n \n_\n \n=>\n \nprintln\n(\n\"Not 42\"\n)\n\n\n}\n   \n// \u201d`v42`\u201d with backticks is interpreted as the existing val v42, and \u201cNot 42\u201d is printed.\n\n\n// GOOD\n\n\nval\n \nUppercaseVal\n \n=\n \n42\n\n\nSome\n(\n3\n)\n \nmatch\n \n{\n\n  \ncase\n \nSome\n(\nUppercaseVal\n)\n \n=>\n \nprintln\n(\n\"42\"\n)\n\n  \ncase\n \n_\n \n=>\n \nprintln\n(\n\"Not 42\"\n)\n\n\n}\n   \n// UppercaseVal is treated as an existing val, rather than a new pattern variable, because it starts with an uppercase letter.\n\n\n// Thus, the value contained within UppercaseVal is checked against 3, and \u201cNot 42\u201d is printed.\n\n\n\n\n\n\nList Matching\n\u00b6\n\n\nval\n \nsecondElement\n \n=\n \nList\n(\n1\n,\n2\n,\n3\n)\n \nmatch\n \n{\n\n      \ncase\n \nx\n \n::\n \ny\n \n::\n \nxs\n \n=>\n \nxs\n\n      \ncase\n \nx\n \n::\n \nNil\n \n=>\n \nx\n\n      \ncase\n \n_\n \n=>\n \n0\n\n    \n}\n\n\n\n\n\n\nRegex\n\u00b6\n\n\nval\n \nMyRegularExpression\n \n=\n \n\"\"\"a=([^,]+),\\s+b=(.+)\"\"\"\n.\nr\n           \n//.r turns a String to a regular expression\n\n\nexpr\n \nmatch\n \n{\n\n      \ncase\n \n(\nMyRegularExpression\n(\na\n,\n \nb\n))\n \n=>\n \na\n \n+\n \nb\n\n      \n}\n\n\n\n\n\n\nimport\n \nscala.util.matching.Regex\n\n\n\nval\n \nnumberPattern\n:\n \nRegex\n \n=\n \n\"[0-9]\"\n.\nr\n\n\n\nnumberPattern\n.\nfindFirstMatchIn\n(\n\"awesomepassword\"\n)\n \nmatch\n \n{\n\n  \ncase\n \nSome\n(\n_\n)\n \n=>\n \nprintln\n(\n\"Password OK\"\n)\n\n  \ncase\n \nNone\n \n=>\n \nprintln\n(\n\"Password must contain a number\"\n)\n\n\n}\n\n\n\n\n\n\nWith groups:\n\n\nval\n \nkeyValPattern\n:\n \nRegex\n \n=\n \n\"([0-9a-zA-Z-#() ]+): ([0-9a-zA-Z-#() ]+)\"\n.\nr\n\n\n\nfor\n \n(\npatternMatch\n \n<-\n \nkeyValPattern\n.\nfindAllMatchIn\n(\ninput\n))\n\n  \nprintln\n(\ns\"key: \n${\npatternMatch\n.\ngroup\n(\n1\n)\n}\n value: \n${\npatternMatch\n.\ngroup\n(\n2\n)\n}\n\"\n)\n\n\n\n\n\n\nExtractors (unapply)\n\u00b6\n\n\nclass\n \nCar\n(\nval\n \nmake\n:\n \nString\n,\n \nval\n \nmodel\n:\n \nString\n,\n \nval\n \nyear\n:\n \nShort\n,\n \nval\n \ntopSpeed\n:\n \nShort\n)\n\n\n\nobject\n \nCar\n \n{\n                                                        \n// What is typical is to create a custom extractor in the companion object of the class.\n\n  \ndef\n \nunapply\n(\nx\n:\n \nCar\n)\n \n=\n \nSome\n(\nx\n.\nmake\n,\n \nx\n.\nmodel\n,\n \nx\n.\nyear\n,\n \nx\n.\ntopSpeed\n)\n   \n// returns an Option[T]\n\n\n}\n\n\n\nval\n \nCar\n(\na\n,\n \nb\n,\n \nc\n,\n \nd\n)\n \n=\n \nnew\n \nCar\n(\n\"Chevy\"\n,\n \n\"Camaro\"\n,\n \n1978\n,\n \n120\n)\n     \n// assign values to a .. d\n\n\n\nval\n \nx\n \n=\n \nnew\n \nCar\n(\n\"Chevy\"\n,\n \n\"Camaro\"\n,\n \n1978\n,\n \n120\n)\n \nmatch\n \n{\n           \n// pattern matching\n\n  \ncase\n \nCar\n(\ns\n,\n \nt\n,\n \n_\n,\n \n_\n)\n \n=>\n \n(\ns\n,\n \nt\n)\n                                \n// _ for variables we don't care about.\n\n  \ncase\n \n_\n \n=>\n \n(\n\"Ford\"\n,\n \n\"Edsel\"\n)\n                                   \n// fallback\n\n\n}\n\n\n\n\n\n\n\n\nAs long as the method signatures aren't the same, you can have an many unapply methods as you want in the same class / object.\n\n\nWhen you create a case class, it automatically can be used with pattern matching since it has an extractor.\n\n\n\n\nValue Class\n\u00b6\n\n\nAvoid allocating runtime objects.\n\n\nclass\n \nWrapper\n(\nval\n \nunderlying\n:\n \nInt\n)\n \nextends\n \nAnyVal\n \n{\n\n  \ndef\n \nfoo\n:\n \nWrapper\n \n=\n \nnew\n \nWrapper\n(\nunderlying\n \n*\n \n19\n)\n\n\n}\n\n\n\n\n\n\nIt has a single, public val parameter that is the underlying runtime representation. The type at compile time is Wrapper, but at runtime, the representation is an Int. A value class can define defs, but no vals, vars, or nested traitss, classes or objects\n\n\nA value class can only extend universal traits and cannot be extended itself. A universal trait is a trait that extends Any, only has defs as members, and does no initialization. Universal traits allow basic inheritance of methods for value classes, but they incur the overhead of allocation.\n\n\nTraits\n\u00b6\n\n\nApart from inheriting code from a super-class, a Scala class can also import code from one or several traits i.e. interfaces which can also contain code.\nIn Scala, when a class inherits from a trait, it implements that traits's interface, and inherits all the code contained in the trait.\n\n\ntrait\n \nOrd\n \n{\n\n    \ndef\n \n<\n \n(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n                              \n// The type Any which is used above is the type which is a super-type of all other types in Scala\n\n    \ndef\n \n<=(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n \n=\n \n(\nthis\n \n<\n \nthat\n)\n \n||\n \n(\nthis\n \n==\n \nthat\n)\n\n    \ndef\n \n>\n \n(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n \n=\n \n!(\nthis\n \n<=\n \nthat\n)\n\n    \ndef\n \n>=(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n \n=\n \n!(\nthis\n \n<\n \nthat\n)\n\n\n}\n\n\n\nclass\n \nDate\n(\ny\n:\n \nInt\n,\n \nm\n:\n \nInt\n,\n \nd\n:\n \nInt\n)\n \nextends\n \nOrd\n \n{\n\n    \ndef\n \nyear\n \n=\n \ny\n\n    \ndef\n \nmonth\n \n=\n \nm\n\n    \ndef\n \nday\n \n=\n \nd\n\n    \noverride\n \ndef\n \ntoString\n()\n:\n \nString\n \n=\n \nyear\n \n+\n \n\"-\"\n \n+\n \nmonth\n \n+\n \n\"-\"\n \n+\n \nday\n\n\n    \noverride\n \ndef\n \nequals\n(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n \n=\n\n        \nthat\n.\nisInstanceOf\n[\nDate\n]\n \n&&\n \n{\n\n        \nval\n \no\n \n=\n \nthat\n.\nasInstanceOf\n[\nDate\n]\n\n        \no\n.\nday\n \n==\n \nday\n \n&&\n \no\n.\nmonth\n \n==\n \nmonth\n \n&&\n \no\n.\nyear\n \n==\n \nyear\n\n    \n}\n\n\n    \ndef\n \n<(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n \n=\n \n{\n    \n// The trait declare the type (e.g. method), where a concrete implementer will satisfy the type\n\n        \nif\n \n(!\nthat\n.\nisInstanceOf\n[\nDate\n])\n\n            \nerror\n(\n\"cannot compare \"\n \n+\n \nthat\n \n+\n \n\" and a Date\"\n)\n\n        \nval\n \no\n \n=\n \nthat\n.\nasInstanceOf\n[\nDate\n](\nyear\n \n<\n \no\n.\nyear\n)\n \n||\n\n            \n(\nyear\n \n==\n \no\n.\nyear\n \n&&\n \n(\nmonth\n \n<\n \no\n.\nmonth\n \n||\n\n            \n(\nmonth\n \n==\n \no\n.\nmonth\n \n&&\n \nday\n \n<\n \no\n.\nday\n    \n)))\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\n\n\n\nTraits can have concrete implementations that can be mixed into concrete classes with its own state\n\n\n\n\n\n\nTraits can be mixed in during instantiation!\n\n\n\n\n\n\ntrait\n \nLogging\n \n{\n\n      \nvar\n \nlogCache\n \n=\n \nList\n[\nString\n]()\n\n\n      \ndef\n \nlog\n(\nvalue\n:\n \nString\n)\n \n=\n \n{\n\n        \nlogCache\n \n=\n \nlogCache\n \n:+\n \nvalue\n\n      \n}\n\n\n      \ndef\n \nlog\n \n=\n \nlogCache\n\n    \n}\n\n\nval\n \na\n \n=\n \nnew\n \nA\n(\n\"stuff\"\n)\n \nwith\n \nLogging\n  \n// mixin traits during instantiation!\n\n\na\n.\nlog\n(\n\"I did something\"\n)\n\n\na\n.\nlog\n.\nsize\n\n\n\n\n\n\nStackable Traits\n\u00b6\n\n\nabstract\n \nclass\n \nIntQueue\n \n{\n\n  \ndef\n \nget\n()\n:\n \nInt\n\n  \ndef\n \nput\n(\nx\n:\n \nInt\n)\n\n\n}\n\n\n\nimport\n \nscala.collection.mutable.ArrayBuffer\n\n\n\nclass\n \nBasicIntQueue\n \nextends\n \nIntQueue\n \n{\n\n  \nprivate\n \nval\n \nbuf\n \n=\n \nnew\n \nArrayBuffer\n[\nInt\n]\n\n  \ndef\n \nget\n()\n \n=\n \nbuf\n.\nremove\n(\n0\n)\n\n  \ndef\n \nput\n(\nx\n:\n \nInt\n)\n \n{\n \nbuf\n \n+=\n \nx\n \n}\n\n\n}\n\n\n\ntrait\n \nDoubling\n \nextends\n \nIntQueue\n \n{\n\n  \nabstract\n \noverride\n \ndef\n \nput\n(\nx\n:\n \nInt\n)\n \n{\n \nsuper\n.\nput\n(\n2\n \n*\n \nx\n)\n \n}\n    \n// abstract override is necessary to stack traits\n\n\n}\n\n\n\nclass\n \nMyQueue\n \nextends\n \nBasicIntQueue\n \nwith\n \nDoubling\n           \n// could also mixin during instantiation\n\n\n\nval\n \nmyQueue\n \n=\n \nnew\n \nMyQueue\n\n\nmyQueue\n.\nput\n(\n3\n)\n\n\nmyQueue\n.\nget\n()\n\n\n\n\n\n\n\n\nMore traits can be stacked one atop another, make sure that all overrides are labelled \nabstract override\n.\n\n\nThe order of the mixins are important. Traits on the right take effect first.\n\n\nTraits are instantiated before a classes instantiation from left to right.\n\n\nLinerization: the diamond inheritance problem is avoided since instantiations are tracked and will not allow multiple instantiations of the same parent trait\n\n\n\n\nClasses versus Traits\n\u00b6\n\n\nUse classes:\n\n\n\n\nWhen a behavior is not going to be reused at all or in multiple places\n\n\nWhen you plan to use your Scala code from another language, for example, if you are building a library that could be used in Java\n\n\n\n\nUse traits:\n\n\n\n\nWhen a behavior is going to be reused in multiple unrelated classes.\n\n\nWhen you want to define interfaces and want to use them outside Scala, for example Java. The reason is that the traits that do not have any implementations are compiled similar to interfaces.\n\n\n\n\nKeyword List\n\u00b6\n\n\nabstract\ncase\ncatch\nclass\ndef\ndo\nelse\nextends\nfalse\nfinal\nfinally\nfor\nforSome\nif\nimplicit\nimport\nlazy\nmatch\nnew\nNull\nobject\noverride\npackage\nprivate\nprotected\nreturn\nsealed\nsuper\nthis\nthrow\ntrait\nTry\ntrue\ntype\nval\nvar\nwhile\nwith\nyield\n -\n :\n =\n =>\n<-\n<:\n<%\n>:\n#\n@",
            "title": "Scala Language"
        },
        {
            "location": "/Scala/Scala_Language/#links",
            "text": "Scala Cheatsheet  Scala @ TutorialPoint  Scala Tutorial (PDF)   Some examples are derived from  Scala Koans .",
            "title": "Links"
        },
        {
            "location": "/Scala/Scala_Language/#basics",
            "text": "",
            "title": "Basics"
        },
        {
            "location": "/Scala/Scala_Language/#style",
            "text": "Class Names - For all class names, the first letter should be in Upper Case. If several words are used to form a name of the class, each inner word's first letter should be in Upper Case.  class MyFirstScalaClass  Method Names - All method names should start with a Lower Case letter. If multiple words are used to form the name of the method, then each inner word's first letter should be in Upper Case.  def myMethodName()  Program File Name - Name of the program file should exactly match the object name. When saving the file you should save it using the object name (Remember Scala is case-sensitive) and append \".scala\" to the end of the name. If the file name and the object name do not match your program will not compile.  Assume 'HelloWorld' is the object name: the file should be saved as 'HelloWorld.scala'.",
            "title": "Style"
        },
        {
            "location": "/Scala/Scala_Language/#packages",
            "text": "package   pkg           // at start of file  package   pkg   {   ...   }   // bracket style",
            "title": "Packages"
        },
        {
            "location": "/Scala/Scala_Language/#imports",
            "text": "import   scala.collection._                     // wildcard import. When importing all the names of a package or class, one uses the underscore character (_) instead of the asterisk (*).  import   scala.collection.Vector                // one class import  import   scala.collection. { Vector ,   Sequence }    // selective import. Multiple classes can be imported from the same package by enclosing them in curly braces  import   scala.collection. { Vector   =>   Vec28 }     // renaming import.  import   java.util. { Date   =>   _ ,   _ }               // import all from java.util except Date.   All classes from the java.lang package are imported by default. The Predef object provides definitions that are accessible in all Scala compilation units without explicit qualification:\n- immutable Map, Set, List, ::, Nil, print, println, assert, assume, require, ensuring  import   scala.collection.mutable.HashMap                 // Mutable collections must be imported.  import   scala.collection.immutable. { TreeMap ,   TreeSet }    // So are specialized collections.",
            "title": "Imports"
        },
        {
            "location": "/Scala/Scala_Language/#application-entry-point",
            "text": "object   HelloWorld   { \n     def   main ( args :   Array [ String ])   { \n         println ( \"Hello, world!\" ) \n     }  }",
            "title": "Application Entry Point"
        },
        {
            "location": "/Scala/Scala_Language/#blocks",
            "text": "You can combine expressions by surrounding them with {}. We call this a block.\nThe result of the last expression in the block is the result of the overall block, too.  println ({ \n   val   x   =   1   +   1 \n   x   +   1  })   // 3",
            "title": "Blocks"
        },
        {
            "location": "/Scala/Scala_Language/#variables-and-values",
            "text": "var   x   =   5   // variable  val   x   =   5   // immutable value / \"const\"  var   x :   Double   =   5   // explicit type  println ( x )   A lazy val is assignment that will not evaluated until it is called. Note there is no lazy var  lazy   val   a   =   { heavymath ();   19 }",
            "title": "Variables and Values"
        },
        {
            "location": "/Scala/Scala_Language/#literals",
            "text": "val   a   =   2         // int  val   b   =   31L       // long  val   c   =   0x30B     // hexadecimal  val   d   =   3 f        // float  val   e   =   3.22d     // double  val   f   =   93 e - 9  val   g   =   'a'           // character  val   h   =   '\\u0061'      // unicode for a  val   i   =   ' \\ 141 '        // octal for a  val   j   =   '\\\"'          // escape sequences  val   k   =   '\\\\'  val   s   =   \"To be or not to be\"      // string  s . charAt ( 0 )  val   s2   =   \"\"\"An apple a day  keeps the doctor away\"\"\"          // multi-lines string  s2 . split ( '\\n' )  val   s3   =   \"\"\"An apple a day             |keeps the doctor away\"\"\"    // Multiline String literals can use | to specify the starting position of subsequent lines, then use stripMargin to remove the surplus indentation.  s3 . stripMargin",
            "title": "Literals"
        },
        {
            "location": "/Scala/Scala_Language/#enumerations",
            "text": "object   Planets   extends   Enumeration   { \n   val   Mercury   =   Value \n   val   Venus   =   Value \n   val   Earth   =   Value \n   val   Mars   =   Value \n   val   Jupiter   =   Value \n   val   Saturn   =   Value \n   val   Uranus   =   Value \n   val   Neptune   =   Value \n   val   Pluto   =   Value  }  Planets . Mercury . id  Planets . Mercury . toString   //How does it get the name? by Reflection.  object   GreekPlanets   extends   Enumeration   { \n   val   Mercury   =   Value ( 1 ,   \"Hermes\" )     // enumeration with your own index and/or your own Strings \n   val   Venus   =   Value ( 2 ,   \"Aphrodite\" ) \n   //Fun Fact: Tellus is Roman for (Mother) Earth \n   val   Earth   =   Value ( 3 ,   \"Gaia\" ) \n   val   Mars   =   Value ( 4 ,   \"Ares\" ) \n   val   Jupiter   =   Value ( 5 ,   \"Zeus\" ) \n   val   Saturn   =   Value ( 6 ,   \"Cronus\" ) \n   val   Uranus   =   Value ( 7 ,   \"Ouranus\" ) \n   val   Neptune   =   Value ( 8 ,   \"Poseidon\" ) \n   val   Pluto   =   Value ( 9 ,   \"Hades\" )  }",
            "title": "Enumerations"
        },
        {
            "location": "/Scala/Scala_Language/#common-data-structures",
            "text": "( 1 , 2 , 3 )                       // tuple literal. (Tuple3)  var   ( x , y , z )   =   ( 1 , 2 , 3 )         // destructuring bind: tuple unpacking via pattern matching.  // BAD var x,y,z = (1,2,3)  // hidden error: each assigned to the entire tuple.  val   tuple   =   ( \"apple\" ,   3 )      // mixed type tuple  tuple . _1  tuple . _2  tuple . swap   var   xs   =   List ( 1 , 2 , 3 )          // list (immutable).  xs ( 2 )                         // paren indexing  1   ::   List ( 2 , 3 )                // cons (create a new list by prepending the element).  1   to   5                        // Range sugar. Same as `1 until 6`  1   to   10   by   2  Range ( 1 ,   10 ,   2 )               // Range does not include the last item, even in a step increment  Range ( 1 ,   9 ,   2 ). inclusive   ()                            // (empty parens)   sole member of the Unit type (like C/Java void).",
            "title": "Common Data Structures"
        },
        {
            "location": "/Scala/Scala_Language/#control-constructs",
            "text": "if   ( check )   happy   else   sad     // conditional.  if   ( check )   happy              //  if   ( check )   happy   else   ()      // same as above  while   ( x   <   5 )   {   println ( x );   x   +=   1 }   // while loop.  do   {   println ( x );   x   +=   1 }   while   ( x   <   5 )    // do while loop.  for   ( x   <-   xs   if   x % 2   ==   0 )   yield   x * 10      // for comprehension with guard  xs . filter ( _ % 2   ==   0 ). map ( _ * 10 )             // same as filter/map  for   (( x , y )   <-   xs   zip   ys )   yield   x * y        // for comprehension: destructuring bind  ( xs   zip   ys )   map   {   case   ( x , y )   =>   x * y   }     // same as  for   ( x   <-   xs ;   y   <-   ys )   yield   x * y          // for comprehension: cross product. Later generators varying more rapidly than earlier ones  xs   flatMap   { x   =>   ys   map   { y   =>   x * y }}       // same as  for   ( x   <-   xs ;   y   <-   ys )   { \n   println ( \"%d/%d = %.1f\" . format ( x ,   y ,   x / y . toFloat ))       // for comprehension: imperative-ish  }  for   ( i   <-   1   to   5 )   {                       // for comprehension: iterate including the upper bound \n   println ( i )  }  for   ( i   <-   1   until   5 )   {                    // for comprehension: iterate omitting the upper bound \n   println ( i )  }  import   scala.util.control.Breaks._        // break  breakable   { \n   for   ( x   <-   xs )   { \n     if   ( Math . random   <   0.1 )   break \n   }  }",
            "title": "Control Constructs"
        },
        {
            "location": "/Scala/Scala_Language/#formatting-and-interpolation",
            "text": "val   helloMessage   =   \"Hello World\"  s\"Application  $helloMessage \"    // string interpolation; can include expressions which can include numbers and strings  // use `f` prefix before the string instead of an `s` for sprintf formatting",
            "title": "Formatting and Interpolation"
        },
        {
            "location": "/Scala/Scala_Language/#functions",
            "text": "Scala is a functional language in the sense that every function is a value and every value is an object so ultimately every function is an object.\nScala provides a lightweight syntax for defining anonymous functions, it supports higher-order functions, it allows functions to be nested, and supports currying.  def   add ( x :   Int ,   y :   Int ) :   Int   =   x   +   y      // the return type is declared after the parameter list and a colon  // GOOD def f(x: Any) = println(x)  // BAD  def f(x) = println(x)           // syntax error: need types for every arg.  def   f ( x :   Int )   =   {                         // inferred return type \n   val   square   =   x * x \n   square . toString \n   }   // The last expression in the body is the method\u2019s return value. (Scala does have a return keyword, but it\u2019s rarely used.)  // BAD def f(x: Int) { x*x }  hidden error: without = it\u2019s a Unit-returning procedure; causes havoc  // When performing recursion, the return type on the method is mandatory!    Backticks for reserved keywords and identifiers with a space (rare)   def   `put employee on probation` ( employee :   Employee )   =   { \n        new   Employee ( employee . `first name` ,   employee . `last name` ,   \"Probation\" ) \n     }",
            "title": "Functions"
        },
        {
            "location": "/Scala/Scala_Language/#multiple-parameter-lists-or-none-at-all",
            "text": "def   addThenMultiply ( x :   Int ,   y :   Int )( multiplier :   Int ) :   Int   =   ( x   +   y )   *   multiplier  def   name :   String   =   System . getProperty ( \"name\" )",
            "title": "Multiple parameter lists or none at all"
        },
        {
            "location": "/Scala/Scala_Language/#procedures",
            "text": "def   foo ( x :   Int )   {   //Note: No `=`; returns Unit \n       print ( x . toString ) \n     }  def   foo ( x :   Int ) :   Unit   =    print ( x . toString )    // or   Convention (not required for the compiler) states that if you a call a method that returns a Unit / has a side effect, invoke that method with empty parenthesis, other leave the parenthesis out  def   performSideEffect () : Unit   =   System . currentTimeMillis  performSideEffect ()",
            "title": "Procedures"
        },
        {
            "location": "/Scala/Scala_Language/#default-and-named-parameters",
            "text": "def   addColorsWithDefaults ( red :   Int   =   0 ,   green :   Int   =   0 ,   blue :   Int   =   0 )   =   { \n   ( red ,   green ,   blue )  }  me . addColors ( blue   =   40 )",
            "title": "Default and named parameters"
        },
        {
            "location": "/Scala/Scala_Language/#variable-length-arguments",
            "text": "def   sum ( args :   Int* )   =   args . reduceLeft ( _ + _ )                // varargs. must be last arg  def   capitalizeAll ( args :   String* )   =   { \n       args . map   {   arg   => \n         arg . capitalize \n       } \n     }  capitalizeAll ( \"rarity\" ,   \"applejack\" )   If you want a collection expanded into a vararg, add  :_*  def   repeatedParameterMethod ( x :   Int ,   y :   String ,   z :   Any* )   =   { \n     \"%d %ss can give you %s\" . format ( x ,   y ,   z . mkString ( \", \" )) \n   }  repeatedParameterMethod ( 3 ,   \"egg\" ,   List ( \"a delicious sandwich\" ,   \"protein\" ,   \"high cholesterol\" ) :_ * )   should   be ( __ )",
            "title": "Variable Length Arguments"
        },
        {
            "location": "/Scala/Scala_Language/#tail-recursion",
            "text": "As a precaution, the helpful @tailrec annotation will throw a compile time if a method is not tail recursive,\nmeaning that the last call and only call of the method is the recursive method. Scala optimizes recursive calls\nto a loop from a stack  import   scala.annotation.tailrec   //  importing annotation!  @tailrec                          //  compiler will check that the function is tail recursive  def   factorial ( i :   BigInt ) :   BigInt   =   { \n       @tailrec \n       def   fact ( i :   BigInt ,   accumulator :   BigInt ) :   BigInt   =   {    // methods can be placed inside in methods; return type is obligatory \n         if   ( i   <=   1 ) \n           accumulator \n         else \n           fact ( i   -   1 ,   i   *   accumulator ) \n       } \n       fact ( i ,   1 ) \n     }  factorial ( 3 )",
            "title": "Tail recursion"
        },
        {
            "location": "/Scala/Scala_Language/#infix-postfix-and-prefix-notations-operators",
            "text": "object   FrenchDate   { \n     def   main ( args :   Array [ String ])   { \n         val   now   =   new   Date \n         val   df   =   getDateInstance ( LONG ,   Locale . FRANCE ) \n     println ( df   format   now )         // Methods taking one argument can be used with an infix syntax. Equivalent to df.format(now) \n     }  }   1 + 2 * 3 / x  consists exclusively of method calls, because it is equivalent to the following expression:  (1).+(((2).*(3))./(x)) \nThis also means that +, *, etc. are valid identifiers in Scala.  Infix Operators do NOT work if an object has a method that takes two parameters.    val   g :   Int   =   31 \n  val   s :   String   =   g   toHexString    // Postfix operators work if an object has a method that takes no parameters   Prefix operators work if an object has a method name that starts with unary_  class   Stereo   { \n       def   unary_+   =   \"on\" \n       def   unary_-   =   \"off\" \n     }  val   stereo   =   new   Stereo  + stereo     // it is on   Methods with colons are right-associative, that means the object that a method is on will be on the  right  and the method parameter will be on the  left  class   Foo   ( y : Int )   { \n       def   ~:( n : Int )   =   n   +   y   +   3 \n     }  val   foo   =   new   Foo ( 9 )  10   ~:   foo  foo .~:( 10 )    // same as",
            "title": "Infix, Postfix and Prefix Notations; Operators"
        },
        {
            "location": "/Scala/Scala_Language/#anonymous-functions",
            "text": "def   lambda   =   ( x :   Int )   =>   x   +   1  // other variants  def   lambda2   =   {   x :   Int   =>   x   +   1   }  val   lambda3   =   new   Function1 [ Int ,  Int ]   { \n       def   apply ( v1 :   Int ) :   Int   =   v1   +   1 \n     }  val   everything   =   ()   =>   42                         // without parameter  val   add   =   ( x :   Int ,   y :   Int )   =>   x   +   y               // multiple parameters  ( 1   to   5 ). map ( _ * 2 )                                 // underscore notation.  ( 1   to   5 )   map   ( _ * 2 )                                // same with infix sugar.  ( 1   to   5 ). reduceLeft (   _ + _   )                        // underscores are positionally matched 1st and 2nd args.  ( 1   to   5 ). map (   x   =>   x * x   )                          // to use an arg twice, have to name it.  ( 1   to   5 ). map   {   x   =>   val   y   =   x * 2 ;   println ( y );   y   }      // block style returns last expression.  ( 1   to   5 )   filter   { _ % 2   ==   0 }   map   { _ * 2 }              // pipeline style (works with parens too).  // GOOD (1 to 5).map(2*)  // BAD (1 to 5).map(*2)                         // anonymous function: bound infix method. Use 2*_ for sanity\u2019s sake instead.  def   compose ( g :   R   =>   R ,   h :   R   =>   R )   =   ( x : R )   =>   g ( h ( x ))  val   f   =   compose ({ _ * 2 },   { _ - 1 })                     // anonymous functions: to pass in multiple blocks, need outer parens.   Passing anonymous functions as parameter:  def   makeWhatEverYouLike ( xs :   List [ String ],   func :   String   =>   String )   =   { \n       xs   map   func \n     }   Function returning another function using an anonymous function:  def   add ( x :   Int )   =   ( y : Int )   =>   x   +   y   Function Values:  object   Timer   { \n     def   oncePerSecond ( callback :   ()   =>   Unit )   {          // () => T is a Function type that takes a Unit type. Unit is known as 'void' to a Java programmer. \n         while   ( true )   {   callback ();   Thread   sleep   1000   } \n     } \n\n     def   timeFlies ()   { \n         println ( \"time flies like an arrow...\" ) \n     } \n\n     def   main ( args :   Array [ String ])   { \n         oncePerSecond ( timeFlies )         // function value; could also be () => timeFlies() \n     }  }",
            "title": "Anonymous Functions"
        },
        {
            "location": "/Scala/Scala_Language/#by-name-parameter",
            "text": "This is used extensively in scala to create blocks.       def   calc ( x :   =>   Int ) :   Either [ Throwable ,  Int ]   =   {     //x is a call by name parameter; delayed execution of x \n       try   { \n         Right ( x ) \n       }   catch   { \n         case   b :   Throwable   =>   Left ( b ) \n       } \n     } \n\n     val   y   =   calc   {                                      //This looks like a natural block \n       println ( \"Here we go!\" )                            //Some superfluous call \n       49   +   20 \n     }   By name parameters can also be used with an Object and apply to make interesting block-like calls  object   PigLatinizer   { \n       def   apply ( x :   =>   String )   =   x . tail   +   x . head   +   \"ay\" \n     }  val   result   =   PigLatinizer   { \n       val   x   =   \"pret\" \n       val   z   =   \"zel\" \n       x   ++   z   //concatenate the strings \n     }",
            "title": "By-name parameter"
        },
        {
            "location": "/Scala/Scala_Language/#closures",
            "text": "var   incrementer   =   1  def   closure   =   { \n   x :   Int   =>   x   +   incrementer  }",
            "title": "Closures"
        },
        {
            "location": "/Scala/Scala_Language/#currying",
            "text": "val   zscore   =   ( mean :   R ,   sd :   R )   =>   ( x : R )   =>   ( x - mean )/ sd         // currying, obvious syntax.  def   zscore ( mean :   R ,   sd :   R )   =   ( x :   R )   =>   ( x - mean )/ sd            // currying, obvious syntax  def   zscore ( mean :   R ,   sd :   R )( x :   R )   =   ( x - mean )/ sd                // currying, sugar syntax. but then:  val   normer   =   zscore ( 7 ,   0.4 )   _                                 // need trailing underscore to get the partial, only for the sugar version.  def   mapmake [ T ]( g :   T   =>   T )( seq :   List [ T ])   =   seq . map ( g )          // generic type.  def   multiply ( x :   Int ,   y :   Int )   =   x   *   y  val   multiplyCurried   =   ( multiply   _ ). curried                    multiply ( 4 ,   5 )  multiplyCurried ( 3 )( 2 )",
            "title": "Currying"
        },
        {
            "location": "/Scala/Scala_Language/#partial-applications",
            "text": "def   adder ( m :   Int ,   n :   Int )   =   m   +   n  val   add2   =   adder ( 2 ,   _: Int )    // You can partially apply any argument in the argument list, not just the last one.  add2 ( 3 )      // which is 5  val   add3   =   adder   _                // underscore to convert from a function to a lambda  adder ( 1 ,   9 )  add3 ( 1 ,   9 )",
            "title": "Partial Applications"
        },
        {
            "location": "/Scala/Scala_Language/#partial-functions",
            "text": "val   doubleEvens :   PartialFunction [ Int ,  Int ]   =   new   PartialFunction [ Int ,  Int ]   {     // full declaration \n       //States that this partial function will take on the task \n       def   isDefinedAt ( x :   Int )   =   x   %   2   ==   0 \n\n       //What we do if this does partial function matches \n       def   apply ( v1 :   Int )   =   v1   *   2 \n     }  val   tripleOdds :   PartialFunction [ Int ,  Int ]   =   { \n       case   x :   Int   if   ( x   %   2 )   !=   0   =>   x   *   3                // syntaxic sugar (usual way) \n     }  val   whatToDo   =   doubleEvens   orElse   tripleOdds              //  combine the partial functions together: OrElse  val   addFive   =   ( x :   Int )   =>   x   +   5  val   whatToDo   =   doubleEvens   orElse   tripleOdds   andThen   addFive    // chain (partial) functions together: andThen",
            "title": "Partial Functions"
        },
        {
            "location": "/Scala/Scala_Language/#classes-objects-and-traits",
            "text": "class   C ( x :   R )                         // constructor params - x is only available in class body  class   C ( val   x :   R )                     // c.x  constructor params - automatic public (immutable) member defined  class   D ( var   x :   R )                     // you can define class with var or val parameters  class   C ( var   x :   R )   { \n   assert ( x   >   0 ,   \"positive please\" )    // constructor is class body \n   var   y   =   x                           // declare a public member \n   val   readonly   =   5                    // declare a gettable but not settable member \n   private   var   secret   =   1              // declare a private member \n   def   this   =   this ( 42 )                 // alternative constructor  }  new {   ...   }    // anonymous class  abstract   class   D   {   ...   }      // define an abstract(non-createable) class.  class   C   extends   D   {   ...   }     // define an inherited class. Class hierarchy is linear, a class can only extend from one parent class  class   C ( x :   R )   extends   D ( x )    // inheritance and constructor params. (wishlist: automatically pass-up params by default)  // A class can be placed inside another class  object   O   extends   D   {   ...   }    // define a singleton.  trait   T   {   ...   }               // traits. See below.  class   C   extends   T   {   ...   }  class   C   extends   D   with   T   {   ...   }  // interfaces-with-implementation. no constructor params. mixin-able.  trait   T1 ;   trait   T2  class   C   extends   T1   with   T2            // multiple traits.  class   C   extends   D   with   T1   with   T2     // parent class and (multiple) trait(s).  class   C   extends   D   {   override   def   f   =   ...}     // must declare method overrides.  var   c   =   new   C ( 4 )          // Instantiation  //BAD new List[Int]  //GOOD List(1,2,3)      // Instead, convention: callable factory shadowing the type  classOf [ String ]   // class literal.  classOf [ String ]. getCanonicalName  classOf [ String ]. getSimpleName  val   zoom   =   \"zoom\"  zoom . getClass   ==   classOf [ String ]  x . isInstanceOf [ String ]    // type check (runtime)  x . asInstanceOf [ String ]    // type cast (runtime)  x :   String                 // compare to parameter ascription (compile time)",
            "title": "Classes, Objects, and Traits"
        },
        {
            "location": "/Scala/Scala_Language/#methods",
            "text": "class   Complex ( real :   Double ,   imaginary :   Double )   { \n     def   re   =   real         // return type inferred automatically by the compiler \n     def   im   =   imaginary    // methods without arguments \n     def   print () :   Unit   =   println ( s\" $real  + i *  $imaginary \" ) \n     override   def   toString ()   =   \"\"   +   re   +   ( if   ( im   <   0 )   \"\"   else   \"+\" )   +   im   +   \"i\"    // override methods inherited from a super-class  }",
            "title": "Methods"
        },
        {
            "location": "/Scala/Scala_Language/#asserts-and-contracts",
            "text": "Asserts take a boolean argument and can take a message.  assert ( true )   // should be true  assert ( true ,   \"This should be true\" )   def   addNaturals ( nats :   List [ Int ]) :   Int   =   { \n   require ( nats   forall   ( _   >=   0 ),   \"List contains negative numbers\" ) \n   nats . foldLeft ( 0 )( _   +   _ )  }   ensuring ( _   >=   0 )",
            "title": "Asserts and Contracts"
        },
        {
            "location": "/Scala/Scala_Language/#path-dependent-classes",
            "text": "When a class is instantiated inside of another object, it belongs to the instance.  This is a path dependent type. Once established, it cannot be placed inside of another object  case   class   Board ( length :   Int ,   height :   Int )   { \n   case   class   Coordinate ( x :   Int ,   y :   Int )  }  val   b1   =   Board ( 20 ,   20 )  val   b2   =   Board ( 30 ,   30 )  val   c1   =   b1 . Coordinate ( 15 ,   15 )  val   c2   =   b2 . Coordinate ( 25 ,   25 )  // val c1 = c2  won't work   Use  A#B  for a Java-style inner class:  class   Graph   { \n   class   Node   { \n     var   connectedNodes :   List [ Graph # Node ]   =   Nil            // accepts Nodes from any Graph \n     def   connectTo ( node :   Graph # Node )   { \n       if   ( connectedNodes . find ( node . equals ). isEmpty )   { \n         connectedNodes   =   node   ::   connectedNodes \n       } \n     } \n   } \n   var   nodes :   List [ Node ]   =   Nil \n   def   newNode :   Node   =   { \n     val   res   =   new   Node \n     nodes   =   res   ::   nodes \n     res \n   }  }",
            "title": "Path-dependent Classes"
        },
        {
            "location": "/Scala/Scala_Language/#companion-objects",
            "text": "Static members (methods or fields) do not exist in Scala. Rather than defining static members, the Scala programmer declares these members in singleton objects, that is a class with a single instance.  object   TimerAnonymous   { \n     def   oncePerSecond ( callback :   ()   =>   Unit )   { \n         while   ( true )   {   callback ();   Thread   sleep   1000   } \n     } \n     def   main ( args :   Array [ String ])   { \n         oncePerSecond (()   =>   println ( \"time flies like an arrow...\" )) \n     }  }    An object that has the same name as class is called a companion object, it is used to contain factories for the class that it complements.  A companion object can also store shared variables and values for every instantiated class to share.  A companion object can see private values and variables of the instantiated object",
            "title": "Companion Objects"
        },
        {
            "location": "/Scala/Scala_Language/#apply-method",
            "text": "The apply method is a magical method in Scala.  class   Employee   ( val   firstName : String ,   val   lastName : String )  object   Employee   { \n     def   apply ( firstName : String ,   lastName : String )   =   new   Employee ( firstName ,   lastName )    // would also work in a class, but rarer  }  val   a   =   Employee ( \"John\" ,   \"Doe\" )  // is equivalent to  var   b   =   Employee . apply ( \"John\" ,   \"Doe\" )",
            "title": "Apply Method"
        },
        {
            "location": "/Scala/Scala_Language/#case-classes",
            "text": "The  new  keyword is not mandatory to create instances of these classes (i.e. one can write Const(5) instead of new Const(5)),  Getter functions are automatically defined for the constructor parameters (i.e. it is possible to get the value of the v constructor parameter of some instance c of class Const just by writing c.v),  Default definitions for methods equals and hashCode are provided, which work on the structure of the instances and not on their identity,  A default definition for method toString is provided, and prints the value in a source form (e.g. the tree for expression x+1 prints as Sum(Var(x),Const(1))),  Instances of these classes can be decomposed through pattern matching   case   class   Person ( first :   String ,   last :   String ,   age :   Int   =   0 )    // Case classes can have default and named parameters  val   p1   =   Person ( \"Fred\" ,   \"Jones\" )      // new is optional  val   p2   =   new   Person ( \"Fred\" ,   \"Jones\" )  p1   ==   p2                              // true  p1 . hashCode   ==   p2 . hashCode            // true  p1   eq   p2                              // false  val   p3   =   p2 . copy ( first   =   \"Jane\" )      // copy the case class but change the name in the copy   case   class   Dog ( var   name :   String ,   breed :   String )       // Case classes can have mutable properties - potentially unsafe   Case classes can be disassembled to their constituent parts as a tuple:  val   parts   =   Person . unapply ( p1 ). get   // returns Option[T]  parts . _1  parts . _2",
            "title": "Case Classes"
        },
        {
            "location": "/Scala/Scala_Language/#algebraic-data-type",
            "text": "sealed   trait   Tree      // or abstract class  final   case   class   Sum ( l :   Tree ,   r :   Tree )   extends   Tree  final   case   class   Var ( n :   String )   extends   Tree  final   case   class   Const ( v :   Int )   extends   Tree",
            "title": "Algebraic data type"
        },
        {
            "location": "/Scala/Scala_Language/#pattern-matching",
            "text": "{ case \"x\" => 5 }  defines a partial function which, when given the string \"x\" as argument, returns the integer 5, and fails with an exception otherwise.  type   Environment   =   String   =>   Int    // the type Environment can be used as an alias of the type of functions from String to Int  def   eval ( t :   Tree ,   env :   Environment ) :   Int   =   t   match   { \n     case   Sum ( l ,   r )   =>   eval ( l ,   env )   +   eval ( r ,   env ) \n     case   Var ( n )   =>   env ( n ) \n     case   Const ( v )   =>   v  }  def   derive ( t :   Tree ,   v :   String ) :   Tree   =   t   match   { \n     case   Sum ( l ,   r )   =>   Sum ( derive ( l ,   v ),   derive ( r ,   v )) \n     case   Var ( n )   if   ( v   ==   n )   =>   Const ( 1 )                    // guard, an expression following the if keyword. \n     case   _   =>   Const ( 0 )                                     // wild-card, written _, which is a pattern matching any value, without giving it a name.  }   // GOOD (xs zip ys) map { case (x,y) => x*y }  // BAD (xs zip ys) map( (x,y) => x*y )  // use case in function args for pattern matching.  // BAD  val   v42   =   42  Some ( 3 )   match   { \n   case   Some ( v42 )   =>   println ( \"42\" ) \n   case   _   =>   println ( \"Not 42\" )  }     // \u201cv42\u201d is interpreted as a name matching any Int value, and \u201c42\u201d is printed.  // GOOD  val   v42   =   42  Some ( 3 )   match   { \n   case   Some ( `v42` )   =>   println ( \"42\" ) \n   case   _   =>   println ( \"Not 42\" )  }     // \u201d`v42`\u201d with backticks is interpreted as the existing val v42, and \u201cNot 42\u201d is printed.  // GOOD  val   UppercaseVal   =   42  Some ( 3 )   match   { \n   case   Some ( UppercaseVal )   =>   println ( \"42\" ) \n   case   _   =>   println ( \"Not 42\" )  }     // UppercaseVal is treated as an existing val, rather than a new pattern variable, because it starts with an uppercase letter.  // Thus, the value contained within UppercaseVal is checked against 3, and \u201cNot 42\u201d is printed.",
            "title": "Pattern Matching"
        },
        {
            "location": "/Scala/Scala_Language/#list-matching",
            "text": "val   secondElement   =   List ( 1 , 2 , 3 )   match   { \n       case   x   ::   y   ::   xs   =>   xs \n       case   x   ::   Nil   =>   x \n       case   _   =>   0 \n     }",
            "title": "List Matching"
        },
        {
            "location": "/Scala/Scala_Language/#regex",
            "text": "val   MyRegularExpression   =   \"\"\"a=([^,]+),\\s+b=(.+)\"\"\" . r             //.r turns a String to a regular expression  expr   match   { \n       case   ( MyRegularExpression ( a ,   b ))   =>   a   +   b \n       }   import   scala.util.matching.Regex  val   numberPattern :   Regex   =   \"[0-9]\" . r  numberPattern . findFirstMatchIn ( \"awesomepassword\" )   match   { \n   case   Some ( _ )   =>   println ( \"Password OK\" ) \n   case   None   =>   println ( \"Password must contain a number\" )  }   With groups:  val   keyValPattern :   Regex   =   \"([0-9a-zA-Z-#() ]+): ([0-9a-zA-Z-#() ]+)\" . r  for   ( patternMatch   <-   keyValPattern . findAllMatchIn ( input )) \n   println ( s\"key:  ${ patternMatch . group ( 1 ) }  value:  ${ patternMatch . group ( 2 ) } \" )",
            "title": "Regex"
        },
        {
            "location": "/Scala/Scala_Language/#extractors-unapply",
            "text": "class   Car ( val   make :   String ,   val   model :   String ,   val   year :   Short ,   val   topSpeed :   Short )  object   Car   {                                                          // What is typical is to create a custom extractor in the companion object of the class. \n   def   unapply ( x :   Car )   =   Some ( x . make ,   x . model ,   x . year ,   x . topSpeed )     // returns an Option[T]  }  val   Car ( a ,   b ,   c ,   d )   =   new   Car ( \"Chevy\" ,   \"Camaro\" ,   1978 ,   120 )       // assign values to a .. d  val   x   =   new   Car ( \"Chevy\" ,   \"Camaro\" ,   1978 ,   120 )   match   {             // pattern matching \n   case   Car ( s ,   t ,   _ ,   _ )   =>   ( s ,   t )                                  // _ for variables we don't care about. \n   case   _   =>   ( \"Ford\" ,   \"Edsel\" )                                     // fallback  }    As long as the method signatures aren't the same, you can have an many unapply methods as you want in the same class / object.  When you create a case class, it automatically can be used with pattern matching since it has an extractor.",
            "title": "Extractors (unapply)"
        },
        {
            "location": "/Scala/Scala_Language/#value-class",
            "text": "Avoid allocating runtime objects.  class   Wrapper ( val   underlying :   Int )   extends   AnyVal   { \n   def   foo :   Wrapper   =   new   Wrapper ( underlying   *   19 )  }   It has a single, public val parameter that is the underlying runtime representation. The type at compile time is Wrapper, but at runtime, the representation is an Int. A value class can define defs, but no vals, vars, or nested traitss, classes or objects  A value class can only extend universal traits and cannot be extended itself. A universal trait is a trait that extends Any, only has defs as members, and does no initialization. Universal traits allow basic inheritance of methods for value classes, but they incur the overhead of allocation.",
            "title": "Value Class"
        },
        {
            "location": "/Scala/Scala_Language/#traits",
            "text": "Apart from inheriting code from a super-class, a Scala class can also import code from one or several traits i.e. interfaces which can also contain code.\nIn Scala, when a class inherits from a trait, it implements that traits's interface, and inherits all the code contained in the trait.  trait   Ord   { \n     def   <   ( that :   Any ) :   Boolean                                // The type Any which is used above is the type which is a super-type of all other types in Scala \n     def   <=( that :   Any ) :   Boolean   =   ( this   <   that )   ||   ( this   ==   that ) \n     def   >   ( that :   Any ) :   Boolean   =   !( this   <=   that ) \n     def   >=( that :   Any ) :   Boolean   =   !( this   <   that )  }  class   Date ( y :   Int ,   m :   Int ,   d :   Int )   extends   Ord   { \n     def   year   =   y \n     def   month   =   m \n     def   day   =   d \n     override   def   toString () :   String   =   year   +   \"-\"   +   month   +   \"-\"   +   day \n\n     override   def   equals ( that :   Any ) :   Boolean   = \n         that . isInstanceOf [ Date ]   &&   { \n         val   o   =   that . asInstanceOf [ Date ] \n         o . day   ==   day   &&   o . month   ==   month   &&   o . year   ==   year \n     } \n\n     def   <( that :   Any ) :   Boolean   =   {      // The trait declare the type (e.g. method), where a concrete implementer will satisfy the type \n         if   (! that . isInstanceOf [ Date ]) \n             error ( \"cannot compare \"   +   that   +   \" and a Date\" ) \n         val   o   =   that . asInstanceOf [ Date ]( year   <   o . year )   || \n             ( year   ==   o . year   &&   ( month   <   o . month   || \n             ( month   ==   o . month   &&   day   <   o . day      ))) \n     }  }     Traits can have concrete implementations that can be mixed into concrete classes with its own state    Traits can be mixed in during instantiation!    trait   Logging   { \n       var   logCache   =   List [ String ]() \n\n       def   log ( value :   String )   =   { \n         logCache   =   logCache   :+   value \n       } \n\n       def   log   =   logCache \n     }  val   a   =   new   A ( \"stuff\" )   with   Logging    // mixin traits during instantiation!  a . log ( \"I did something\" )  a . log . size",
            "title": "Traits"
        },
        {
            "location": "/Scala/Scala_Language/#stackable-traits",
            "text": "abstract   class   IntQueue   { \n   def   get () :   Int \n   def   put ( x :   Int )  }  import   scala.collection.mutable.ArrayBuffer  class   BasicIntQueue   extends   IntQueue   { \n   private   val   buf   =   new   ArrayBuffer [ Int ] \n   def   get ()   =   buf . remove ( 0 ) \n   def   put ( x :   Int )   {   buf   +=   x   }  }  trait   Doubling   extends   IntQueue   { \n   abstract   override   def   put ( x :   Int )   {   super . put ( 2   *   x )   }      // abstract override is necessary to stack traits  }  class   MyQueue   extends   BasicIntQueue   with   Doubling             // could also mixin during instantiation  val   myQueue   =   new   MyQueue  myQueue . put ( 3 )  myQueue . get ()    More traits can be stacked one atop another, make sure that all overrides are labelled  abstract override .  The order of the mixins are important. Traits on the right take effect first.  Traits are instantiated before a classes instantiation from left to right.  Linerization: the diamond inheritance problem is avoided since instantiations are tracked and will not allow multiple instantiations of the same parent trait",
            "title": "Stackable Traits"
        },
        {
            "location": "/Scala/Scala_Language/#classes-versus-traits",
            "text": "Use classes:   When a behavior is not going to be reused at all or in multiple places  When you plan to use your Scala code from another language, for example, if you are building a library that could be used in Java   Use traits:   When a behavior is going to be reused in multiple unrelated classes.  When you want to define interfaces and want to use them outside Scala, for example Java. The reason is that the traits that do not have any implementations are compiled similar to interfaces.",
            "title": "Classes versus Traits"
        },
        {
            "location": "/Scala/Scala_Language/#keyword-list",
            "text": "abstract\ncase\ncatch\nclass\ndef\ndo\nelse\nextends\nfalse\nfinal\nfinally\nfor\nforSome\nif\nimplicit\nimport\nlazy\nmatch\nnew\nNull\nobject\noverride\npackage\nprivate\nprotected\nreturn\nsealed\nsuper\nthis\nthrow\ntrait\nTry\ntrue\ntype\nval\nvar\nwhile\nwith\nyield\n -\n :\n =\n =>\n<-\n<:\n<%\n>:\n#\n@",
            "title": "Keyword List"
        },
        {
            "location": "/Scala/Scala_Testing/",
            "text": "Links\n\u00b6\n\n\nhttp://www.scalatest.org/\n\n\nWriting TDD unit tests with scalatest\n\n\nAt a Glance\n\n\nExamples\n\u00b6\n\n\nlibraryDependencies\n \n+=\n \n\"org.scalatest\"\n \n%%\n \n\"scalatest\"\n \n%\n \n\"2.2.6\"\n \n%\n \n\"test\"\n\n\n\n\n\n\npackage\n \ncom.acme.pizza\n\n\n\nimport\n \norg.scalatest.FunSuite\n\n\nimport\n \norg.scalatest.BeforeAndAfter\n\n\n\nclass\n \nPizzaTests\n \nextends\n \nFunSuite\n \nwith\n \nBeforeAndAfter\n \n{\n\n\n  \nvar\n \npizza\n:\n \nPizza\n \n=\n \n_\n\n\n  \nbefore\n \n{\n\n    \npizza\n \n=\n \nnew\n \nPizza\n\n  \n}\n\n\n  \ntest\n(\n\"new pizza has zero toppings\"\n)\n \n{\n\n    \nassert\n(\npizza\n.\ngetToppings\n.\nsize\n \n==\n \n0\n)\n\n  \n}\n\n\n  \ntest\n(\n\"adding one topping\"\n)\n \n{\n\n    \npizza\n.\naddTopping\n(\nTopping\n(\n\"green olives\"\n))\n\n    \nassert\n(\npizza\n.\ngetToppings\n.\nsize\n \n===\n \n1\n)\n\n  \n}\n\n\n  \n// mark that you want a test here in the future\n\n  \ntest\n \n(\n\"test pizza pricing\"\n)\n \n(\npending\n)\n\n\n\n}\n\n\n\n\n\n\nStyles\n\u00b6\n\n\nFunSuite\n\u00b6\n\n\nimport\n \norg.scalatest.FunSuite\n \n\nclass\n \nAddSuite\n \nextends\n \nFunSuite\n \n{\n    \n  \ntest\n(\n\"3 plus 3 is 6\"\n)\n \n{\n      \n     \nassert\n((\n3\n \n+\n \n3\n)\n \n==\n \n6\n)\n    \n  \n}\n  \n\n}\n\n\n\n\n\n\nFlatSpec\n\u00b6\n\n\nThe structure of this test is flat\u2014like xUnit, but the test name can be written in specification style:\n\n\nimport\n \norg.scalatest.FlatSpec\n  \n\nclass\n \nAddSpec\n \nextends\n \nFlatSpec\n \n{\n  \n  \n\"Addition of 3 and 3\"\n \nshould\n \n\"have result 6\"\n \nin\n \n{\n \n    \nassert\n((\n3\n \n+\n \n3\n)\n \n==\n \n0\n)\n    \n  \n}\n  \n\n}\n\n\n\n\n\n\nimport\n \ncollection.mutable.Stack\n\n\nimport\n \norg.scalatest._\n\n\n\nclass\n \nExampleSpec\n \nextends\n \nFlatSpec\n \nwith\n \nMatchers\n \n{\n\n\n  \n\"A Stack\"\n \nshould\n \n\"pop values in last-in-first-out order\"\n \nin\n \n{\n\n    \nval\n \nstack\n \n=\n \nnew\n \nStack\n[\nInt\n]\n\n    \nstack\n.\npush\n(\n1\n)\n\n    \nstack\n.\npush\n(\n2\n)\n\n    \nstack\n.\npop\n()\n \nshould\n \nbe\n \n(\n2\n)\n\n    \nstack\n.\npop\n()\n \nshould\n \nbe\n \n(\n1\n)\n\n  \n}\n\n\n  \nit\n \nshould\n \n\"throw NoSuchElementException if an empty stack is popped\"\n \nin\n \n{\n\n    \nval\n \nemptyStack\n \n=\n \nnew\n \nStack\n[\nInt\n]\n\n    \na\n \n[\nNoSuchElementException\n]\n \nshould\n \nbe\n \nthrownBy\n \n{\n\n      \nemptyStack\n.\npop\n()\n\n    \n}\n \n  \n}\n\n\n}\n\n\n\n\n\n\nFeatureSpec\n\u00b6\n\n\nimport\n \norg.scalatest._\n  \n\n\nclass\n \nCalculator\n \n{\n    \n  \ndef\n \nadd\n(\na\n:\nInt\n,\n \nb\n:\nInt\n)\n:\n \nInt\n \n=\n \na\n \n+\n \nb\n \n\n}\n \n\n\nclass\n \nCalcSpec\n \nextends\n \nFeatureSpec\n \nwith\n \nGivenWhenThen\n \n{\n\n  \ninfo\n(\n\"As a calculator owner\"\n)\n    \n  \ninfo\n(\n\"I want to be able add two numbers\"\n)\n    \n  \ninfo\n(\n\"so I can get a correct result\"\n)\n    \n  \nfeature\n(\n\"Addition\"\n)\n \n{\n\n    \nscenario\n(\n\"User adds two numbers\"\n)\n \n{\n\n      \nGiven\n(\n\"a calculator\"\n)\n        \n      \nval\n \ncalc\n \n=\n \nnew\n \nCalculator\n \n      \nWhen\n(\n\"two numbers are added\"\n)\n        \n      \nvar\n \nresult\n \n=\n \ncalc\n.\nadd\n(\n3\n,\n \n3\n)\n \n      \nThen\n(\n\"we get correct result\"\n)\n        \n       \nassert\n(\nresult\n \n==\n \n6\n)\n      \n    \n}\n\n  \n}\n  \n\n}",
            "title": "Scala Testing"
        },
        {
            "location": "/Scala/Scala_Testing/#links",
            "text": "http://www.scalatest.org/  Writing TDD unit tests with scalatest  At a Glance",
            "title": "Links"
        },
        {
            "location": "/Scala/Scala_Testing/#examples",
            "text": "libraryDependencies   +=   \"org.scalatest\"   %%   \"scalatest\"   %   \"2.2.6\"   %   \"test\"   package   com.acme.pizza  import   org.scalatest.FunSuite  import   org.scalatest.BeforeAndAfter  class   PizzaTests   extends   FunSuite   with   BeforeAndAfter   { \n\n   var   pizza :   Pizza   =   _ \n\n   before   { \n     pizza   =   new   Pizza \n   } \n\n   test ( \"new pizza has zero toppings\" )   { \n     assert ( pizza . getToppings . size   ==   0 ) \n   } \n\n   test ( \"adding one topping\" )   { \n     pizza . addTopping ( Topping ( \"green olives\" )) \n     assert ( pizza . getToppings . size   ===   1 ) \n   } \n\n   // mark that you want a test here in the future \n   test   ( \"test pizza pricing\" )   ( pending )  }",
            "title": "Examples"
        },
        {
            "location": "/Scala/Scala_Testing/#styles",
            "text": "",
            "title": "Styles"
        },
        {
            "location": "/Scala/Scala_Testing/#funsuite",
            "text": "import   org.scalatest.FunSuite   class   AddSuite   extends   FunSuite   {     \n   test ( \"3 plus 3 is 6\" )   {       \n      assert (( 3   +   3 )   ==   6 )     \n   }    }",
            "title": "FunSuite"
        },
        {
            "location": "/Scala/Scala_Testing/#flatspec",
            "text": "The structure of this test is flat\u2014like xUnit, but the test name can be written in specification style:  import   org.scalatest.FlatSpec    class   AddSpec   extends   FlatSpec   {   \n   \"Addition of 3 and 3\"   should   \"have result 6\"   in   {  \n     assert (( 3   +   3 )   ==   0 )     \n   }    }   import   collection.mutable.Stack  import   org.scalatest._  class   ExampleSpec   extends   FlatSpec   with   Matchers   { \n\n   \"A Stack\"   should   \"pop values in last-in-first-out order\"   in   { \n     val   stack   =   new   Stack [ Int ] \n     stack . push ( 1 ) \n     stack . push ( 2 ) \n     stack . pop ()   should   be   ( 2 ) \n     stack . pop ()   should   be   ( 1 ) \n   } \n\n   it   should   \"throw NoSuchElementException if an empty stack is popped\"   in   { \n     val   emptyStack   =   new   Stack [ Int ] \n     a   [ NoSuchElementException ]   should   be   thrownBy   { \n       emptyStack . pop () \n     }  \n   }  }",
            "title": "FlatSpec"
        },
        {
            "location": "/Scala/Scala_Testing/#featurespec",
            "text": "import   org.scalatest._    class   Calculator   {     \n   def   add ( a : Int ,   b : Int ) :   Int   =   a   +   b   }   class   CalcSpec   extends   FeatureSpec   with   GivenWhenThen   { \n   info ( \"As a calculator owner\" )     \n   info ( \"I want to be able add two numbers\" )     \n   info ( \"so I can get a correct result\" )     \n   feature ( \"Addition\" )   { \n     scenario ( \"User adds two numbers\" )   { \n       Given ( \"a calculator\" )         \n       val   calc   =   new   Calculator  \n       When ( \"two numbers are added\" )         \n       var   result   =   calc . add ( 3 ,   3 )  \n       Then ( \"we get correct result\" )         \n        assert ( result   ==   6 )       \n     } \n   }    }",
            "title": "FeatureSpec"
        },
        {
            "location": "/Scala/Scala_Types/",
            "text": "Type Refinement\n\u00b6\n\n\nType Refinement = \"subclassing without naming the subclass\".\n\n\nclass\n \nEntity\n\n\n\ntrait\n \nPersister\n \n{\n\n  \ndef\n \ndoPersist\n(\ne\n:\n \nEntity\n)\n \n=\n \n{\n\n    \ne\n.\npersistForReal\n()\n\n  \n}\n\n\n}\n\n\n\n// our refined instance (and type):\n\n\nval\n \nrefinedMockPersister\n \n=\n \nnew\n \nPersister\n \n{\n  \n  \noverride\n \ndef\n \ndoPersist\n(\ne\n:\n \nEntity\n)\n \n=\n \n()\n   \n\n}\n\n\n\n\n\n\nScala Types of Types\n\n\nGenerics\n\u00b6\n\n\nclass\n \nReference\n[\nT\n]\n \n{\n\n    \nprivate\n \nvar\n \ncontents\n:\n \nT\n \n=\n \n_\n             \n// _ represents a default value. This default value is 0 for numeric types, false for the Boolean type, () for the Unit type and null for all object types.\n\n    \ndef\n \nset\n(\nvalue\n:\n \nT\n)\n \n{\n \ncontents\n \n=\n \nvalue\n \n}\n\n    \ndef\n \nget\n:\n \nT\n \n=\n \ncontents\n\n\n}\n\n\n\ntrait\n \nCache\n[\nK\n, \nV\n]\n \n{\n\n      \ndef\n \nget\n(\nkey\n:\n \nK\n)\n:\n \nV\n\n      \ndef\n \nput\n(\nkey\n:\n \nK\n,\n \nvalue\n:\n \nV\n)\n\n      \ndef\n \ndelete\n(\nkey\n:\n \nK\n)\n\n    \n}\n\n\n\ndef\n \nremove\n[\nK\n](\nkey\n:\n \nK\n)\n  \n// function\n\n\n\n\n\n\nType Variance\n\u00b6\n\n\nCovariance \n+A\n allow you to set the your container to a either a variable with the same type or parent type.\n\n\nclass\n \nMyContainer\n[\n+A\n](\na\n:\n \nA\n)(\nimplicit\n \nmanifest\n:\n \nscala.reflect.Manifest\n[\nA\n])\n \n{\n\n      \nprivate\n[\nthis\n]\n \nval\n \nitem\n \n=\n \na\n\n\n      \ndef\n \nget\n \n=\n \nitem\n\n\n      \ndef\n \ncontents\n \n=\n \nmanifest\n.\nruntimeClass\n.\ngetSimpleName\n\n    \n}\n\n\n    \nval\n \nfruitBasket\n:\n \nMyContainer\n[\nFruit\n]\n \n=\n \nnew\n \nMyContainer\n[\nOrange\n](\nnew\n \nOrange\n())\n\n    \nfruitBasket\n.\ncontents\n\n\n\n\n\n\nContravariance \n-A\n is the opposite of covariance\n\n\nDeclaring neither -/+, indicates invariance variance.  You cannot use a superclass variable reference (\"contravariant\" position) or a subclass variable reference (\"covariant\" position) of that type.\n\n\nUpper and Lower Type Bounds\n\u00b6\n\n\nabstract\n \nclass\n \nPet\n \nextends\n \nAnimal\n \n{\n \ndef\n \nname\n:\n \nString\n \n}\n\n\n\nclass\n \nCat\n \nextends\n \nPet\n \n{\n\n  \noverride\n \ndef\n \nname\n:\n \nString\n \n=\n \n\"Cat\"\n\n\n}\n\n\n\nclass\n \nPetContainer\n[\nP\n \n<:\n \nPet\n](\np\n:\n \nP\n)\n \n{\n\n  \ndef\n \npet\n:\n \nP\n \n=\n \np\n   \n// The class PetContainer take a type parameter P which must be a subtype of Pet.\n\n\n}\n\n\n\n\n\n\nLower type bounds declare a type to be a supertype of another type. The term \nB >: A\n expresses that the type parameter B or the abstract type B refer to a supertype of type A.\n\n\nAbstract Types\n\u00b6\n\n\ntype\n \nR\n \n=\n \nDouble\n \n// type alias\n\n\n\n\n\n\ntrait\n \nContainer\n \n{\n\n  \ntype\n \nT\n\n  \nval\n \ndata\n:\n \nT\n\n\n  \ndef\n \ncompare\n(\nother\n:\n \nT\n)\n \n=\n \ndata\n.\nequals\n(\nother\n)\n\n\n}\n\n\n\nclass\n \nStringContainer\n(\nval\n \ndata\n:\n \nString\n)\n \nextends\n \nContainer\n \n{\n\n  \noverride\n \ntype\n \nT\n \n=\n \nString\n\n\n}\n\n\n\n\n\n\nGenerics vs Abstract Types\n\u00b6\n\n\nGenerics:\n\n\n\n\nIf you need just type instantiation. A good example is the standard collection classes.\n\n\nIf you are creating a family of types.\n\n\n\n\nAbstract types:\n\n\n\n\nIf you want to allow people to mix in types using traits.\n\n\nIf you need better readability in scenarios where both could be interchangeable.\n\n\nIf you want to hide the type definition from the client code.\n\n\n\n\nInfix Type\n\u00b6\n\n\nWe can make a type infix, meaning that a generic type with two type parameters can be displayed between two types.\nThe type specifier \nPair[String,Int]\n can be written as \nString Pair Int\n.\n\n\nclass\n \nPair\n[\nA\n, \nB\n](\na\n:\n \nA\n,\n \nb\n:\n \nB\n)\n\n\n\ntype\n \n~\n[\nA\n,\nB\n]\n \n=\n \nPair\n[\nA\n,\nB\n]\n\n\nval\n \npairlist\n:\n \nList\n[\nString\n \n~\n \nInt\n]\n   \n// operator-like usage\n\n\n\ncase\n \nclass\n \nItem\n[\nT\n](\ni\n:\n \nT\n)\n \n{\n\n    \ndef\n \n~(\nj\n:\n \nItem\n[\nT\n])\n \n=\n \nnew\n \nPair\n(\nthis\n,\n \nj\n)\n  \n// creating an infix operator method to use with our infix type\n\n\n}\n\n\n\n(\nItem\n(\n\"a\"\n)\n \n~\n \nItem\n(\n\"b\"\n)).\nisInstanceOf\n[\nString\n \n~\n \nString\n]\n\n\n\n\n\n\nShapeLess\n\n\nStructural Types\n\u00b6\n\n\nimport\n \nscala.language.reflectiveCalls\n   \n// use reflection --> slow\n\n\n\ndef\n \nonlyThoseThatCanPerformQuacks\n(\nquacker\n:\n \n{\ndef\n \nquack:String\n})\n:\n \nString\n \n=\n \n{\n\n        \n\"received message: %s\"\n.\nformat\n(\nquacker\n.\nquack\n)\n\n      \n}\n\n\n\ntype\n \nSpeakerAndMover\n \n=\n \n{\ndef\n \nspeak\n:\nString\n;\n \ndef\n \nmove\n(\nsteps\n:\nInt\n,\n \ndirection\n:\nString\n)\n:\nString\n}\n  \n// with type aliasing\n\n\n\n\n\n\nSelf-type\n\u00b6\n\n\nSelf-types are a way to declare that a trait must be mixed into another trait, even though it doesn\u2019t directly extend it. That makes the members of the dependency available without imports.\n\n\ntrait\n \nUser\n \n{\n\n  \ndef\n \nusername\n:\n \nString\n\n\n}\n\n\n\ntrait\n \nTweeter\n \n{\n\n  \nthis:\n \nUser\n \n=>\n  \n// reassign this\n\n  \ndef\n \ntweet\n(\ntweetText\n:\n \nString\n)\n \n=\n \nprintln\n(\ns\"\n$username\n: \n$tweetText\n\"\n)\n\n\n}\n\n\n\nclass\n \nVerifiedTweeter\n(\nval\n \nusername_\n \n:\n \nString\n)\n \nextends\n \nTweeter\n \nwith\n \nUser\n \n{\n  \n// We mixin User because Tweeter required it\n\n  \ndef\n \nusername\n \n=\n \ns\"real \n$username_\n\"\n\n\n}\n\n\n\n\n\n\nDifference between a self type and extending a trait\n\u00b6\n\n\n\n\nIf you say that B extends A, then B is an A. When you use self-types, B requires an A.\n\n\n\n\nThere are two specific requirements that are created with self-types:\n1. If B is extended, then you're required to mix-in an A.\n1. When a concrete class finally extends/mixes-in these traits, some class/trait must implement A.\n\n\ntrait\n \nWrong\n \nextends\n \nTweeter\n \n{\n\n     \ndef\n \nnoCanDo\n \n=\n \nname\n        \n// does not compile\n\n\n}\n\n\n\n\n\n\nIf Tweeter was a subclass of User, there would be no error. In the code above, we required a User whenever Tweeter is used, however a User wasn't provided to Wrong, so we got an error.\n\n\n\n\nSelf types allow you to define cyclical dependencies. For example, you can achieve this:\n\n\n\n\ntrait\n \nA\n \n{\n \nself\n:\n \nB\n \n=>\n \n}\n\n\ntrait\n \nB\n \n{\n \nself\n:\n \nA\n \n=>\n \n}\n\n\n\n\n\n\nInheritance using extends does not allow that.\n\n\n\n\nBecause self-types aren't part of the hierarchy of the required class they can be excluded from pattern matching, especially when you are exhaustively matching against a sealed hierarchy. This is convenient when you want to model orthogonal behaviors such as:\n\n\n\n\nsealed\n \ntrait\n \nPerson\n\n\ntrait\n \nStudent\n \nextends\n \nPerson\n\n\ntrait\n \nTeacher\n \nextends\n \nPerson\n\n\ntrait\n \nAdult\n \n{\n \nthis\n \n:\n \nPerson\n \n=>\n \n}\n \n// orthogonal to its condition\n\n\n\nval\n \np\n \n:\n \nPerson\n \n=\n \nnew\n \nStudent\n \n{}\n\n\np\n \nmatch\n \n{\n\n  \ncase\n \ns\n \n:\n \nStudent\n \n=>\n \nprintln\n(\n\"a student\"\n)\n\n  \ncase\n \nt\n \n:\n \nTeacher\n \n=>\n \nprintln\n(\n\"a teacher\"\n)\n\n\n}\n \n// that's it we're exhaustive\n\n\n\n\n\n\nImplicits\n\u00b6\n\n\nImplicits wrap around existing classes to provide extra functionality\n\n\nobject\n \nMyPredef\n \n{\n   \n// usually in a companion object\n\n\n  \nclass\n \nIntWrapper\n(\nval\n \noriginal\n:\n \nInt\n)\n \n{\n\n    \ndef\n \nisOdd\n \n=\n \noriginal\n \n%\n \n2\n \n!=\n \n0\n\n    \ndef\n \nisEven\n \n=\n \n!\nisOdd\n\n  \n}\n\n\n  \nimplicit\n \ndef\n \nthisMethodNameIsIrrelevant\n(\nvalue\n:\n \nInt\n)\n \n=\n \nnew\n \nIntWrapper\n(\nvalue\n)\n\n\n}\n\n\n\nimport\n \nMyPredef._\n\n\n//imported implicits come into effect within this scope\n\n\n19.\nisOdd\n\n\n\n// Implicits can be used to automatically convert one type to another\n\n\nimport\n \njava.math.BigInteger\n\n\nimplicit\n \ndef\n \nInt2BigIntegerConvert\n(\nvalue\n:\n \nInt\n)\n:\n \nBigInteger\n \n=\n \nnew\n \nBigInteger\n(\nvalue\n.\ntoString\n)\n\n\ndef\n \nadd\n(\na\n:\n \nBigInteger\n,\n \nb\n:\n \nBigInteger\n)\n \n=\n \na\n.\nadd\n(\nb\n)\n\n\nadd\n(\n3\n,\n \n6\n)\n  \n// 3 and 6 are Int\n\n\n\n// Implicits function parameters\n\n\ndef\n \nhowMuchCanIMake_?\n(\nhours\n:\n \nInt\n)(\nimplicit\n \namount\n:\n \nBigDecimal\n,\n \ncurrencyName\n:\n \nString\n)\n \n=\n \n(\namount\n \n*\n \nhours\n).\ntoString\n()\n \n+\n \n\" \"\n \n+\n \ncurrencyName\n\n\nimplicit\n \nvar\n \nhourlyRate\n \n=\n \nBigDecimal\n(\n34.00\n)\n\n\nimplicit\n \nval\n \ncurrencyName\n \n=\n \n\"Dollars\"\n\n\nhowMuchCanIMake_?\n(\n30\n)\n\n\n\n\n\n\nDefault arguments though are preferred to Implicit Function Parameters.\n\n\nContext-bound Types\n\u00b6\n\n\ndef\n \ninspect\n[\nT\n \n:\n \nTypeTag\n](\nl\n:\n \nList\n[\nT\n])\n \n=\n \ntypeOf\n[\nT\n].\ntypeSymbol\n.\nname\n.\ndecoded\n\n\nval\n \nlist\n \n=\n \n1\n \n::\n \n2\n \n::\n \n3\n \n::\n \n4\n \n::\n \n5\n \n::\n \nNil\n\n\ninspect\n(\nlist\n)\n\n\n\n\n\n\nequivalent to\n\n\ndef\n \ninspect\n[\nT\n](\nl\n:\n \nList\n[\nT\n])(\nimplicit\n \ntt\n:\n \nTypeTag\n[\nT\n])\n \n=\n \ntt\n.\ntpe\n.\ntypeSymbol\n.\nname\n.\ndecoded\n\n    \nval\n \nlist\n \n=\n \n1\n \n::\n \n2\n \n::\n \n3\n \n::\n \n4\n \n::\n \n5\n \n::\n \nNil\n\n    \ninspect\n(\nlist\n)\n\n\n\n\n\n\nTypeTags can be used to determine a type used before it erased by the VM by using an implicit TypeTag argument.",
            "title": "Scala Types"
        },
        {
            "location": "/Scala/Scala_Types/#type-refinement",
            "text": "Type Refinement = \"subclassing without naming the subclass\".  class   Entity  trait   Persister   { \n   def   doPersist ( e :   Entity )   =   { \n     e . persistForReal () \n   }  }  // our refined instance (and type):  val   refinedMockPersister   =   new   Persister   {   \n   override   def   doPersist ( e :   Entity )   =   ()     }   Scala Types of Types",
            "title": "Type Refinement"
        },
        {
            "location": "/Scala/Scala_Types/#generics",
            "text": "class   Reference [ T ]   { \n     private   var   contents :   T   =   _               // _ represents a default value. This default value is 0 for numeric types, false for the Boolean type, () for the Unit type and null for all object types. \n     def   set ( value :   T )   {   contents   =   value   } \n     def   get :   T   =   contents  }  trait   Cache [ K ,  V ]   { \n       def   get ( key :   K ) :   V \n       def   put ( key :   K ,   value :   V ) \n       def   delete ( key :   K ) \n     }  def   remove [ K ]( key :   K )    // function",
            "title": "Generics"
        },
        {
            "location": "/Scala/Scala_Types/#type-variance",
            "text": "Covariance  +A  allow you to set the your container to a either a variable with the same type or parent type.  class   MyContainer [ +A ]( a :   A )( implicit   manifest :   scala.reflect.Manifest [ A ])   { \n       private [ this ]   val   item   =   a \n\n       def   get   =   item \n\n       def   contents   =   manifest . runtimeClass . getSimpleName \n     } \n\n     val   fruitBasket :   MyContainer [ Fruit ]   =   new   MyContainer [ Orange ]( new   Orange ()) \n     fruitBasket . contents   Contravariance  -A  is the opposite of covariance  Declaring neither -/+, indicates invariance variance.  You cannot use a superclass variable reference (\"contravariant\" position) or a subclass variable reference (\"covariant\" position) of that type.",
            "title": "Type Variance"
        },
        {
            "location": "/Scala/Scala_Types/#upper-and-lower-type-bounds",
            "text": "abstract   class   Pet   extends   Animal   {   def   name :   String   }  class   Cat   extends   Pet   { \n   override   def   name :   String   =   \"Cat\"  }  class   PetContainer [ P   <:   Pet ]( p :   P )   { \n   def   pet :   P   =   p     // The class PetContainer take a type parameter P which must be a subtype of Pet.  }   Lower type bounds declare a type to be a supertype of another type. The term  B >: A  expresses that the type parameter B or the abstract type B refer to a supertype of type A.",
            "title": "Upper and Lower Type Bounds"
        },
        {
            "location": "/Scala/Scala_Types/#abstract-types",
            "text": "type   R   =   Double   // type alias   trait   Container   { \n   type   T \n   val   data :   T \n\n   def   compare ( other :   T )   =   data . equals ( other )  }  class   StringContainer ( val   data :   String )   extends   Container   { \n   override   type   T   =   String  }",
            "title": "Abstract Types"
        },
        {
            "location": "/Scala/Scala_Types/#generics-vs-abstract-types",
            "text": "Generics:   If you need just type instantiation. A good example is the standard collection classes.  If you are creating a family of types.   Abstract types:   If you want to allow people to mix in types using traits.  If you need better readability in scenarios where both could be interchangeable.  If you want to hide the type definition from the client code.",
            "title": "Generics vs Abstract Types"
        },
        {
            "location": "/Scala/Scala_Types/#infix-type",
            "text": "We can make a type infix, meaning that a generic type with two type parameters can be displayed between two types.\nThe type specifier  Pair[String,Int]  can be written as  String Pair Int .  class   Pair [ A ,  B ]( a :   A ,   b :   B )  type   ~ [ A , B ]   =   Pair [ A , B ]  val   pairlist :   List [ String   ~   Int ]     // operator-like usage  case   class   Item [ T ]( i :   T )   { \n     def   ~( j :   Item [ T ])   =   new   Pair ( this ,   j )    // creating an infix operator method to use with our infix type  }  ( Item ( \"a\" )   ~   Item ( \"b\" )). isInstanceOf [ String   ~   String ]   ShapeLess",
            "title": "Infix Type"
        },
        {
            "location": "/Scala/Scala_Types/#structural-types",
            "text": "import   scala.language.reflectiveCalls     // use reflection --> slow  def   onlyThoseThatCanPerformQuacks ( quacker :   { def   quack:String }) :   String   =   { \n         \"received message: %s\" . format ( quacker . quack ) \n       }  type   SpeakerAndMover   =   { def   speak : String ;   def   move ( steps : Int ,   direction : String ) : String }    // with type aliasing",
            "title": "Structural Types"
        },
        {
            "location": "/Scala/Scala_Types/#self-type",
            "text": "Self-types are a way to declare that a trait must be mixed into another trait, even though it doesn\u2019t directly extend it. That makes the members of the dependency available without imports.  trait   User   { \n   def   username :   String  }  trait   Tweeter   { \n   this:   User   =>    // reassign this \n   def   tweet ( tweetText :   String )   =   println ( s\" $username :  $tweetText \" )  }  class   VerifiedTweeter ( val   username_   :   String )   extends   Tweeter   with   User   {    // We mixin User because Tweeter required it \n   def   username   =   s\"real  $username_ \"  }",
            "title": "Self-type"
        },
        {
            "location": "/Scala/Scala_Types/#difference-between-a-self-type-and-extending-a-trait",
            "text": "If you say that B extends A, then B is an A. When you use self-types, B requires an A.   There are two specific requirements that are created with self-types:\n1. If B is extended, then you're required to mix-in an A.\n1. When a concrete class finally extends/mixes-in these traits, some class/trait must implement A.  trait   Wrong   extends   Tweeter   { \n      def   noCanDo   =   name          // does not compile  }   If Tweeter was a subclass of User, there would be no error. In the code above, we required a User whenever Tweeter is used, however a User wasn't provided to Wrong, so we got an error.   Self types allow you to define cyclical dependencies. For example, you can achieve this:   trait   A   {   self :   B   =>   }  trait   B   {   self :   A   =>   }   Inheritance using extends does not allow that.   Because self-types aren't part of the hierarchy of the required class they can be excluded from pattern matching, especially when you are exhaustively matching against a sealed hierarchy. This is convenient when you want to model orthogonal behaviors such as:   sealed   trait   Person  trait   Student   extends   Person  trait   Teacher   extends   Person  trait   Adult   {   this   :   Person   =>   }   // orthogonal to its condition  val   p   :   Person   =   new   Student   {}  p   match   { \n   case   s   :   Student   =>   println ( \"a student\" ) \n   case   t   :   Teacher   =>   println ( \"a teacher\" )  }   // that's it we're exhaustive",
            "title": "Difference between a self type and extending a trait"
        },
        {
            "location": "/Scala/Scala_Types/#implicits",
            "text": "Implicits wrap around existing classes to provide extra functionality  object   MyPredef   {     // usually in a companion object \n\n   class   IntWrapper ( val   original :   Int )   { \n     def   isOdd   =   original   %   2   !=   0 \n     def   isEven   =   ! isOdd \n   } \n\n   implicit   def   thisMethodNameIsIrrelevant ( value :   Int )   =   new   IntWrapper ( value )  }  import   MyPredef._  //imported implicits come into effect within this scope  19. isOdd  // Implicits can be used to automatically convert one type to another  import   java.math.BigInteger  implicit   def   Int2BigIntegerConvert ( value :   Int ) :   BigInteger   =   new   BigInteger ( value . toString )  def   add ( a :   BigInteger ,   b :   BigInteger )   =   a . add ( b )  add ( 3 ,   6 )    // 3 and 6 are Int  // Implicits function parameters  def   howMuchCanIMake_? ( hours :   Int )( implicit   amount :   BigDecimal ,   currencyName :   String )   =   ( amount   *   hours ). toString ()   +   \" \"   +   currencyName  implicit   var   hourlyRate   =   BigDecimal ( 34.00 )  implicit   val   currencyName   =   \"Dollars\"  howMuchCanIMake_? ( 30 )   Default arguments though are preferred to Implicit Function Parameters.",
            "title": "Implicits"
        },
        {
            "location": "/Scala/Scala_Types/#context-bound-types",
            "text": "def   inspect [ T   :   TypeTag ]( l :   List [ T ])   =   typeOf [ T ]. typeSymbol . name . decoded  val   list   =   1   ::   2   ::   3   ::   4   ::   5   ::   Nil  inspect ( list )   equivalent to  def   inspect [ T ]( l :   List [ T ])( implicit   tt :   TypeTag [ T ])   =   tt . tpe . typeSymbol . name . decoded \n     val   list   =   1   ::   2   ::   3   ::   4   ::   5   ::   Nil \n     inspect ( list )   TypeTags can be used to determine a type used before it erased by the VM by using an implicit TypeTag argument.",
            "title": "Context-bound Types"
        },
        {
            "location": "/Scala/Scaladoc/",
            "text": "Scaladoc\n\n\nScaladoc Style Guide\n\n\n/** Start the comment here\n  * and use the left star followed by a\n  * white space on every line.\n  *\n  * Even on empty paragraph-break lines.\n  *\n  * Note that the * on each line is aligned\n  * with the second * in /** so that the\n  * left margin is on the same column on the\n  * first line and on subsequent ones.\n  *\n  * The closing Scaladoc tag goes on its own,\n  * separate line. E.g.\n  *\n  * Calculate the square of the given number\n  *\n  * @param d the Double to square\n  * @return the result of squaring d\n  */\n def square(d: Double): Double = d * d\n\n\n\n\n\nTags\n\u00b6\n\n\nClass specific tags\n\u00b6\n\n\n@constructor\n placed in the class comment will describe the primary constructor.\nMethod specific tags\n\n\n@return\n detail the return value from a method (one per method).\nMethod, Constructor and/or Class tags\n\n\n@throws\n what exceptions (if any) the method or constructor may throw.\n\n\n@param\n detail a value parameter for a method or constructor, provide one per parameter to the method/constructor.\n\n\n@tparam\n detail a type parameter for a method, constructor or class. Provide one per type parameter.\n\n\nUsage tags\n\u00b6\n\n\n@see\n reference other sources of information like external document links or related entities in the documentation.\n\n\n@note\n add a note for pre or post conditions, or any other notable restrictions or expectations.\n\n\n@example\n for providing example code or related example documentation.\n\n\n@usecase\n provide a simplified method definition for when the full method definition is too complex or noisy. An example is (in the collections API), providing documentation for methods that omit the implicit canBuildFrom.\n\n\nMember grouping tags\n\u00b6\n\n\n@group <group>\n - mark the entity as a member of the \n group.\n\n\n@groupname <group> <name>\n - provide an optional name for the group. \n is displayed as the group header\nbefore the group description.\n\n\n@groupdesc <group> <description>\n - add optional descriptive text to display under the group name. Supports multiline formatted text.\n\n\n@groupprio\n - control the order of the group on the page. Defaults to 0. Ungrouped elements have an implicit priority of 1000. Use a value between 0 and 999 to set a relative position to other groups. Low values will appear before high values.\n\n\nDiagram tags\n\u00b6\n\n\n@contentDiagram\n - use with traits and classes to include a content hierarchy diagram showing included types. The diagram content can be fine tuned with additional specifiers taken from hideNodes, hideOutgoingImplicits, hideSubclasses, hideEdges, hideIncomingImplicits, hideSuperclasses and hideInheritedNode. hideDiagram can be supplied to prevent a diagram from being created if it would be created by default. Packages and objects have content diagrams by default.\n\n\n@inheritanceDiagram\n\n\nOther tags\n\u00b6\n\n\n@author\n provide author information for the following entity\n\n\n@version\n the version of the system or API that this entity is a part of.\n\n\n@since\n like \n@version\n but defines the system or API that this entity was first defined in.\n\n\n@todo\n for documenting unimplemented features or unimplemented aspects of an entity.\n\n\n@deprecated\n marks the entity as deprecated, providing both the replacement implementation that should be used and the version/date at which this entity was deprecated.\n\n\n@migration\n like deprecated but provides advanced warning of planned changes ahead of deprecation. Same fields as \n@deprecated\n.\n\n\n@inheritdoc\n take comments from a superclass as defaults if comments are not provided locally.\n\n\n@documentable\n Expand a type alias and abstract type into a full template page. - TODO: Test the \u201cabstract type\u201d claim - no examples of this in the Scala code base\n\n\nMacros\n\u00b6\n\n\n@define <name> <definition>\n allows use of $name in other Scaladoc comments within the same source file which will be expanded to the contents of \n.\n\n\nMarkup\n\u00b6\n\n\n`monospace`\n''italic text''\n'''bold text'''\n__underline__\n^superscript^\n,,subscript,,\n[[entity link]], e.g. [[scala.collection.Seq]]\n[[http://external.link External Link]],\n  e.g. [[http://scala-lang.org Scala Language Site]]\n\n\n\n\n\nOther formatting notes\n\u00b6\n\n\nParagraphs are started with one (or more) blank lines. \n\n\n*\n in the margin for the comment is valid (and should be included) but the line should be blank otherwise.\n\n\nCode blocks are contained within {{{ this }}} and may be multi-line. \n\n\nIndentation is relative to the starting * for the comment.\n\n\nHeadings are defined with surrounding = characters, with more = denoting subheadings. E.g. =Heading=, ==Sub-Heading==, etc.\n\n\nList blocks are a sequence of list items with the same style and level, with no interruptions from other block styles. Unordered lists can be bulleted using -, while numbered lists can be denoted using 1., i., I., a. for the various numbering styles.",
            "title": "Scaladoc"
        },
        {
            "location": "/Scala/Scaladoc/#tags",
            "text": "",
            "title": "Tags"
        },
        {
            "location": "/Scala/Scaladoc/#class-specific-tags",
            "text": "@constructor  placed in the class comment will describe the primary constructor.\nMethod specific tags  @return  detail the return value from a method (one per method).\nMethod, Constructor and/or Class tags  @throws  what exceptions (if any) the method or constructor may throw.  @param  detail a value parameter for a method or constructor, provide one per parameter to the method/constructor.  @tparam  detail a type parameter for a method, constructor or class. Provide one per type parameter.",
            "title": "Class specific tags"
        },
        {
            "location": "/Scala/Scaladoc/#usage-tags",
            "text": "@see  reference other sources of information like external document links or related entities in the documentation.  @note  add a note for pre or post conditions, or any other notable restrictions or expectations.  @example  for providing example code or related example documentation.  @usecase  provide a simplified method definition for when the full method definition is too complex or noisy. An example is (in the collections API), providing documentation for methods that omit the implicit canBuildFrom.",
            "title": "Usage tags"
        },
        {
            "location": "/Scala/Scaladoc/#member-grouping-tags",
            "text": "@group <group>  - mark the entity as a member of the   group.  @groupname <group> <name>  - provide an optional name for the group.   is displayed as the group header\nbefore the group description.  @groupdesc <group> <description>  - add optional descriptive text to display under the group name. Supports multiline formatted text.  @groupprio  - control the order of the group on the page. Defaults to 0. Ungrouped elements have an implicit priority of 1000. Use a value between 0 and 999 to set a relative position to other groups. Low values will appear before high values.",
            "title": "Member grouping tags"
        },
        {
            "location": "/Scala/Scaladoc/#diagram-tags",
            "text": "@contentDiagram  - use with traits and classes to include a content hierarchy diagram showing included types. The diagram content can be fine tuned with additional specifiers taken from hideNodes, hideOutgoingImplicits, hideSubclasses, hideEdges, hideIncomingImplicits, hideSuperclasses and hideInheritedNode. hideDiagram can be supplied to prevent a diagram from being created if it would be created by default. Packages and objects have content diagrams by default.  @inheritanceDiagram",
            "title": "Diagram tags"
        },
        {
            "location": "/Scala/Scaladoc/#other-tags",
            "text": "@author  provide author information for the following entity  @version  the version of the system or API that this entity is a part of.  @since  like  @version  but defines the system or API that this entity was first defined in.  @todo  for documenting unimplemented features or unimplemented aspects of an entity.  @deprecated  marks the entity as deprecated, providing both the replacement implementation that should be used and the version/date at which this entity was deprecated.  @migration  like deprecated but provides advanced warning of planned changes ahead of deprecation. Same fields as  @deprecated .  @inheritdoc  take comments from a superclass as defaults if comments are not provided locally.  @documentable  Expand a type alias and abstract type into a full template page. - TODO: Test the \u201cabstract type\u201d claim - no examples of this in the Scala code base",
            "title": "Other tags"
        },
        {
            "location": "/Scala/Scaladoc/#macros",
            "text": "@define <name> <definition>  allows use of $name in other Scaladoc comments within the same source file which will be expanded to the contents of  .",
            "title": "Macros"
        },
        {
            "location": "/Scala/Scaladoc/#markup",
            "text": "`monospace`\n''italic text''\n'''bold text'''\n__underline__\n^superscript^\n,,subscript,,\n[[entity link]], e.g. [[scala.collection.Seq]]\n[[http://external.link External Link]],\n  e.g. [[http://scala-lang.org Scala Language Site]]",
            "title": "Markup"
        },
        {
            "location": "/Scala/Scaladoc/#other-formatting-notes",
            "text": "Paragraphs are started with one (or more) blank lines.   *  in the margin for the comment is valid (and should be included) but the line should be blank otherwise.  Code blocks are contained within {{{ this }}} and may be multi-line.   Indentation is relative to the starting * for the comment.  Headings are defined with surrounding = characters, with more = denoting subheadings. E.g. =Heading=, ==Sub-Heading==, etc.  List blocks are a sequence of list items with the same style and level, with no interruptions from other block styles. Unordered lists can be bulleted using -, while numbered lists can be denoted using 1., i., I., a. for the various numbering styles.",
            "title": "Other formatting notes"
        },
        {
            "location": "/Scala/sbt/",
            "text": "SBT Links\n\u00b6\n\n\nSBT Home Page\n\n\nScala school's SBT page\n\n\nSBT: The Missing Tutorial\n\n\nCreate a New Project\n\u00b6\n\n\n$ sbt new sbt/scala-seed.g8\n$ \ncd\n hello\n$ sbt\n...\n> run\n> \nexit\n\n\n\n\n\n\nGiter8 templates\n\n\nLayout\n\u00b6\n\n\nsbt uses the same directory structure as Maven for source files by default (all paths are relative to the base directory):\n\n\nsrc/\n  main/\n    resources/\n       <files to include in main jar here>\n    scala/\n       <main Scala sources>\n    java/\n       <main Java sources>\n  test/\n    resources\n       <files to include in test jar here>\n    scala/\n       <test Scala sources>\n    java/\n       <test Java sources>\n\n\n\n\n\nOther directories in \nsrc/\n will be ignored. Additionally, all hidden directories will be ignored.\n\n\nSource code can be placed in the project\u2019s base directory as \nhello/app.scala\n, which may be for small projects, though for normal projects people tend to keep the projects in the src/main/ directory to keep things neat.\n\n\nBuild Definition\n\u00b6\n\n\nThe build definition goes in a file called \nbuild.sbt\n, located in the project\u2019s base directory. The \u201cbase directory\u201d is the directory containing the project.\nIn addition to \nbuild.sbt\n, the project directory can contain .scala files that defines helper objects and one-off plugins.\n\n\nbuild.sbt\nproject/\n  Dependencies.scala\n\n\n\n\n\n.gitignore\n (or equivalent for other version control systems) should contain:\n\n\ntarget/\n\n\n\n\n\nAs part of your build definition, specify the version of \nsbt\n that your build uses. This allows people with different versions of the sbt launcher to build the same projects with consistent results. \nTo do this, create a file named \nproject/build.properties\n that specifies the sbt version as follows:\n\n\nsbt.version=1.0.2\n\n\n\n\n\nA build definition is defined in build.sbt, and it consists of a set of projects (of type Project). Because the term project can be ambiguous, we often call it a subproject.\n\n\nlazy\n \nval\n \nroot\n \n=\n \n(\nproject\n \nin\n \nfile\n(\n\".\"\n))\n\n  \n.\nsettings\n(\n\n    \nname\n \n:=\n \n\"Hello\"\n,\n\n    \nscalaVersion\n \n:=\n \n\"2.12.3\"\n\n  \n)\n\n\n\n\n\n\nEach subproject is configured by key-value pairs.\n\n\nbuild.sbt\n may also be interspersed with vals, lazy vals, and defs. Top-level objects and classes are not allowed in \nbuild.sbt\n.\nThose should go in the \nproject/\n directory as Scala source files.\n\n\nThere are three flavors of key:\n\n\nSettingKey[T]: a key for a value computed once (the value is computed when loading the subproject, and kept around).\nTaskKey[T]: a key for a value, called a task, that has to be recomputed each time, potentially with side effects.\nInputKey[T]: a key for a task that has command line arguments as input. Check out Input Tasks for more details.\n\n\n\n\n\nBuilt-in Keys\n\u00b6\n\n\nThe built-in keys are just fields in an object called Keys. A \nbuild.sbt\n implicitly has an \nimport sbt.Keys._\n, so sbt.Keys.name can be referred to as name.\n\n\nAdding Library Dependencies\n\u00b6\n\n\nTo depend on third-party libraries, there are two options. The first is to drop jars in \nlib/\n (unmanaged dependencies) and the other is to add managed dependencies, which will look like this in \nbuild.sbt\n:\n\n\nval\n \nderby\n \n=\n \n\"org.apache.derby\"\n \n%\n \n\"derby\"\n \n%\n \n\"10.4.1.3\"\n\n\n\nlazy\n \nval\n \ncommonSettings\n \n=\n \nSeq\n(\n\n  \norganization\n \n:=\n \n\"com.example\"\n,\n\n  \nversion\n \n:=\n \n\"0.1.0-SNAPSHOT\"\n,\n\n  \nscalaVersion\n \n:=\n \n\"2.12.3\"\n\n\n)\n\n\n\nlazy\n \nval\n \nroot\n \n=\n \n(\nproject\n \nin\n \nfile\n(\n\".\"\n))\n\n  \n.\nsettings\n(\n\n    \ncommonSettings\n,\n\n    \nname\n \n:=\n \n\"Hello\"\n,\n\n    \nlibraryDependencies\n \n+=\n \nderby\n\n  \n)\n\n\n\n\n\n\nThe libraryDependencies key involves two complexities: \n+=\n rather than \n:=\n, and the \n%\n method. \n+=\n appends to the key\u2019s old value rather than replacing it. \nThe \n%\n method is used to construct an Ivy module ID from strings.",
            "title": "Sbt"
        },
        {
            "location": "/Scala/sbt/#sbt-links",
            "text": "SBT Home Page  Scala school's SBT page  SBT: The Missing Tutorial",
            "title": "SBT Links"
        },
        {
            "location": "/Scala/sbt/#create-a-new-project",
            "text": "$ sbt new sbt/scala-seed.g8\n$  cd  hello\n$ sbt\n...\n> run\n>  exit   Giter8 templates",
            "title": "Create a New Project"
        },
        {
            "location": "/Scala/sbt/#layout",
            "text": "sbt uses the same directory structure as Maven for source files by default (all paths are relative to the base directory):  src/\n  main/\n    resources/\n       <files to include in main jar here>\n    scala/\n       <main Scala sources>\n    java/\n       <main Java sources>\n  test/\n    resources\n       <files to include in test jar here>\n    scala/\n       <test Scala sources>\n    java/\n       <test Java sources>  Other directories in  src/  will be ignored. Additionally, all hidden directories will be ignored.  Source code can be placed in the project\u2019s base directory as  hello/app.scala , which may be for small projects, though for normal projects people tend to keep the projects in the src/main/ directory to keep things neat.",
            "title": "Layout"
        },
        {
            "location": "/Scala/sbt/#build-definition",
            "text": "The build definition goes in a file called  build.sbt , located in the project\u2019s base directory. The \u201cbase directory\u201d is the directory containing the project.\nIn addition to  build.sbt , the project directory can contain .scala files that defines helper objects and one-off plugins.  build.sbt\nproject/\n  Dependencies.scala  .gitignore  (or equivalent for other version control systems) should contain:  target/  As part of your build definition, specify the version of  sbt  that your build uses. This allows people with different versions of the sbt launcher to build the same projects with consistent results. \nTo do this, create a file named  project/build.properties  that specifies the sbt version as follows:  sbt.version=1.0.2  A build definition is defined in build.sbt, and it consists of a set of projects (of type Project). Because the term project can be ambiguous, we often call it a subproject.  lazy   val   root   =   ( project   in   file ( \".\" )) \n   . settings ( \n     name   :=   \"Hello\" , \n     scalaVersion   :=   \"2.12.3\" \n   )   Each subproject is configured by key-value pairs.  build.sbt  may also be interspersed with vals, lazy vals, and defs. Top-level objects and classes are not allowed in  build.sbt .\nThose should go in the  project/  directory as Scala source files.  There are three flavors of key:  SettingKey[T]: a key for a value computed once (the value is computed when loading the subproject, and kept around).\nTaskKey[T]: a key for a value, called a task, that has to be recomputed each time, potentially with side effects.\nInputKey[T]: a key for a task that has command line arguments as input. Check out Input Tasks for more details.",
            "title": "Build Definition"
        },
        {
            "location": "/Scala/sbt/#built-in-keys",
            "text": "The built-in keys are just fields in an object called Keys. A  build.sbt  implicitly has an  import sbt.Keys._ , so sbt.Keys.name can be referred to as name.",
            "title": "Built-in Keys"
        },
        {
            "location": "/Scala/sbt/#adding-library-dependencies",
            "text": "To depend on third-party libraries, there are two options. The first is to drop jars in  lib/  (unmanaged dependencies) and the other is to add managed dependencies, which will look like this in  build.sbt :  val   derby   =   \"org.apache.derby\"   %   \"derby\"   %   \"10.4.1.3\"  lazy   val   commonSettings   =   Seq ( \n   organization   :=   \"com.example\" , \n   version   :=   \"0.1.0-SNAPSHOT\" , \n   scalaVersion   :=   \"2.12.3\"  )  lazy   val   root   =   ( project   in   file ( \".\" )) \n   . settings ( \n     commonSettings , \n     name   :=   \"Hello\" , \n     libraryDependencies   +=   derby \n   )   The libraryDependencies key involves two complexities:  +=  rather than  := , and the  %  method.  +=  appends to the key\u2019s old value rather than replacing it. \nThe  %  method is used to construct an Ivy module ID from strings.",
            "title": "Adding Library Dependencies"
        },
        {
            "location": "/Search/ElasticSearch/",
            "text": "Cheatsheets\n\u00b6\n\n\nJolicode\n\n\nDevelopment URLs\n\u00b6\n\n\nKibana (port 5601)\n\n\nSense\n\n\nElasticSearch (port 9200)\n\n\nINSTALL\n\u00b6\n\n\n\n\nInstall \ncurl\n\n\nInstall \nJava\n\n\nDownload \nElasticSearch\n\n\nOptionally change the \ncluster.name\n in the \nelasticsearch.yml\n configuration\n\n\n\n\ncd\n elasticsearch-<version>\n./bin/elasticsearch -d\n\n# or on Windows \n\n\n# bin\\elasticsearch.bat\n\ncurl \n'http://localhost:9200/?pretty'\n\n\n\n\n\n\n\n\n\n\nInstall \nKibana\n\n\n\n\nOpen \nconfig/kibana.yml\n in an editor\n\n\nSet the elasticsearch.url to point at your Elasticsearch instance\n\n\nRun \n./bin/kibana\n (orbin\\kibana.bat on Windows)\n\n\nPoint your browser at \nhttp://localhost:5601\n\n\n\n\n\n\n\n\nInstall \nSense\n\n\n\n\n\n\n./bin/kibana plugin --install elastic/sense\n\n\n\n\n\nOn Windows:\n \n\n\nbin\n\\k\nibana.bat plugin --install elastic/sense\n\n\n\n\n\nThen go to\n\n\nhttp://localhost:5601/app/sense\n\n\nCURL\n\u00b6\n\n\ncurl -X<VERB> \n'<PROTOCOL>://<HOST>:<PORT>/<PATH>?<QUERY_STRING>'\n -d \n'<BODY>'\n\n\n\n\n\n\nverb is GET, POST, PUT, HEAD, or DELETE\n\n\nExamples\n\u00b6\n\n\ncurl -XGET \n'http://localhost:9200/_count?pretty'\n -d \n'{ \"query\": { \"match_all\": {} }}'\n\n\n\n\n\n\ncurl -XGET <id>.us-west-2.es.amazonaws.com\n\ncurl -XGET \n'https://<id>.us-west-2.es.amazonaws.com/_count?pretty'\n -d \n'{ \"query\": { \"match_all\": {} } }'\n\n\ncurl -XPUT https://<id>.us-west-2.es.amazonaws.com/movies/movie/tt0116996 -d \n'{\"directors\" : [\"Tim Burton\"],\"genres\" : [\"Comedy\",\"Sci-Fi\"], \"plot\": \"The Earth is invaded by Martians with irresistible weapons and a cruel sense of humor.\", \"title\" : \"Mars Attacks!\", \"actors\" :[\"Jack Nicholson\",\"Pierce Brosnan\",\"Sarah Jessica Parker\"], \"year\" : 1996}'\n\n\n\n\n\n\nSense\n\u00b6\n\n\nSense syntax is similar to curl:\n\n\nIndex a document\n\n\nPUT index/type/1\n{\n \"body\": \"here\"\n}\n\n\n\n\n\nand retrieve it\n\n\nGET index/type/1\n\n\n\n\n\nPLUGINS\n\u00b6\n\n\nURL pattern\n\n\nhttp://yournode:9200/_plugin/<plugin name>\n\n\nOn Debian, the script is in: \n/usr/share/elasticsearch/bin/plugin\n.\n\n\nInstall various plugins\n\n\n./bin/plugin --install mobz/elasticsearch-head\n./bin/plugin --install lmenezes/elasticsearch-kopf/1.2\n./bin/plugin --install elasticsearch/marvel/latest\n\n\n\n\n\nRemove a plugin\n\n\n./bin/plugin --remove\n\n\n\n\n\nList installed plugins\n\n\n./bin/plugin --list\n\n\n\n\n\nGET /_nodes?plugin=true\n\n\n\n\n\nElasticsearch monitoring and management plugins\n\n\nHead\n\n\nHead\n\n\n\n\nelasticsearch/bin/plugin -install mobz/elasticsearch-head\n\n\nopen \nhttp://localhost:9200/_plugin/head\n\n\n\n\nelastichq.org\n\n\nBigDesk\n\n\nLive charts and statistics for elasticsearch cluster: \n\nBigDesk\n\n\nKopf\n\n\nKopf\n\n\n./bin/plugin --install lmenezes/elasticsearch-kopf/1.2`\n\n\n\n\n\nMarvel\n\n\n./bin/plugin --install elasticsearch/marvel/latest\n\n\n\n\n\nIntegrations (CMS, import/export, hadoop...)\n\u00b6\n\n\nIntegrations\n\n\nAspire\n\n\nAspire\n\n\nAspire is a framework and libraries of extensible components designed to enable creation of solutions to acquire data from one or more content repositories (such as file systems, relational databases, cloud storage, or content management systems), extract metadata and text from the documents, analyze, modify and enhance the content and metadata if needed, and then publish each document, together with its metadata, to a search engine or other target application\n\n\nDocs\n\n\nIntegration with Hadoop\n\n\nIntegration with Hadoop\n\n\nBulk loading for elastic search http://infochimps.com\n\n\nIntegration with Spring\n\n\nSpring Data\n\n\nWordPress\n\n\nWordpress\n\n\nTOOLS\n\u00b6\n\n\nBI platforms that can use ES as an analytics engine:\n\n\n\n\nKibana\n\n\nGrafana\n\n\n\n\nBIRT\n\n\n\n\nBirt\n\n\nBirt\n\n\n\n\n\n\n\n\nAdminer\n\n\n\n\nAdminer.org\n\n\nDatabase management in a single PHP file. Works with MySQL, PostgreSQL, SQLite, MS SQL, Oracle, SimpleDB, Elasticsearch, MongoDB. Needs a webserver + PHP: \nWAMP\n\n\n\n\n\n\n\n\nMongolastic\n\n\n\n\nA tool that migrates data from MongoDB to Elasticsearch and vice versa\n\n\nMongolastic\n\n\n\n\n\n\n\n\nElasticsearch-exporter\n\n\n\n\nElasticsearch-exporter\n\n\n\n\n\n\n\n\nCode Examples - developing a Web UI for ES\n\u00b6\n\n\n\n\nSitepoint\n\n\nCottageLabs\n\n\nscrutmydocs.org\n\n\nqbox.io\n\n\n\n\nJava API\n\u00b6\n\n\n\n\nJava clients\n\n\nelasticsearch tutorial\n\n\nelasticsearchfr/\n\n\nIBM\n\n\ndzone\n\n\n\n\nBASICS\n\u00b6\n\n\nAn Elasticsearch cluster can contain multiple indices, which in turn contain multiple types. These types hold multiple documents, and each document has multiple fields.\n\n\nExplore (using Sense)\n\u00b6\n\n\nGET _stats/\n\n\n# List indices\n\n\nGET /_cat/indices/\nGET /_cat/indices/my_ind*\n\n\n\n\n\n# Get info about one index\n\n\nGET /twitter\nGET /my_index_nr_1*/_settings?pretty   or ?v\nGET /twitter/_settings,_mappings\n\n\n\n\n\nThe available features are _settings, _mappings, _warmers and _aliases\n\n\n# cluster\n\n\nGET /_nodes\n\n\n\n\n\n# insert data\n\n\nPUT my_index/user/1\n{\n\"first_name\":    \"John\",\n\"last_name\":     \"Smith\",\n\"date_of_birth\": \"1970-10-24\"\n}\n\n\n\n\n\n#search\n\n\nGET my_index/_search\n\nGET _count?pretty\n\n\n\n\n\n# Data schema\n\n\nGET my_index/_mapping\n\n\n\n\n\nINSERT DOCUMENTS\n\u00b6\n\n\nPUT /index/type/ID\nPUT /megacorp/employee/1\n{ \"first_name\" : \"John\", \"last_name\" : \"Smith\", \"age\" : 25, \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ]}\n\nPUT /megacorp/employee/2\n{ \"first_name\" : \"Jane\", \"last_name\" : \"Smith\", \"age\" : 32, \"about\" : \"I like to collect rock albums\", \"interests\": [ \"music\" ]}\n\nGET /megacorp/employee/1\n\n\n\n\n\nField names can be any valid string, but may not include periods.\nEvery document in Elasticsearch has a version number. Every time a change is made to a document (including deleting it), the _version number is incremented.\n\n\nOptimistic concurrency control\n\n\nPUT /website/blog/1?version=1  { \"title\": \"My first blog entry\", \"text\": \"Starting to get the hang of this...\"}\n\nWe want this update to succeed only if the current _version of this document in our index is version 1\n\nExternal version:\n\nPUT /website/blog/2?version=5&version_type=external { \"title\": \"My first external blog entry\", \"text\": \"Starting to get the hang of this...\"}\n\n\n\n\n\nINSERT DOCUMENTS - AUTOGENERATED IDS\n\u00b6\n\n\nPOST /website/blog/\n{\n\"title\": \"My second blog entry\",\n\"text\":  \"Still trying this out...\",\n\"date\":  \"2014/01/01\"\n}\n\n\n\n\n\nResponse:\n\n\n{\n\"_index\":    \"website\",\n\"_type\":     \"blog\",\n\"_id\":       \"AVFgSgVHUP18jI2wRx0w\",\n\"_version\":  1,\n\"created\":   true\n}\n\n\n\n\n\n#  creating an entirely new document and not overwriting an existing one\n\n\nPUT /website/blog/123?op_type=create { ... }\nPUT /website/blog/123/_create { ... }\n\n\n\n\n\nRETRIEVE DOCUMENTS\n\u00b6\n\n\nGET /website/blog/123  # optional ?pretty\n\n\n\n\n\n{ \"_index\" : \"website\", \"_type\" : \"blog\", \"_id\" : \"123\", \"_version\" : 1, \"found\" : true, \"_source\" : { \"title\": \"My first blog entry\", \"text\": \"Just trying this out...\", \"date\": \"2014/01/01\" }}\n\n\n# Contains just the fields that we requested\n\n\nGET /website/blog/123?_source=title,text\n\n\n\n\n\n# Just get the original doc\n\n\nGET /website/blog/123/_source\n\n\n\n\n\n# check if doc exists -- HTTP 200 or 404\n\n\ncurl -i -XHEAD http://localhost:9200/website/blog/123\n\n\n\n\n\n# Note: HEAD/exists requests do not work in Sense\n# because they only return HTTP headers, not\n# a JSON body\n\n\n# multiple docs at once\n\n\nGET /website/blog/_mget { \"ids\" : [ \"2\", \"1\" ]}\n\n\n\n\n\nUPDATE\n\u00b6\n\n\nDocuments in Elasticsearch are immutable; we cannot change them. Instead, if we need to update an existing document, we reindex or replace it\n\n\n# Accepts a partial document as the doc parameter, which just gets merged with the existing document.\n\n\nPOST /website/blog/1/_update\n{ \"doc\" : { \"tags\" : [ \"testing\" ], \"views\": 0 }}\n\n\n\n\n\n# Script\n\n\nPOST /website/blog/1/_update\n{ \"script\" : \"ctx._source.views+=1\"}\n\n\n\n\n\n# script with parameters\n\n\nPOST /website/blog/1/_update\n{ \"script\" : \"ctx._source.tags+=new_tag\", \"params\" : { \"new_tag\" : \"search\" }}\n\n\n\n\n\n# upsert\n\n\nPOST/website/pageviews/1/_update\n{\"script\":\"ctx._source.views+=1\",\"upsert\":{\"views\":1}}\n\n\n\n\n\nDELETE\n\u00b6\n\n\nDELETE /website/blog/123\n\n\n\n\n\n# delete doc based on its contents\n\n\nPOST /website/blog/1/_update { \"script\" : \"ctx.op = ctx._source.views == count ? 'delete' : 'none'\", \"params\" : { \"count\": 1 }}\n\n\n\n\n\nBULK\n\u00b6\n\n\nPOST /_bulk\n{\"delete\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}}\n{\"create\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}} #  Create a document only if the document does not already exist\n{\"title\":\"My first blog post\"}\n{\"index\":{\"_index\":\"website\",\"_type\":\"blog\"}}\n{\"title\":\"My second blog post\"}\n{\"update\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\",\"_retry_on_conflict\":3}}\n{\"doc\":{\"title\":\"My updated blog post\"}}\n\n\n\n\n\nBulk in the same index or index/type\n\n\nPOST /website/_bulk\n{\"index\":{\"_type\":\"log\"}}\n{\"event\":\"User logged in\"}\n{\"index\":{\"_type\":\"blog\"}}\n{\"title\":\"My second blog post\"}\n\n\n\n\n\nTry  around 5-15MB in size.\n\n\nSEARCH\n\u00b6\n\n\nEvery field in a document is indexed and can be queried.\n\n\n# Search for all employees in the megacorp index:\n\n\nGET /megacorp/employee/_search\n\n\n\n\n\n# Search for all employees in the megacorp index\n# who have \"Smith\" in the last_name field\n\n\nGET /megacorp/employee/_search?q=last_name:Smith\n\n\n\n\n\n# Same query as above, but using the Query DSL\n\n\nGET /megacorp/employee/_search\n{\n    \"query\": {\n      \"match\": {\n        \"last_name\": \"smith\"\n      }\n    }\n}\n\n\n\n\n\n# SEARCH QUERY STRING\n\n\nGET /_all/tweet/_search?q=tweet:elasticsearch\n\n\n\n\n\nDon't forget to URL encode special characters e.g. +name:john +tweet:mary\n\n\nGET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary\n\n\n\n\n\nThe + prefix indicates conditions that must be satisfied for our query to match. Similarly a - prefix would indicate conditions that must not match. All conditions without a + or - are optional\n\n\n+name:(mary john) +date:>2014-09-10 +(aggregations geo) # last part searches _all\n\n\n\n\n\nQUERY DSL\n\u00b6\n\n\nWhen used in filtering context, the query is said to be a \"non-scoring\" or \"filtering\" query. That is, the query simply asks the question: \"Does this document match?\". The answer is always a simple, binary yes|no.\nWhen used in a querying context, the query becomes a \"scoring\" query.\n\n\n# Find all employees whose `last_name` is Smith\n# and who are older than 30\nGET /megacorp/employee/_search\n{\n\"query\" : {\n  \"filtered\" : {\n      \"filter\" : {\n      \"range\" : {\n          \"age\" : { \"gt\" : 30 }\n      }\n      },\n      \"query\" : {\n      \"match\" : {\n          \"last_name\" : \"smith\"\n      }\n      }\n  }\n}\n}\n\n\n\n\n\nMATCH\n\u00b6\n\n\n# Find all employees who enjoy \"rock\" or \"climbing\"\n\n\nGET /megacorp/employee/_search\n{\n\"query\" : {\n  \"match\" : {\n      \"about\" : \"rock climbing\"\n  }\n}\n}\n\n\n\n\n\nThe match query should be the standard query that you reach for whenever you want to query for a full-text or exact value in almost any field.\nIf you run a match query against a full-text field, it will analyze the query string by using the correct analyzer for that field before executing the search\nIf you use it on a field containing an exact value, such as a number, a date, a Boolean, or a not_analyzedstring field, then it will search for that exact value\n\n\nMATCH ON MULTIPLE FIELDS\n\u00b6\n\n\n{\n\"multi_match\": {\n  \"query\":    \"full text search\",\n  \"fields\":  [ \"title\", \"body\" ]\n}}\n\n\n\n\n\nEXACT SEARCH\n\u00b6\n\n\n# Find all employees who enjoy \"rock climbing\"\n\n\nGET /megacorp/employee/_search\n{\n\"query\" : {\n  \"match_phrase\" : {\n      \"about\" : \"rock climbing\"\n  }\n}\n}\n\n\n\n\n\n# EXACT VALUES\n\n\nThe term query is used to search by exact values, be they numbers, dates, Booleans, or not_analyzed exact-value string fields\n\n\nThe terms query is the same as the term query, but allows you to specify multiple values to match. If the field contains any of the specified values, the document matches\n\n\n{ \"terms\": { \"tag\": [ \"search\", \"full_text\", \"nosql\" ] }}\n\n\n\n\n\n# Compound Queries\n\n\n{\n     \"bool\": {\n       \"must\": { \"match\": { \"tweet\": \"elasticsearch\" }},\n        \"must_not\": { \"match\": { \"name\": \"mary\" }},\n        \"should\": { \"match\": { \"tweet\": \"full text\" }},\n        \"filter\": { \"range\": { \"age\" : { \"gt\" : 30 }} }\n     }\n  }\n\n\n# VALIDATE A QUERY\n\n\nGET /gb/tweet/_validate/query?explain { \"query\": { \"tweet\" : { \"match\" : \"really powerful\" } }}\n\n\n# understand why one particular document matched or, more important, why it didn\u2019t match\n\n\nGET /us/tweet/12/_explain { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 2 }}, \"must\" : { \"match\" : { \"tweet\" : \"honeymoon\" }} } }}\n\n\nMULTIPLE INDICES OR TYPES\n\u00b6\n\n\n# all documents all indices\n\n\n\n\n\n/_search\n\n\n/gb,us/_search\n  Search all types in the gb and us indices\n\n\n/g\n,u\n/_search\n  Search all types in any indices beginning with g or beginning with u\n\n\n/gb/user/_search\n  Search type user in the gb index\n\n\n/gb,us/user,tweet/_search\n  Search types user and tweet in the gb and us indices\n\n\n/_all/user,tweet/_search\n  Search types user and tweet in all indices\n\n\nPAGINATION\n\u00b6\n\n\n GET /_search?size=5GET /_search?size=5&from=5\n\n\n\n\n\nSORTING\n\u00b6\n\n\n GET /_search { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 1 }} } }, \"sort\": { \"date\": { \"order\": \"desc\" }}}\n\n\n\n\n\nFor string sorting, use multi-field mapping:\n\n\n \"tweet\": { \"type\": \"string\", \"analyzer\": \"english\", \"fields\": { \"raw\": {\"type\": \"string\", \"index\": \"not_analyzed\" } }}\n\n\n\n\n\nThe main tweet field is just the same as before: an analyzed full-text field.\nThe new tweet.raw subfield is not_analyzed.\n\n\nthen sort on the new field\n\n\n GET /_search { \"query\": { \"match\": { \"tweet\": \"elasticsearch\" } }, \"sort\": \"tweet.raw\"}\n\n\n\n\n\nHIGHLIGHTS\n\u00b6\n\n\n# Find all employees who enjoy \"rock climbing\" - highlights\n# and highlight the matches\n\n\nGET /megacorp/employee/_search\n  {\n      \"query\" : {\n          \"match_phrase\" : {\n              \"about\" : \"rock climbing\"\n          }\n      },\n      \"highlight\": {\n          \"fields\" : {\n              \"about\" : {}\n          }\n      }\n  }\n\n\nANALYSIS\n\u00b6\n\n\nAn analyzer is really just a wrapper that combines three functions into a single package:\n\n\n* Character filters\n* Tokenizer\n* Token filters\n\n\n\n\n\n#  See how text is analyzed\n\n\nGET /_analyze { \"analyzer\": \"standard\", \"text\": \"Text to analyze\"}\n\n\n# test analyzer\n\n\nGET /gb/_analyze { \"field\": \"tweet\", \"text\": \"Black-cats\"}\n\n\nMAPPINGS (schemas)\n\u00b6\n\n\nEvery type has its own mapping, or schema definition. A mapping defines the fields within a type, the datatype for each field, and how the field should be handled by Elasticsearch. A mapping is also used to configure metadata associated with the type.\n\n\nYou can control dynamic nature of mappings\n\n\nMapping (or schema definition) for the tweet type in the gb index\n\n\n  GET /gb/_mapping/tweet\n\n\n\n\n\nElasticsearch supports the following simple field types:\n\n String: string\n\n Whole number: byte, short, integer, long\n\n Floating-point: float, double\n\n Boolean: boolean\n* Date: date\n\n\nFields of type string are, by default, considered to contain full text. That is, their value will be passed through an analyzer before being indexed, and a full-text query on the field will pass the query string through an analyzer before searching.\nThe two most important mapping attributes for string fields are index and analyzer.\n\n\nThe index attribute controls how the string will be indexed. It can contain one of three values:\n\n analyzed  First analyze the string and then index it. In other words, index this field as full text.\n\n not_analyzed  Index this field, so it is searchable, but index the value exactly as specified. Do not analyze it.\n* no  Don\u2019t index this field at all. This field will not be searchable.\n\n\nIf we want to map the field as an exact value, we need to set it to not_analyzed:\n\n\n  {\n    \"tag\": {\n    \"type\": \"string\",\n    \"index\": \"not_analyzed\"\n    }\n  }\n\n\n\n\n\nFor analyzed string fields, use the analyzer attribute to specify which analyzer to apply both at search time and at index time. By default, Elasticsearch uses the standard analyzer, but you can change this by specifying one of the built-in analyzers, such as whitespace, simple, or english:\n\n\n  {\n    \"tweet\": {\n    \"type\": \"string\",\n    \"analyzer\": \"english\"\n    }\n  }\n\n\n\n\n\n#  create a new index, specifying that the tweet field should use the english analyzer\n\n\nPUT /gb\n  { \"mappings\":\n       { \"tweet\" :\n                 { \"properties\" : {\n                      \"tweet\" : { \"type\" : \"string\", \"analyzer\": \"english\" },\n                      \"date\" : { \"type\" : \"date\" },\n                      \"name\" : { \"type\" : \"string\" },\n                      \"user_id\" : { \"type\" : \"long\" }\n                    }}}}\n\n\nnull, arrays, objects: see \ncomplex core fields\n\n\nParent Child Relationships\n\u00b6\n\n\nDELETE\n \n/test_index\n\n\n\nPUT\n \n/test_index\n\n\n{\n\n   \n\"mappings\"\n:\n \n{\n\n      \n\"parent_type\"\n:\n \n{\n\n         \n\"properties\"\n:\n \n{\n\n            \n\"num_prop\"\n:\n \n{\n\n               \n\"type\"\n:\n \n\"integer\"\n\n            \n},\n\n            \n\"str_prop\"\n:\n \n{\n\n               \n\"type\"\n:\n \n\"string\"\n\n            \n}\n\n         \n}\n\n      \n},\n\n      \n\"child_type\"\n:\n \n{\n\n         \n\"_parent\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"parent_type\"\n\n         \n},\n\n         \n\"properties\"\n:\n \n{\n\n            \n\"child_num\"\n:\n \n{\n\n               \n\"type\"\n:\n \n\"integer\"\n\n            \n},\n\n            \n\"child_str\"\n:\n \n{\n\n               \n\"type\"\n:\n \n\"string\"\n\n            \n}\n\n         \n}\n\n      \n}\n\n   \n}\n\n\n}\n\n\n\nPOST\n \n/test_index/_bulk\n\n\n{\n\"index\"\n:{\n\"_type\"\n:\n\"parent_type\"\n,\n\"_id\"\n:\n1\n}}\n\n\n{\n\"num_prop\"\n:\n1\n,\n\"str_prop\"\n:\n\"hello\"\n}\n\n\n{\n\"index\"\n:{\n\"_type\"\n:\n\"child_type\"\n,\n\"_id\"\n:\n1\n,\n\"_parent\"\n:\n1\n}}\n\n\n{\n\"child_num\"\n:\n11\n,\n\"child_str\"\n:\n\"foo\"\n}\n\n\n{\n\"index\"\n:{\n\"_type\"\n:\n\"child_type\"\n,\n\"_id\"\n:\n2\n,\n\"_parent\"\n:\n1\n}}\n\n\n{\n\"child_num\"\n:\n12\n,\n\"child_str\"\n:\n\"bar\"\n}\n\n\n{\n\"index\"\n:{\n\"_type\"\n:\n\"parent_type\"\n,\n\"_id\"\n:\n2\n}}\n\n\n{\n\"num_prop\"\n:\n2\n,\n\"str_prop\"\n:\n\"goodbye\"\n}\n\n\n{\n\"index\"\n:{\n\"_type\"\n:\n\"child_type\"\n,\n\"_id\"\n:\n3\n,\n\"_parent\"\n:\n2\n}}\n\n\n{\n\"child_num\"\n:\n21\n,\n\"child_str\"\n:\n\"baz\"\n}\n\n\n\nPOST\n \n/test_index/child_type/_search\n\n\n\nPOST\n \n/test_index/child_type/\n2\n?parent=\n1\n\n\n{\n\n   \n\"child_num\"\n:\n \n13\n,\n\n   \n\"child_str\"\n:\n \n\"bars\"\n\n\n}\n\n\n\nPOST\n \n/test_index/child_type/_search\n\n\n\nPOST\n \n/test_index/child_type/\n3\n/_update?parent=\n2\n\n\n{\n\n   \n\"script\"\n:\n \n\"ctx._source.child_num+=1\"\n\n\n}\n\n\n\nPOST\n \n/test_index/child_type/_search\n\n\n\nPOST\n \n/test_index/child_type/_search\n\n\n{\n\n    \n\"query\"\n:\n \n{\n\n        \n\"term\"\n:\n \n{\n\n           \n\"child_str\"\n:\n \n{\n\n              \n\"value\"\n:\n \n\"foo\"\n\n           \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\nPOST\n \n/test_index/parent_type/_search\n\n\n{\n\n   \n\"query\"\n:\n \n{\n\n      \n\"filtered\"\n:\n \n{\n\n         \n\"query\"\n:\n \n{\n\n            \n\"match_all\"\n:\n \n{}\n\n         \n},\n\n         \n\"filter\"\n:\n \n{\n\n            \n\"has_child\"\n:\n \n{\n\n               \n\"type\"\n:\n \n\"child_type\"\n,\n\n               \n\"filter\"\n:\n \n{\n\n                  \n\"term\"\n:\n \n{\n\n                     \n\"child_str\"\n:\n \n\"foo\"\n\n                  \n}\n\n               \n}\n\n            \n}\n\n         \n}\n\n      \n}\n\n   \n}\n\n\n}\n\n\n\n\n\n\nAGGREGATES\n\u00b6\n\n\nAggregations and searches can span multiple indices\n\n\n# Calculate the most popular interests for all employees\n\n\nGET /megacorp/employee/_search\n  {\n    \"aggs\": {\n      \"all_interests\": {\n        \"terms\": {\n          \"field\": \"interests\"\n        }\n      }\n    }\n  }\n\n\n# Calculate the most popular interests for\n# employees named \"Smith\"\n\n\nGET /megacorp/employee/_search\n  {\n    \"query\": {\n      \"match\": {\n        \"last_name\": \"smith\"\n      }\n    },\n    \"aggs\": {\n      \"all_interests\": {\n        \"terms\": {\n          \"field\": \"interests\"\n        }\n      }\n    }\n  }\n\n\n# Calculate the average age of employee per interest - hierarchical aggregates\n\n\nGET /megacorp/employee/_search\n  {\n      \"aggs\" : {\n          \"all_interests\" : {\n              \"terms\" : { \"field\" : \"interests\" },\n              \"aggs\" : {\n                  \"avg_age\" : {\n                      \"avg\" : { \"field\" : \"age\" }\n                  }\n              }\n          }\n      }\n  }\n\n\n# requires in config/elasticsearch.yml\n# script.inline: true\n# script.indexed: true\n\n\nGET /tlo/contacts/_search\n  {\n    \"size\" : 0,\n    \"query\": {\n      \"constant_score\": {\n        \"filter\": {\n          \"terms\": {\n            \"version\": [\n              \"20160301\",\n              \"20160401\"\n            ]\n          }\n        }\n      }\n    },\n    \"aggs\": {\n      \"counts\": {\n        \"cardinality\": {\n          \"script\": \"doc['first_name'].value + ' ' + doc['last_name'].value + ' ' + doc['company'].value\",\n          \"missing\": \"N/A\"\n        }\n      }\n    }\n  }\n\n\nINDEX MANAGEMENT\n\u00b6\n\n\nBy default, indices are assigned five primary shards. The number of primary shards can be set only when an index is created and never changed\n\n\n# Add an index\n\n\n  PUT /blogs { \"settings\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 1 }}\n  PUT /blogs/_settings { \"number_of_replicas\" : 2}\n\n\n\n\n\n\n\nElasticSearch Shards should be 50 GB or less in size.\n\n\nUse aliases to shelter the underlying index (or indices) and allow index swapping\n\n\n\n\nCLUSTER MANAGEMENT\n\u00b6\n\n\n GET /_cluster/health\n\n\n\n\n\nCONFIGURATION\n\u00b6\n\n\n\n\nconfig directory\n\n\n\n\nyaml file\n\n\n\n\n\n\nSets the JVM heap size to 0.5 memory size. The OS will use it for file system cache\n\n\n\n\nPrefer not to allocate 30GB !! --> uncompressed pointers\n\n\nNever let the JVM swap    bootstrap.mlockall = true\n\n\nKeep the JVM defaults\n\n\nDo not use G1GC alternative garbage collector\n\n\n\n\ncluster.name: <my cluster>\n\n\n\n\n\n\n\nAll nodes in the cluster must have the same cluster name\n\n\n\n\nnode.name: <my_node_name>\n\n\n\n\n\n./bin/elasticsearch --node.name=`hostname`\n\n\n\n\n\nto override the configuration file\n\n\n\n\nHTTP port: 9200 and successors\n\n\nTransport : 9300 (internal communications)\n\n\n\n\nDiscovery\n\u00b6\n\n\n\n\nAWS plugin available   --> also include integration with S3 (snapshot to S3)\n\n\nAWS: multi-AZ is OK but replication across far data centers is not recommended\n\n\nSee: resiliency\n\n\n\n\nSites plugins -- kopf / head / paramedic / bigdesk / kibana\n- contain static web content (JS, HTML....)\n\n\nInstall plugins on ALL machines of the cluster\n\n\nTo install,\n\n\n  ./bin/plugin install marvel-agent\n  ./bin/plugin remove marvel-agent\n\n\n\n\n\nOne type per index is recommended, except for parent child / nested indexes.\n\n\nindex size optimization:\n- can disable \n_source\n and \n_all\n (the index that captures every field - not needed unless the top search bar changes)\n- by default, Kibana will search \n_all\n\n\ndata types:\nstring, number, bool, datetime, binary, array, object, geo_point, geo_shape, ip, multifield\nbinary should be base64 encoded before storage\n\n\nMAINTENANCE\n\u00b6\n\n\nSteps to restore elastic search data:\n\n\n\n\nStop elastic search\n\n\nExtract the zip file (dump file)\n\n\nStart elastic search\n\n\nReload elastic search\n\n\n\n\nThe commands to do the above are as below:\n\n\n\n\nsystemctl stop elasticsearch\n\n\nextract gz file to destination path\n\n\nsystemctl start elasticsearch\n\n\nsystemctl daemon-reload elasticsearch",
            "title": "ElasticSearch Cheatsheet"
        },
        {
            "location": "/Search/ElasticSearch/#cheatsheets",
            "text": "Jolicode",
            "title": "Cheatsheets"
        },
        {
            "location": "/Search/ElasticSearch/#development-urls",
            "text": "Kibana (port 5601)  Sense  ElasticSearch (port 9200)",
            "title": "Development URLs"
        },
        {
            "location": "/Search/ElasticSearch/#install",
            "text": "Install  curl  Install  Java  Download  ElasticSearch  Optionally change the  cluster.name  in the  elasticsearch.yml  configuration   cd  elasticsearch-<version>\n./bin/elasticsearch -d # or on Windows   # bin\\elasticsearch.bat \ncurl  'http://localhost:9200/?pretty'     Install  Kibana   Open  config/kibana.yml  in an editor  Set the elasticsearch.url to point at your Elasticsearch instance  Run  ./bin/kibana  (orbin\\kibana.bat on Windows)  Point your browser at  http://localhost:5601     Install  Sense    ./bin/kibana plugin --install elastic/sense  On Windows:    bin \\k ibana.bat plugin --install elastic/sense  Then go to  http://localhost:5601/app/sense",
            "title": "INSTALL"
        },
        {
            "location": "/Search/ElasticSearch/#curl",
            "text": "curl -X<VERB>  '<PROTOCOL>://<HOST>:<PORT>/<PATH>?<QUERY_STRING>'  -d  '<BODY>'   verb is GET, POST, PUT, HEAD, or DELETE",
            "title": "CURL"
        },
        {
            "location": "/Search/ElasticSearch/#examples",
            "text": "curl -XGET  'http://localhost:9200/_count?pretty'  -d  '{ \"query\": { \"match_all\": {} }}'   curl -XGET <id>.us-west-2.es.amazonaws.com\n\ncurl -XGET  'https://<id>.us-west-2.es.amazonaws.com/_count?pretty'  -d  '{ \"query\": { \"match_all\": {} } }' \n\ncurl -XPUT https://<id>.us-west-2.es.amazonaws.com/movies/movie/tt0116996 -d  '{\"directors\" : [\"Tim Burton\"],\"genres\" : [\"Comedy\",\"Sci-Fi\"], \"plot\": \"The Earth is invaded by Martians with irresistible weapons and a cruel sense of humor.\", \"title\" : \"Mars Attacks!\", \"actors\" :[\"Jack Nicholson\",\"Pierce Brosnan\",\"Sarah Jessica Parker\"], \"year\" : 1996}'",
            "title": "Examples"
        },
        {
            "location": "/Search/ElasticSearch/#sense",
            "text": "Sense syntax is similar to curl:  Index a document  PUT index/type/1\n{\n \"body\": \"here\"\n}  and retrieve it  GET index/type/1",
            "title": "Sense"
        },
        {
            "location": "/Search/ElasticSearch/#plugins",
            "text": "URL pattern  http://yournode:9200/_plugin/<plugin name>  On Debian, the script is in:  /usr/share/elasticsearch/bin/plugin .  Install various plugins  ./bin/plugin --install mobz/elasticsearch-head\n./bin/plugin --install lmenezes/elasticsearch-kopf/1.2\n./bin/plugin --install elasticsearch/marvel/latest  Remove a plugin  ./bin/plugin --remove  List installed plugins  ./bin/plugin --list  GET /_nodes?plugin=true  Elasticsearch monitoring and management plugins  Head  Head   elasticsearch/bin/plugin -install mobz/elasticsearch-head  open  http://localhost:9200/_plugin/head   elastichq.org  BigDesk  Live charts and statistics for elasticsearch cluster:  BigDesk  Kopf  Kopf  ./bin/plugin --install lmenezes/elasticsearch-kopf/1.2`  Marvel  ./bin/plugin --install elasticsearch/marvel/latest",
            "title": "PLUGINS"
        },
        {
            "location": "/Search/ElasticSearch/#integrations-cms-importexport-hadoop",
            "text": "Integrations  Aspire  Aspire  Aspire is a framework and libraries of extensible components designed to enable creation of solutions to acquire data from one or more content repositories (such as file systems, relational databases, cloud storage, or content management systems), extract metadata and text from the documents, analyze, modify and enhance the content and metadata if needed, and then publish each document, together with its metadata, to a search engine or other target application  Docs  Integration with Hadoop  Integration with Hadoop  Bulk loading for elastic search http://infochimps.com  Integration with Spring  Spring Data  WordPress  Wordpress",
            "title": "Integrations (CMS, import/export, hadoop...)"
        },
        {
            "location": "/Search/ElasticSearch/#tools",
            "text": "BI platforms that can use ES as an analytics engine:   Kibana  Grafana   BIRT   Birt  Birt     Adminer   Adminer.org  Database management in a single PHP file. Works with MySQL, PostgreSQL, SQLite, MS SQL, Oracle, SimpleDB, Elasticsearch, MongoDB. Needs a webserver + PHP:  WAMP     Mongolastic   A tool that migrates data from MongoDB to Elasticsearch and vice versa  Mongolastic     Elasticsearch-exporter   Elasticsearch-exporter",
            "title": "TOOLS"
        },
        {
            "location": "/Search/ElasticSearch/#code-examples-developing-a-web-ui-for-es",
            "text": "Sitepoint  CottageLabs  scrutmydocs.org  qbox.io",
            "title": "Code Examples - developing a Web UI for ES"
        },
        {
            "location": "/Search/ElasticSearch/#java-api",
            "text": "Java clients  elasticsearch tutorial  elasticsearchfr/  IBM  dzone",
            "title": "Java API"
        },
        {
            "location": "/Search/ElasticSearch/#basics",
            "text": "An Elasticsearch cluster can contain multiple indices, which in turn contain multiple types. These types hold multiple documents, and each document has multiple fields.",
            "title": "BASICS"
        },
        {
            "location": "/Search/ElasticSearch/#explore-using-sense",
            "text": "GET _stats/  # List indices  GET /_cat/indices/\nGET /_cat/indices/my_ind*  # Get info about one index  GET /twitter\nGET /my_index_nr_1*/_settings?pretty   or ?v\nGET /twitter/_settings,_mappings  The available features are _settings, _mappings, _warmers and _aliases  # cluster  GET /_nodes  # insert data  PUT my_index/user/1\n{\n\"first_name\":    \"John\",\n\"last_name\":     \"Smith\",\n\"date_of_birth\": \"1970-10-24\"\n}  #search  GET my_index/_search\n\nGET _count?pretty  # Data schema  GET my_index/_mapping",
            "title": "Explore (using Sense)"
        },
        {
            "location": "/Search/ElasticSearch/#insert-documents",
            "text": "PUT /index/type/ID\nPUT /megacorp/employee/1\n{ \"first_name\" : \"John\", \"last_name\" : \"Smith\", \"age\" : 25, \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ]}\n\nPUT /megacorp/employee/2\n{ \"first_name\" : \"Jane\", \"last_name\" : \"Smith\", \"age\" : 32, \"about\" : \"I like to collect rock albums\", \"interests\": [ \"music\" ]}\n\nGET /megacorp/employee/1  Field names can be any valid string, but may not include periods.\nEvery document in Elasticsearch has a version number. Every time a change is made to a document (including deleting it), the _version number is incremented.  Optimistic concurrency control  PUT /website/blog/1?version=1  { \"title\": \"My first blog entry\", \"text\": \"Starting to get the hang of this...\"}\n\nWe want this update to succeed only if the current _version of this document in our index is version 1\n\nExternal version:\n\nPUT /website/blog/2?version=5&version_type=external { \"title\": \"My first external blog entry\", \"text\": \"Starting to get the hang of this...\"}",
            "title": "INSERT DOCUMENTS"
        },
        {
            "location": "/Search/ElasticSearch/#insert-documents-autogenerated-ids",
            "text": "POST /website/blog/\n{\n\"title\": \"My second blog entry\",\n\"text\":  \"Still trying this out...\",\n\"date\":  \"2014/01/01\"\n}  Response:  {\n\"_index\":    \"website\",\n\"_type\":     \"blog\",\n\"_id\":       \"AVFgSgVHUP18jI2wRx0w\",\n\"_version\":  1,\n\"created\":   true\n}  #  creating an entirely new document and not overwriting an existing one  PUT /website/blog/123?op_type=create { ... }\nPUT /website/blog/123/_create { ... }",
            "title": "INSERT DOCUMENTS - AUTOGENERATED IDS"
        },
        {
            "location": "/Search/ElasticSearch/#retrieve-documents",
            "text": "GET /website/blog/123  # optional ?pretty  { \"_index\" : \"website\", \"_type\" : \"blog\", \"_id\" : \"123\", \"_version\" : 1, \"found\" : true, \"_source\" : { \"title\": \"My first blog entry\", \"text\": \"Just trying this out...\", \"date\": \"2014/01/01\" }}  # Contains just the fields that we requested  GET /website/blog/123?_source=title,text  # Just get the original doc  GET /website/blog/123/_source  # check if doc exists -- HTTP 200 or 404  curl -i -XHEAD http://localhost:9200/website/blog/123  # Note: HEAD/exists requests do not work in Sense\n# because they only return HTTP headers, not\n# a JSON body  # multiple docs at once  GET /website/blog/_mget { \"ids\" : [ \"2\", \"1\" ]}",
            "title": "RETRIEVE DOCUMENTS"
        },
        {
            "location": "/Search/ElasticSearch/#update",
            "text": "Documents in Elasticsearch are immutable; we cannot change them. Instead, if we need to update an existing document, we reindex or replace it  # Accepts a partial document as the doc parameter, which just gets merged with the existing document.  POST /website/blog/1/_update\n{ \"doc\" : { \"tags\" : [ \"testing\" ], \"views\": 0 }}  # Script  POST /website/blog/1/_update\n{ \"script\" : \"ctx._source.views+=1\"}  # script with parameters  POST /website/blog/1/_update\n{ \"script\" : \"ctx._source.tags+=new_tag\", \"params\" : { \"new_tag\" : \"search\" }}  # upsert  POST/website/pageviews/1/_update\n{\"script\":\"ctx._source.views+=1\",\"upsert\":{\"views\":1}}",
            "title": "UPDATE"
        },
        {
            "location": "/Search/ElasticSearch/#delete",
            "text": "DELETE /website/blog/123  # delete doc based on its contents  POST /website/blog/1/_update { \"script\" : \"ctx.op = ctx._source.views == count ? 'delete' : 'none'\", \"params\" : { \"count\": 1 }}",
            "title": "DELETE"
        },
        {
            "location": "/Search/ElasticSearch/#bulk",
            "text": "POST /_bulk\n{\"delete\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}}\n{\"create\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}} #  Create a document only if the document does not already exist\n{\"title\":\"My first blog post\"}\n{\"index\":{\"_index\":\"website\",\"_type\":\"blog\"}}\n{\"title\":\"My second blog post\"}\n{\"update\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\",\"_retry_on_conflict\":3}}\n{\"doc\":{\"title\":\"My updated blog post\"}}  Bulk in the same index or index/type  POST /website/_bulk\n{\"index\":{\"_type\":\"log\"}}\n{\"event\":\"User logged in\"}\n{\"index\":{\"_type\":\"blog\"}}\n{\"title\":\"My second blog post\"}  Try  around 5-15MB in size.",
            "title": "BULK"
        },
        {
            "location": "/Search/ElasticSearch/#search",
            "text": "Every field in a document is indexed and can be queried.  # Search for all employees in the megacorp index:  GET /megacorp/employee/_search  # Search for all employees in the megacorp index\n# who have \"Smith\" in the last_name field  GET /megacorp/employee/_search?q=last_name:Smith  # Same query as above, but using the Query DSL  GET /megacorp/employee/_search\n{\n    \"query\": {\n      \"match\": {\n        \"last_name\": \"smith\"\n      }\n    }\n}  # SEARCH QUERY STRING  GET /_all/tweet/_search?q=tweet:elasticsearch  Don't forget to URL encode special characters e.g. +name:john +tweet:mary  GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary  The + prefix indicates conditions that must be satisfied for our query to match. Similarly a - prefix would indicate conditions that must not match. All conditions without a + or - are optional  +name:(mary john) +date:>2014-09-10 +(aggregations geo) # last part searches _all",
            "title": "SEARCH"
        },
        {
            "location": "/Search/ElasticSearch/#query-dsl",
            "text": "When used in filtering context, the query is said to be a \"non-scoring\" or \"filtering\" query. That is, the query simply asks the question: \"Does this document match?\". The answer is always a simple, binary yes|no.\nWhen used in a querying context, the query becomes a \"scoring\" query.  # Find all employees whose `last_name` is Smith\n# and who are older than 30\nGET /megacorp/employee/_search\n{\n\"query\" : {\n  \"filtered\" : {\n      \"filter\" : {\n      \"range\" : {\n          \"age\" : { \"gt\" : 30 }\n      }\n      },\n      \"query\" : {\n      \"match\" : {\n          \"last_name\" : \"smith\"\n      }\n      }\n  }\n}\n}",
            "title": "QUERY DSL"
        },
        {
            "location": "/Search/ElasticSearch/#match",
            "text": "# Find all employees who enjoy \"rock\" or \"climbing\"  GET /megacorp/employee/_search\n{\n\"query\" : {\n  \"match\" : {\n      \"about\" : \"rock climbing\"\n  }\n}\n}  The match query should be the standard query that you reach for whenever you want to query for a full-text or exact value in almost any field.\nIf you run a match query against a full-text field, it will analyze the query string by using the correct analyzer for that field before executing the search\nIf you use it on a field containing an exact value, such as a number, a date, a Boolean, or a not_analyzedstring field, then it will search for that exact value",
            "title": "MATCH"
        },
        {
            "location": "/Search/ElasticSearch/#match-on-multiple-fields",
            "text": "{\n\"multi_match\": {\n  \"query\":    \"full text search\",\n  \"fields\":  [ \"title\", \"body\" ]\n}}",
            "title": "MATCH ON MULTIPLE FIELDS"
        },
        {
            "location": "/Search/ElasticSearch/#exact-search",
            "text": "# Find all employees who enjoy \"rock climbing\"  GET /megacorp/employee/_search\n{\n\"query\" : {\n  \"match_phrase\" : {\n      \"about\" : \"rock climbing\"\n  }\n}\n}  # EXACT VALUES  The term query is used to search by exact values, be they numbers, dates, Booleans, or not_analyzed exact-value string fields  The terms query is the same as the term query, but allows you to specify multiple values to match. If the field contains any of the specified values, the document matches  { \"terms\": { \"tag\": [ \"search\", \"full_text\", \"nosql\" ] }}  # Compound Queries  {\n     \"bool\": {\n       \"must\": { \"match\": { \"tweet\": \"elasticsearch\" }},\n        \"must_not\": { \"match\": { \"name\": \"mary\" }},\n        \"should\": { \"match\": { \"tweet\": \"full text\" }},\n        \"filter\": { \"range\": { \"age\" : { \"gt\" : 30 }} }\n     }\n  }  # VALIDATE A QUERY  GET /gb/tweet/_validate/query?explain { \"query\": { \"tweet\" : { \"match\" : \"really powerful\" } }}  # understand why one particular document matched or, more important, why it didn\u2019t match  GET /us/tweet/12/_explain { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 2 }}, \"must\" : { \"match\" : { \"tweet\" : \"honeymoon\" }} } }}",
            "title": "EXACT SEARCH"
        },
        {
            "location": "/Search/ElasticSearch/#multiple-indices-or-types",
            "text": "# all documents all indices  /_search  /gb,us/_search\n  Search all types in the gb and us indices  /g ,u /_search\n  Search all types in any indices beginning with g or beginning with u  /gb/user/_search\n  Search type user in the gb index  /gb,us/user,tweet/_search\n  Search types user and tweet in the gb and us indices  /_all/user,tweet/_search\n  Search types user and tweet in all indices",
            "title": "MULTIPLE INDICES OR TYPES"
        },
        {
            "location": "/Search/ElasticSearch/#pagination",
            "text": "GET /_search?size=5GET /_search?size=5&from=5",
            "title": "PAGINATION"
        },
        {
            "location": "/Search/ElasticSearch/#sorting",
            "text": "GET /_search { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 1 }} } }, \"sort\": { \"date\": { \"order\": \"desc\" }}}  For string sorting, use multi-field mapping:   \"tweet\": { \"type\": \"string\", \"analyzer\": \"english\", \"fields\": { \"raw\": {\"type\": \"string\", \"index\": \"not_analyzed\" } }}  The main tweet field is just the same as before: an analyzed full-text field.\nThe new tweet.raw subfield is not_analyzed.  then sort on the new field   GET /_search { \"query\": { \"match\": { \"tweet\": \"elasticsearch\" } }, \"sort\": \"tweet.raw\"}",
            "title": "SORTING"
        },
        {
            "location": "/Search/ElasticSearch/#highlights",
            "text": "# Find all employees who enjoy \"rock climbing\" - highlights\n# and highlight the matches  GET /megacorp/employee/_search\n  {\n      \"query\" : {\n          \"match_phrase\" : {\n              \"about\" : \"rock climbing\"\n          }\n      },\n      \"highlight\": {\n          \"fields\" : {\n              \"about\" : {}\n          }\n      }\n  }",
            "title": "HIGHLIGHTS"
        },
        {
            "location": "/Search/ElasticSearch/#analysis",
            "text": "An analyzer is really just a wrapper that combines three functions into a single package:  * Character filters\n* Tokenizer\n* Token filters  #  See how text is analyzed  GET /_analyze { \"analyzer\": \"standard\", \"text\": \"Text to analyze\"}  # test analyzer  GET /gb/_analyze { \"field\": \"tweet\", \"text\": \"Black-cats\"}",
            "title": "ANALYSIS"
        },
        {
            "location": "/Search/ElasticSearch/#mappings-schemas",
            "text": "Every type has its own mapping, or schema definition. A mapping defines the fields within a type, the datatype for each field, and how the field should be handled by Elasticsearch. A mapping is also used to configure metadata associated with the type.  You can control dynamic nature of mappings  Mapping (or schema definition) for the tweet type in the gb index    GET /gb/_mapping/tweet  Elasticsearch supports the following simple field types:  String: string  Whole number: byte, short, integer, long  Floating-point: float, double  Boolean: boolean\n* Date: date  Fields of type string are, by default, considered to contain full text. That is, their value will be passed through an analyzer before being indexed, and a full-text query on the field will pass the query string through an analyzer before searching.\nThe two most important mapping attributes for string fields are index and analyzer.  The index attribute controls how the string will be indexed. It can contain one of three values:  analyzed  First analyze the string and then index it. In other words, index this field as full text.  not_analyzed  Index this field, so it is searchable, but index the value exactly as specified. Do not analyze it.\n* no  Don\u2019t index this field at all. This field will not be searchable.  If we want to map the field as an exact value, we need to set it to not_analyzed:    {\n    \"tag\": {\n    \"type\": \"string\",\n    \"index\": \"not_analyzed\"\n    }\n  }  For analyzed string fields, use the analyzer attribute to specify which analyzer to apply both at search time and at index time. By default, Elasticsearch uses the standard analyzer, but you can change this by specifying one of the built-in analyzers, such as whitespace, simple, or english:    {\n    \"tweet\": {\n    \"type\": \"string\",\n    \"analyzer\": \"english\"\n    }\n  }  #  create a new index, specifying that the tweet field should use the english analyzer  PUT /gb\n  { \"mappings\":\n       { \"tweet\" :\n                 { \"properties\" : {\n                      \"tweet\" : { \"type\" : \"string\", \"analyzer\": \"english\" },\n                      \"date\" : { \"type\" : \"date\" },\n                      \"name\" : { \"type\" : \"string\" },\n                      \"user_id\" : { \"type\" : \"long\" }\n                    }}}}  null, arrays, objects: see  complex core fields",
            "title": "MAPPINGS (schemas)"
        },
        {
            "location": "/Search/ElasticSearch/#parent-child-relationships",
            "text": "DELETE   /test_index  PUT   /test_index  { \n    \"mappings\" :   { \n       \"parent_type\" :   { \n          \"properties\" :   { \n             \"num_prop\" :   { \n                \"type\" :   \"integer\" \n             }, \n             \"str_prop\" :   { \n                \"type\" :   \"string\" \n             } \n          } \n       }, \n       \"child_type\" :   { \n          \"_parent\" :   { \n             \"type\" :   \"parent_type\" \n          }, \n          \"properties\" :   { \n             \"child_num\" :   { \n                \"type\" :   \"integer\" \n             }, \n             \"child_str\" :   { \n                \"type\" :   \"string\" \n             } \n          } \n       } \n    }  }  POST   /test_index/_bulk  { \"index\" :{ \"_type\" : \"parent_type\" , \"_id\" : 1 }}  { \"num_prop\" : 1 , \"str_prop\" : \"hello\" }  { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 1 , \"_parent\" : 1 }}  { \"child_num\" : 11 , \"child_str\" : \"foo\" }  { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 2 , \"_parent\" : 1 }}  { \"child_num\" : 12 , \"child_str\" : \"bar\" }  { \"index\" :{ \"_type\" : \"parent_type\" , \"_id\" : 2 }}  { \"num_prop\" : 2 , \"str_prop\" : \"goodbye\" }  { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 3 , \"_parent\" : 2 }}  { \"child_num\" : 21 , \"child_str\" : \"baz\" }  POST   /test_index/child_type/_search  POST   /test_index/child_type/ 2 ?parent= 1  { \n    \"child_num\" :   13 , \n    \"child_str\" :   \"bars\"  }  POST   /test_index/child_type/_search  POST   /test_index/child_type/ 3 /_update?parent= 2  { \n    \"script\" :   \"ctx._source.child_num+=1\"  }  POST   /test_index/child_type/_search  POST   /test_index/child_type/_search  { \n     \"query\" :   { \n         \"term\" :   { \n            \"child_str\" :   { \n               \"value\" :   \"foo\" \n            } \n         } \n     }  }  POST   /test_index/parent_type/_search  { \n    \"query\" :   { \n       \"filtered\" :   { \n          \"query\" :   { \n             \"match_all\" :   {} \n          }, \n          \"filter\" :   { \n             \"has_child\" :   { \n                \"type\" :   \"child_type\" , \n                \"filter\" :   { \n                   \"term\" :   { \n                      \"child_str\" :   \"foo\" \n                   } \n                } \n             } \n          } \n       } \n    }  }",
            "title": "Parent Child Relationships"
        },
        {
            "location": "/Search/ElasticSearch/#aggregates",
            "text": "Aggregations and searches can span multiple indices  # Calculate the most popular interests for all employees  GET /megacorp/employee/_search\n  {\n    \"aggs\": {\n      \"all_interests\": {\n        \"terms\": {\n          \"field\": \"interests\"\n        }\n      }\n    }\n  }  # Calculate the most popular interests for\n# employees named \"Smith\"  GET /megacorp/employee/_search\n  {\n    \"query\": {\n      \"match\": {\n        \"last_name\": \"smith\"\n      }\n    },\n    \"aggs\": {\n      \"all_interests\": {\n        \"terms\": {\n          \"field\": \"interests\"\n        }\n      }\n    }\n  }  # Calculate the average age of employee per interest - hierarchical aggregates  GET /megacorp/employee/_search\n  {\n      \"aggs\" : {\n          \"all_interests\" : {\n              \"terms\" : { \"field\" : \"interests\" },\n              \"aggs\" : {\n                  \"avg_age\" : {\n                      \"avg\" : { \"field\" : \"age\" }\n                  }\n              }\n          }\n      }\n  }  # requires in config/elasticsearch.yml\n# script.inline: true\n# script.indexed: true  GET /tlo/contacts/_search\n  {\n    \"size\" : 0,\n    \"query\": {\n      \"constant_score\": {\n        \"filter\": {\n          \"terms\": {\n            \"version\": [\n              \"20160301\",\n              \"20160401\"\n            ]\n          }\n        }\n      }\n    },\n    \"aggs\": {\n      \"counts\": {\n        \"cardinality\": {\n          \"script\": \"doc['first_name'].value + ' ' + doc['last_name'].value + ' ' + doc['company'].value\",\n          \"missing\": \"N/A\"\n        }\n      }\n    }\n  }",
            "title": "AGGREGATES"
        },
        {
            "location": "/Search/ElasticSearch/#index-management",
            "text": "By default, indices are assigned five primary shards. The number of primary shards can be set only when an index is created and never changed  # Add an index    PUT /blogs { \"settings\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 1 }}\n  PUT /blogs/_settings { \"number_of_replicas\" : 2}   ElasticSearch Shards should be 50 GB or less in size.  Use aliases to shelter the underlying index (or indices) and allow index swapping",
            "title": "INDEX MANAGEMENT"
        },
        {
            "location": "/Search/ElasticSearch/#cluster-management",
            "text": "GET /_cluster/health",
            "title": "CLUSTER MANAGEMENT"
        },
        {
            "location": "/Search/ElasticSearch/#configuration",
            "text": "config directory   yaml file    Sets the JVM heap size to 0.5 memory size. The OS will use it for file system cache   Prefer not to allocate 30GB !! --> uncompressed pointers  Never let the JVM swap    bootstrap.mlockall = true  Keep the JVM defaults  Do not use G1GC alternative garbage collector   cluster.name: <my cluster>   All nodes in the cluster must have the same cluster name   node.name: <my_node_name>  ./bin/elasticsearch --node.name=`hostname`  to override the configuration file   HTTP port: 9200 and successors  Transport : 9300 (internal communications)",
            "title": "CONFIGURATION"
        },
        {
            "location": "/Search/ElasticSearch/#discovery",
            "text": "AWS plugin available   --> also include integration with S3 (snapshot to S3)  AWS: multi-AZ is OK but replication across far data centers is not recommended  See: resiliency   Sites plugins -- kopf / head / paramedic / bigdesk / kibana\n- contain static web content (JS, HTML....)  Install plugins on ALL machines of the cluster  To install,    ./bin/plugin install marvel-agent\n  ./bin/plugin remove marvel-agent  One type per index is recommended, except for parent child / nested indexes.  index size optimization:\n- can disable  _source  and  _all  (the index that captures every field - not needed unless the top search bar changes)\n- by default, Kibana will search  _all  data types:\nstring, number, bool, datetime, binary, array, object, geo_point, geo_shape, ip, multifield\nbinary should be base64 encoded before storage",
            "title": "Discovery"
        },
        {
            "location": "/Search/ElasticSearch/#maintenance",
            "text": "Steps to restore elastic search data:   Stop elastic search  Extract the zip file (dump file)  Start elastic search  Reload elastic search   The commands to do the above are as below:   systemctl stop elasticsearch  extract gz file to destination path  systemctl start elasticsearch  systemctl daemon-reload elasticsearch",
            "title": "MAINTENANCE"
        },
        {
            "location": "/Software_Development/Development_Tools/",
            "text": "Communication / IM\n\u00b6\n\n\n\n\nSlack\n\n\nTrillian / Pandion\n\n\nSkype, WeChat, Viber, Hangouts\n\n\n\n\nWiki / Knowledge Base\n\u00b6\n\n\n\n\nConfluence\n\n\nEvernote\n\n\n\n\nProject / Bug Tracking\n\u00b6\n\n\n\n\nJIRA\n\n\nBugzilla\n\n\nMantis\n\n\nRedMine\n\n\nTFS\n\n\n\n\nEnterprise Architecture / UML\n\u00b6\n\n\n\n\nViolet UML Editor\n\n\nVisio\n\n\nRational Rose \n\n\n\n\nTerminals / SSH\n\u00b6\n\n\n\n\nPutty\n\n\nMobaXterm\n\n\nmRemoteNG\n\n\nRemote Desktop Connection Manager\n\n\n\n\nEditors/ IDEs\n\u00b6\n\n\n\n\n\n\nComparison of integrated development environments\n\n\n\n\n\n\nNotepad++\n\n\n\n\nPlugins\n\n\n\n\n\n\nGedit\n\n\n\n\nSublime Text\n\n\n\n\n\n\nEclipse\n\n\n\n\nList of Eclipse-based software\n\n\n\n\n\n\nVisual Studio\n\n\nVisual Studio Code\n\n\n\n\nfor Python\n\n\n\n\nPython Tools for Visual Studio\n\n\nPyDev\n\n\nPyCharm\n\n\nAnaconda\n\n\n\n\nSource Control\n\u00b6\n\n\n\n\nSourceTree (Atlassian)\n\n\nGitHub\n\n\nTortoiseGit\n\n\n\n\nCode Quality\n\u00b6\n\n\n\n\nSonarQube\n\n\nPhabricator\n \n\n\nCode Coverage\n\n\n\n\nVirtual Machines and Containers\n\u00b6\n\n\n\n\n\n\nOracle Virtualbox\n\n\n\n\n\n\nDocker\n\n\n\n\nKubernetes\n\n\n\n\nSQL Tools\n\u00b6\n\n\n\n\nMySQL Workbench\n\n\nHeidiSQL\n\n\nAginity Workbench for Redshift (AWS)\n\n\n\n\nMongoDB tools\n\u00b6\n\n\n\n\nRoboMongo\n\n\nMongoChef\n\n\nMongoDB Compass\n\n\n\n\nData Quality\n\u00b6\n\n\n\n\nDQ Analyzer (Attacama)\n\n\n\n\nData Science\n\u00b6\n\n\n\n\nJupyter / IPython\n\n\nRodeo\n\n\nGephi\n\n\n\n\nAWS\n\u00b6\n\n\n\n\nS3 Browser\n\n\nFastGlacier\n\n\n\n\nFile Handling\n\u00b6\n\n\n\n\n7zip\n\n\nFileZilla\n\n\nFolderSize\n\n\n\n\nOther\n\u00b6\n\n\n\n\nCygwin\n\n\nLog Tail\n\n\nDiff - WinMerge",
            "title": "Development Tools"
        },
        {
            "location": "/Software_Development/Development_Tools/#communication-im",
            "text": "Slack  Trillian / Pandion  Skype, WeChat, Viber, Hangouts",
            "title": "Communication / IM"
        },
        {
            "location": "/Software_Development/Development_Tools/#wiki-knowledge-base",
            "text": "Confluence  Evernote",
            "title": "Wiki / Knowledge Base"
        },
        {
            "location": "/Software_Development/Development_Tools/#project-bug-tracking",
            "text": "JIRA  Bugzilla  Mantis  RedMine  TFS",
            "title": "Project / Bug Tracking"
        },
        {
            "location": "/Software_Development/Development_Tools/#enterprise-architecture-uml",
            "text": "Violet UML Editor  Visio  Rational Rose",
            "title": "Enterprise Architecture / UML"
        },
        {
            "location": "/Software_Development/Development_Tools/#terminals-ssh",
            "text": "Putty  MobaXterm  mRemoteNG  Remote Desktop Connection Manager",
            "title": "Terminals / SSH"
        },
        {
            "location": "/Software_Development/Development_Tools/#editors-ides",
            "text": "Comparison of integrated development environments    Notepad++   Plugins    Gedit   Sublime Text    Eclipse   List of Eclipse-based software    Visual Studio  Visual Studio Code   for Python   Python Tools for Visual Studio  PyDev  PyCharm  Anaconda",
            "title": "Editors/ IDEs"
        },
        {
            "location": "/Software_Development/Development_Tools/#source-control",
            "text": "SourceTree (Atlassian)  GitHub  TortoiseGit",
            "title": "Source Control"
        },
        {
            "location": "/Software_Development/Development_Tools/#code-quality",
            "text": "SonarQube  Phabricator    Code Coverage",
            "title": "Code Quality"
        },
        {
            "location": "/Software_Development/Development_Tools/#virtual-machines-and-containers",
            "text": "Oracle Virtualbox    Docker   Kubernetes",
            "title": "Virtual Machines and Containers"
        },
        {
            "location": "/Software_Development/Development_Tools/#sql-tools",
            "text": "MySQL Workbench  HeidiSQL  Aginity Workbench for Redshift (AWS)",
            "title": "SQL Tools"
        },
        {
            "location": "/Software_Development/Development_Tools/#mongodb-tools",
            "text": "RoboMongo  MongoChef  MongoDB Compass",
            "title": "MongoDB tools"
        },
        {
            "location": "/Software_Development/Development_Tools/#data-quality",
            "text": "DQ Analyzer (Attacama)",
            "title": "Data Quality"
        },
        {
            "location": "/Software_Development/Development_Tools/#data-science",
            "text": "Jupyter / IPython  Rodeo  Gephi",
            "title": "Data Science"
        },
        {
            "location": "/Software_Development/Development_Tools/#aws",
            "text": "S3 Browser  FastGlacier",
            "title": "AWS"
        },
        {
            "location": "/Software_Development/Development_Tools/#file-handling",
            "text": "7zip  FileZilla  FolderSize",
            "title": "File Handling"
        },
        {
            "location": "/Software_Development/Development_Tools/#other",
            "text": "Cygwin  Log Tail  Diff - WinMerge",
            "title": "Other"
        },
        {
            "location": "/Software_Development/IntelliJ/",
            "text": "IntelliJ\n\u00b6\n\n\nShortcuts\n\u00b6\n\n\nSearch Anywhere     Double Shift\nGot to file         Ctrl + Shift + N\nRecent files        Ctrl + E\nCode Completion     Ctrl + Space \nParameters          Ctrl + P\nHighlight usages in file            Ctrl + Shift + F7\nDeclaration of the current method   Alt + Q\nCode Templates      Ctrl + J",
            "title": "IntelliJ Cheatsheet"
        },
        {
            "location": "/Software_Development/IntelliJ/#intellij",
            "text": "",
            "title": "IntelliJ"
        },
        {
            "location": "/Software_Development/IntelliJ/#shortcuts",
            "text": "Search Anywhere     Double Shift\nGot to file         Ctrl + Shift + N\nRecent files        Ctrl + E\nCode Completion     Ctrl + Space \nParameters          Ctrl + P\nHighlight usages in file            Ctrl + Shift + F7\nDeclaration of the current method   Alt + Q\nCode Templates      Ctrl + J",
            "title": "Shortcuts"
        },
        {
            "location": "/Web/Bootstrap/",
            "text": "Bootstrap\n\u00b6\n\n\nUseful Links\n\u00b6\n\n\nBootstrap\n\n\nInstall\n\u00b6\n\n\n$ npm install bootstrap\n\n\n\n\n\nCDN\n\u00b6\n\n\n<!-- Latest compiled and minified CSS -->\n<\nlink\n \nrel\n=\n\"stylesheet\"\n \nhref\n=\n\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\"\n \nintegrity\n=\n\"sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7\"\n \ncrossorigin\n=\n\"anonymous\"\n>\n\n\n\n<!-- Optional theme -->\n<\nlink\n \nrel\n=\n\"stylesheet\"\n \nhref\n=\n\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap-theme.min.css\"\n \nintegrity\n=\n\"sha384-fLW2N01lMqjakBkx3l/M9EahuwpSfeNvV63J5ezn3uZzapT0u7EYsXMjQV+0En5r\"\n \ncrossorigin\n=\n\"anonymous\"\n>\n\n\n\n<!-- Latest compiled and minified JavaScript -->\n<\nscript\n \nsrc\n=\n\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js\"\n \nintegrity\n=\n\"sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS\"\n \ncrossorigin\n=\n\"anonymous\"\n></\nscript\n>",
            "title": "Bootstrap"
        },
        {
            "location": "/Web/Bootstrap/#bootstrap",
            "text": "",
            "title": "Bootstrap"
        },
        {
            "location": "/Web/Bootstrap/#useful-links",
            "text": "Bootstrap",
            "title": "Useful Links"
        },
        {
            "location": "/Web/Bootstrap/#install",
            "text": "$ npm install bootstrap",
            "title": "Install"
        },
        {
            "location": "/Web/Bootstrap/#cdn",
            "text": "<!-- Latest compiled and minified CSS --> < link   rel = \"stylesheet\"   href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\"   integrity = \"sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7\"   crossorigin = \"anonymous\" >  <!-- Optional theme --> < link   rel = \"stylesheet\"   href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap-theme.min.css\"   integrity = \"sha384-fLW2N01lMqjakBkx3l/M9EahuwpSfeNvV63J5ezn3uZzapT0u7EYsXMjQV+0En5r\"   crossorigin = \"anonymous\" >  <!-- Latest compiled and minified JavaScript --> < script   src = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js\"   integrity = \"sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS\"   crossorigin = \"anonymous\" ></ script >",
            "title": "CDN"
        },
        {
            "location": "/dotNET/ASPdotNET/",
            "text": "Using Yeoman to generate a ASP.NET Core app from a template\n\u00b6\n\n\nInstall Node.js and npm\n\u00b6\n\n\nTo get started with Yeoman, install \nNode.js\n. The installer includes Node.js and \nnpm\n.\n\n\n\n\nfor Mac OS X\n\n\n\n\nbrew install node\n\n\n\n\n\n\n\nfor Windows OS\n\n\n\n\nchoco install nodejs\n\n\n\n\n\nInstall \nYeoman\n and \nBower\n\u00b6\n\n\nnpm install -g yo\n\nnpm install -g bower\n\n\n\n\n\nInstall \ngenerator-aspnet\n\u00b6\n\n\nnpm install -g generator-aspnet\n\n\n\n\n\nRun with \n\n\nyo aspnet\n\n\n\n\n\nSee also: \nBuilding Projects with Yeoman on docs.asp.net\n\n\nOptionaly install the \nyeoman extension\n in Visual Studio Code\n\u00b6\n\n\nArchitecture\n\u00b6\n\n\nOnion Architecture In ASP.NET Core MVC\n\n\nExample of a Web API built on ASP.NET Core\n\n\nRouting Examples\n\u00b6\n\n\npublic\n \nclass\n \nTestController\n \n:\n \nController\n\n\n{\n\n\n\n// /hello\n\n\n[Route(\"/hello\")]\n\n\npublic\n \nIActionResult\n \nHello\n()\n \n=>\n \nOk\n(\n\"Hello\"\n);\n\n\n\n// /hi only GET method\n\n\n[Route(\"/hi\")]\n \n[\nHttpGet\n]\n\n\npublic\n \nIActionResult\n \nHi\n()\n \n=>\n \nOk\n(\n\"Hi\"\n);\n\n\n\n//Alternative for previous\n\n\n[HttpGet(\"/hi\")]\n\n\npublic\n \nIActionResult\n \nHi\n()\n \n=>\n \nOk\n(\n\"Hi\"\n);\n\n\n}\n\n\n\n//Route prefix\n\n\n[Route(\"test\")]\n\n\npublic\n \nclass\n \nTestController2\n \n:\n \nController\n\n\n{\n\n\n//You can have multiple routes on an action\n\n\n[Route(\"\")]\n \n// /test\n\n\n[Route(\"hello\")]\n  \n// /test/hello\n\n\npublic\n \nIActionResult\n \nHello\n()\n \n=>\n \nOk\n(\n\"Hello\"\n);\n\n\n\n// Maps to both: // /test/hi, and: // /hi\n\n\n[Route(\"/hi\")]\n \n// Overrides the prefix with /, you can also use ~/\n\n\n[Route(\"hi\")]\n\n\npublic\n \nIActionResult\n \nHi\n()\n \n=>\n \nOk\n(\n\"Hi\"\n);\n\n\n\n// /test/greet/Joon -> maps Joon to the name parameter\n\n\n[Route(\"greet/{name}\")]\n\n\npublic\n \nIActionResult\n \nGreet\n(\nstring\n \nname\n)\n \n=>\n \nOk\n(\n$\n\"Hello {name}!\"\n);\n\n\n\n//Parameters can be optional\n\n\n// /test/greetopt -> name == null\n\n\n// /test/greetopt/Joon -> name == Joon\n\n\n[Route(\"greetopt/{name?}\")]\n\n\npublic\n \nIActionResult\n \nGreetOptional\n(\nstring\n \nname\n)\n \n=>\n \nOk\n(\nname\n \n==\n \nnull\n \n?\n \n\"No name\"\n \n:\n \n\"Hi!\"\n);\n\n\n}\n\n\n\n// You can use [controller], [action], and [area] to create generic templates\n\n\n[Route(\"[controller]\n/[\naction\n]\n\")]\n\n\npublic\n \nclass\n \nMyController\n \n:\n \nController\n\n\n{\n\n\n// /my/info\n\n\npublic\n \nIActionResult\n \nInfo\n()\n \n=>\n \nOk\n(\n\"Info\"\n);\n\n\n// /my/i\n\n\n[Route(\"/[controller]\n/\ni\n\")]\n\n\npublic\n \nIActionResult\n \nInfo2\n()\n \n=>\n \nOk\n(\n\"Info2\"\n);\n \n}\n\n\n\n[Route(\"users\")]\n\n\npublic\n \nclass\n \nSelectionController\n \n:\n \nController\n\n\n{\n\n\n//You can use constraints to influence route selection\n\n\n//Do not use for validation!\n\n\n// /users/123\n\n\n[Route(\"{id:int}\")]\n\n\npublic\n \nIActionResult\n \nInt\n(\nint\n \nid\n)\n \n=>\n \nOk\n(\n$\n\"Looked up user id {id}\"\n);\n\n\n\n// /users/joonas\n\n\n[Route(\"{name:alpha}\")]\n\n\npublic\n \nIActionResult\n \nString\n(\nstring\n \nname\n)\n \n=>\n \nOk\n(\n$\n\"User name {name}\"\n);\n \n\n}",
            "title": "ASP.NET Core app"
        },
        {
            "location": "/dotNET/ASPdotNET/#using-yeoman-to-generate-a-aspnet-core-app-from-a-template",
            "text": "",
            "title": "Using Yeoman to generate a ASP.NET Core app from a template"
        },
        {
            "location": "/dotNET/ASPdotNET/#install-nodejs-and-npm",
            "text": "To get started with Yeoman, install  Node.js . The installer includes Node.js and  npm .   for Mac OS X   brew install node   for Windows OS   choco install nodejs",
            "title": "Install Node.js and npm"
        },
        {
            "location": "/dotNET/ASPdotNET/#install-yeoman-and-bower",
            "text": "npm install -g yo\n\nnpm install -g bower",
            "title": "Install Yeoman and Bower"
        },
        {
            "location": "/dotNET/ASPdotNET/#install-generator-aspnet",
            "text": "npm install -g generator-aspnet  Run with   yo aspnet  See also:  Building Projects with Yeoman on docs.asp.net",
            "title": "Install generator-aspnet"
        },
        {
            "location": "/dotNET/ASPdotNET/#optionaly-install-the-yeoman-extension-in-visual-studio-code",
            "text": "",
            "title": "Optionaly install the yeoman extension in Visual Studio Code"
        },
        {
            "location": "/dotNET/ASPdotNET/#architecture",
            "text": "Onion Architecture In ASP.NET Core MVC  Example of a Web API built on ASP.NET Core",
            "title": "Architecture"
        },
        {
            "location": "/dotNET/ASPdotNET/#routing-examples",
            "text": "public   class   TestController   :   Controller  {  // /hello  [Route(\"/hello\")]  public   IActionResult   Hello ()   =>   Ok ( \"Hello\" );  // /hi only GET method  [Route(\"/hi\")]   [ HttpGet ]  public   IActionResult   Hi ()   =>   Ok ( \"Hi\" );  //Alternative for previous  [HttpGet(\"/hi\")]  public   IActionResult   Hi ()   =>   Ok ( \"Hi\" );  }  //Route prefix  [Route(\"test\")]  public   class   TestController2   :   Controller  {  //You can have multiple routes on an action  [Route(\"\")]   // /test  [Route(\"hello\")]    // /test/hello  public   IActionResult   Hello ()   =>   Ok ( \"Hello\" );  // Maps to both: // /test/hi, and: // /hi  [Route(\"/hi\")]   // Overrides the prefix with /, you can also use ~/  [Route(\"hi\")]  public   IActionResult   Hi ()   =>   Ok ( \"Hi\" );  // /test/greet/Joon -> maps Joon to the name parameter  [Route(\"greet/{name}\")]  public   IActionResult   Greet ( string   name )   =>   Ok ( $ \"Hello {name}!\" );  //Parameters can be optional  // /test/greetopt -> name == null  // /test/greetopt/Joon -> name == Joon  [Route(\"greetopt/{name?}\")]  public   IActionResult   GreetOptional ( string   name )   =>   Ok ( name   ==   null   ?   \"No name\"   :   \"Hi!\" );  }  // You can use [controller], [action], and [area] to create generic templates  [Route(\"[controller] /[ action ] \")]  public   class   MyController   :   Controller  {  // /my/info  public   IActionResult   Info ()   =>   Ok ( \"Info\" );  // /my/i  [Route(\"/[controller] / i \")]  public   IActionResult   Info2 ()   =>   Ok ( \"Info2\" );   }  [Route(\"users\")]  public   class   SelectionController   :   Controller  {  //You can use constraints to influence route selection  //Do not use for validation!  // /users/123  [Route(\"{id:int}\")]  public   IActionResult   Int ( int   id )   =>   Ok ( $ \"Looked up user id {id}\" );  // /users/joonas  [Route(\"{name:alpha}\")]  public   IActionResult   String ( string   name )   =>   Ok ( $ \"User name {name}\" );   }",
            "title": "Routing Examples"
        },
        {
            "location": "/dotNET/AkkadotNET/",
            "text": "Akka.NET Examples\n\u00b6\n\n\n// To install Akka.NET Distributed Actor Framework, run the following command in the Package Manager Console\n\n\n// PM> Install-Package Akka\n\n\n// PM> Install-Package Akka.Remote\n\n\n// Installing with Topshelf is as easy as calling \n.exe install on the command line.\n\n\nusing\n \nSystem\n;\n\n\nusing\n \nSystem.Collections.Generic\n;\n\n\nusing\n \nSystem.Linq\n;\n\n\nusing\n \nSystem.Text\n;\n\n\nusing\n \nSystem.Threading.Tasks\n;\n\n\n\n// Add these two lines\n\n\nusing\n \nAkka\n;\n\n\nusing\n \nAkka.Actor\n;\n\n\nusing\n \nTopshelf\n;\n   \n// http://topshelf.readthedocs.io/en/latest/configuration/quickstart.html\n\n\n\ninternal\n \nclass\n \nProgram\n\n\n{\n\n    \nprivate\n \nstatic\n \nvoid\n \nMain\n(\nstring\n[]\n \nargs\n)\n\n    \n{\n\n        \n// \u2018x\u2019 exposes all of the host level configuration\n\n        \nHostFactory\n.\nRun\n(\nx\n \n=>\n\n        \n{\n\n            \nx\n.\nService\n<\nActorService\n>(\ns\n \n=>\n     \n//  telling Topshelf that there is a service of type ActorService. service configuration options exposed through the \u2018s\u2019 parameter.\n\n            \n{\n\n                \ns\n.\nConstructUsing\n(\nname\n \n=>\n \nnew\n \nActorService\n());\n \n// build an instance of the service; new or pull from IoC container\n\n                \ns\n.\nWhenStarted\n(\nservice\n \n=>\n \nservice\n.\nStart\n());\n\n                \ns\n.\nWhenStopped\n(\nservice\n \n=>\n \nservice\n.\nStop\n());\n\n                \n////continue and restart directives are also available\n\n                \n//s.WhenPaused(service => service.Pause());\n\n                \n//s.WhenContinued(service => service.Continue());\n\n                \n//s.WhenShutdown(service => service.Shutdown());\n\n\n            \n});\n\n\n            \nx\n.\nRunAsLocalSystem\n();\n \n// service \u2018run as\u2019 the \u2018local system\u2019. Alternatively x.RunAsLocalSystem(); x.RunAs(\"username\", \"password\");  x.RunAsPrompt();\n\n            \nx\n.\nUseAssemblyInfoForServiceInfo\n();\n\n\n            \n//x.SetDescription(\"Orchestrator Host\"); \n\n            \n//x.SetDisplayName(\"Orchestrator\"); // display name for the winservice in the windows service monitor\n\n            \n//x.SetServiceName(\"Orchestrator\"); // service name for the winservice in the windows service monitor\n\n            \n//x.SetInstanceName(\"MyService\"); // instance name of the service, which is combined with the base service name and separated by a $.\n\n        \n});\n\n    \n}\n\n\n}\n\n\n\n// <summary>\n\n\n/// This class acts as an interface between the application and TopShelf\n\n\n/// </summary>\n\n\npublic\n \nclass\n \nActorService\n\n\n{\n\n    \nprivate\n \nActorSystem\n \nsystem\n;\n\n\n    \npublic\n \nvoid\n \nStart\n()\n\n    \n{\n\n        \n// Create a new actor system (a container for actors)\n\n        \nthis\n.\nsystem\n \n=\n \nActorSystem\n.\nCreate\n(\n\"MainSystem\"\n);\n\n    \n}\n\n\n    \npublic\n \nasync\n \nvoid\n \nStop\n()\n\n    \n{\n\n        \n//this is where you stop your actor system\n\n        \nawait\n \nthis\n.\nsystem\n.\nTerminate\n();\n\n    \n}\n\n\n    \nprivate\n \nvoid\n \nCreate\n()\n\n    \n{\n\n        \n// Create your actor and get a reference to it.\n\n        \n// This will be an \"ActorRef\", which is not a\n\n        \n// reference to the actual actor instance\n\n        \n// but rather a client or proxy to it.\n\n        \nvar\n \njob\n \n=\n \nsystem\n.\nActorOf\n<\nJobActor\n>(\n\"Job\"\n);\n\n        \n//or: var myActor = system.ActorOf(Props.Create<JobActor>());\n\n\n        \n// Send a message to the actor\n\n        \njob\n.\nTell\n(\nnew\n \nMessage\n<\nstring\n>(\n\"Hello World\"\n));\n\n    \n}\n\n\n}\n\n\n\n// Example immutable message class - C# 7.0\n\n\npublic\n \nclass\n \nMessage\n<\nT\n>\n\n\n{\n\n    \npublic\n \nMessage\n(\nT\n \ndata\n)\n\n    \n{\n\n        \nthis\n.\nData\n \n=\n \ndata\n;\n\n    \n}\n\n\n    \npublic\n \nT\n \nData\n \n{\n \nget\n;\n \n}\n\n\n    \n// Allow convesion to a tuple \n\n    \npublic\n \nvoid\n \nDeconstruct\n(\nout\n \nT\n \ndata\n)\n \n{\n \ndata\n \n=\n \nthis\n.\nData\n;\n \n}\n\n\n}\n\n\n\n// Example of ReceiveActor\n\n\npublic\n \nclass\n \nJobActor\n:\n \nReceiveActor\n\n\n{\n\n  \nprivate\n \nreadonly\n \nILoggingAdapter\n \nlog\n \n=\n \nContext\n.\nGetLogger\n();\n\n\n  \npublic\n \nJobActor\n()\n\n  \n{\n\n    \nReceive\n<\nMessage\n<\nstring\n>>(\nmessage\n \n=>\n \n{\n\n      \nlog\n.\nInfo\n(\n\"Received String message: {0}\"\n,\n \nmessage\n.\nData\n);\n\n      \n// Console.WriteLine(message.Data);\n\n\n      \n// reply back\n\n      \nSender\n.\nTell\n(\nmessage\n);\n\n    \n});\n\n  \n}\n\n\n}\n\n\n\n\n\n// Example of untyped actor\n\n\n\npublic\n \nclass\n \nMyActor\n \n:\n \nUntypedActor\n\n\n{\n\n    \nprivate\n \nActorRef\n \nlogger\n \n=\n \nContext\n.\nActorOf\n<\nLogActor\n>();\n\n\n    \n// if any child, e.g. the logger above. throws an exception\n\n    \n// apply the rules below\n\n    \n// e.g. Restart the child, if 10 exceptions occur in 30 seconds or\n\n    \n// less, then stop the actor\n\n    \nprotected\n \noverride\n \nSupervisorStrategy\n \nSupervisorStrategy\n()\n\n    \n{\n\n        \nreturn\n \nnew\n \nOneForOneStrategy\n(\n \n//or AllForOneStrategy\n\n            \nmaxNumberOfRetries\n:\n \n10\n,\n\n            \nduration\n:\n \nTimeSpan\n.\nFromSeconds\n(\n30\n),\n\n            \ndecider\n:\n \nDecider\n.\nFrom\n(\nx\n \n=>\n\n            \n{\n\n                \n//Maybe we consider ArithmeticException to not be application critical\n\n                \n//so we just ignore the error and keep going.\n\n                \nif\n \n(\nx\n \nis\n \nArithmeticException\n)\n \nreturn\n \nDirective\n.\nResume\n;\n\n                \n//Error that we cannot recover from, stop the failing actor\n\n                \nelse\n \nif\n \n(\nx\n \nis\n \nNotSupportedException\n)\n \nreturn\n \nDirective\n.\nStop\n;\n\n                \n//In all other cases, just restart the failing actor\n\n                \nelse\n \nreturn\n \nDirective\n.\nRestart\n;\n\n             \n}));\n\n    \n}\n\n\n}\n\n\n\n// Example of long-running operation in an Actor - PipeTo / Become / Stash\n\n\n\n/*\n\n\nIf you stick a long-running operation inside your Receive method then your actors will be unable to process any messages, including system messages, until that operation finishes. And if it\u2019s possible that the operation will never finish, it\u2019s possible to deadlock your actor.\n\n\nThe solution to this is simple: you need to encapsulate any long-running I/O-bound or CPU-bound operations inside a Task and make it possible to cancel that task from within the actor.\n\n\nHere\u2019s an example of how you can use behavior switching, stashing, and control messages to do this.\n\n\n\nhttps://petabridge.com/blog/akka-actors-finite-state-machines-switchable-behavior/\n\n\n\n*/\n\n\npublic\n \nclass\n \nFooActor\n \n:\n \nReceiveActor\n,\n \nIWithUnboundedStash\n\n\n{\n\n\n    \nprivate\n \nTask\n \n_runningTask\n;\n\n    \nprivate\n \nCancellationTokenSource\n \n_cancel\n;\n\n\n    \npublic\n \nIStash\n \nStash\n \n{\nget\n;\n \nset\n;}\n\n\n    \npublic\n \nFooActor\n(){\n\n        \n_cancel\n \n=\n \nnew\n \nCancellationTokenSource\n();\n\n        \nReady\n();\n\n    \n}\n\n\n    \nprivate\n \nvoid\n \nReady\n(){\n\n        \nReceive\n<\nStart\n>(\ns\n \n=>\n \n{\n\n            \nvar\n \nself\n \n=\n \nSelf\n;\n \n// closure\n\n            \n_runningTask\n \n=\n \nTask\n.\nRun\n(()\n \n=>\n \n{\n\n                \n// ... work\n\n            \n},\n \n_cancel\n.\nToken\n).\nContinueWith\n(\nx\n \n=>\n\n            \n{\n\n                \nif\n(\nx\n.\nIsCancelled\n \n||\n \nx\n.\nIsFaulted\n)\n\n                    \nreturn\n \nnew\n \nFailed\n();\n\n                \nreturn\n \nnew\n \nFinished\n();\n\n            \n},\n \nTaskContinuationOptions\n.\nExecuteSynchronously\n)\n\n            \n.\nPipeTo\n(\nself\n);\n\n\n            \n// switch behavior\n\n            \nBecome\n(\nWorking\n);\n\n        \n})\n\n    \n}\n\n\n    \nprivate\n \nvoid\n \nWorking\n(){\n\n        \nReceive\n<\nCancel\n>(\ncancel\n \n=>\n \n{\n\n            \n_cancel\n.\nCancel\n();\n \n// cancel work\n\n            \nBecomeReady\n();\n\n        \n});\n\n        \nReceive\n<\nFailed\n>(\nf\n \n=>\n \nBecomeReady\n());\n\n        \nReceive\n<\nFinished\n>(\nf\n \n=>\n \nBecomeReady\n());\n\n        \nReceiveAny\n(\no\n \n=>\n \nStash\n.\nStash\n());\n\n    \n}\n\n\n    \nprivate\n \nvoid\n \nBecomeReady\n(){\n\n        \n_cancel\n \n=\n \nnew\n \nCancellationTokenSource\n();\n\n        \nStash\n.\nUnstashAll\n();\n\n        \nBecome\n(\nReady\n);\n\n    \n}\n\n\n}",
            "title": "Akka.NET Examples"
        },
        {
            "location": "/dotNET/AkkadotNET/#akkanet-examples",
            "text": "// To install Akka.NET Distributed Actor Framework, run the following command in the Package Manager Console  // PM> Install-Package Akka  // PM> Install-Package Akka.Remote  // Installing with Topshelf is as easy as calling  .exe install on the command line.  using   System ;  using   System.Collections.Generic ;  using   System.Linq ;  using   System.Text ;  using   System.Threading.Tasks ;  // Add these two lines  using   Akka ;  using   Akka.Actor ;  using   Topshelf ;     // http://topshelf.readthedocs.io/en/latest/configuration/quickstart.html  internal   class   Program  { \n     private   static   void   Main ( string []   args ) \n     { \n         // \u2018x\u2019 exposes all of the host level configuration \n         HostFactory . Run ( x   => \n         { \n             x . Service < ActorService >( s   =>       //  telling Topshelf that there is a service of type ActorService. service configuration options exposed through the \u2018s\u2019 parameter. \n             { \n                 s . ConstructUsing ( name   =>   new   ActorService ());   // build an instance of the service; new or pull from IoC container \n                 s . WhenStarted ( service   =>   service . Start ()); \n                 s . WhenStopped ( service   =>   service . Stop ()); \n                 ////continue and restart directives are also available \n                 //s.WhenPaused(service => service.Pause()); \n                 //s.WhenContinued(service => service.Continue()); \n                 //s.WhenShutdown(service => service.Shutdown()); \n\n             }); \n\n             x . RunAsLocalSystem ();   // service \u2018run as\u2019 the \u2018local system\u2019. Alternatively x.RunAsLocalSystem(); x.RunAs(\"username\", \"password\");  x.RunAsPrompt(); \n             x . UseAssemblyInfoForServiceInfo (); \n\n             //x.SetDescription(\"Orchestrator Host\");  \n             //x.SetDisplayName(\"Orchestrator\"); // display name for the winservice in the windows service monitor \n             //x.SetServiceName(\"Orchestrator\"); // service name for the winservice in the windows service monitor \n             //x.SetInstanceName(\"MyService\"); // instance name of the service, which is combined with the base service name and separated by a $. \n         }); \n     }  }  // <summary>  /// This class acts as an interface between the application and TopShelf  /// </summary>  public   class   ActorService  { \n     private   ActorSystem   system ; \n\n     public   void   Start () \n     { \n         // Create a new actor system (a container for actors) \n         this . system   =   ActorSystem . Create ( \"MainSystem\" ); \n     } \n\n     public   async   void   Stop () \n     { \n         //this is where you stop your actor system \n         await   this . system . Terminate (); \n     } \n\n     private   void   Create () \n     { \n         // Create your actor and get a reference to it. \n         // This will be an \"ActorRef\", which is not a \n         // reference to the actual actor instance \n         // but rather a client or proxy to it. \n         var   job   =   system . ActorOf < JobActor >( \"Job\" ); \n         //or: var myActor = system.ActorOf(Props.Create<JobActor>()); \n\n         // Send a message to the actor \n         job . Tell ( new   Message < string >( \"Hello World\" )); \n     }  }  // Example immutable message class - C# 7.0  public   class   Message < T >  { \n     public   Message ( T   data ) \n     { \n         this . Data   =   data ; \n     } \n\n     public   T   Data   {   get ;   } \n\n     // Allow convesion to a tuple  \n     public   void   Deconstruct ( out   T   data )   {   data   =   this . Data ;   }  }  // Example of ReceiveActor  public   class   JobActor :   ReceiveActor  { \n   private   readonly   ILoggingAdapter   log   =   Context . GetLogger (); \n\n   public   JobActor () \n   { \n     Receive < Message < string >>( message   =>   { \n       log . Info ( \"Received String message: {0}\" ,   message . Data ); \n       // Console.WriteLine(message.Data); \n\n       // reply back \n       Sender . Tell ( message ); \n     }); \n   }  }  // Example of untyped actor  public   class   MyActor   :   UntypedActor  { \n     private   ActorRef   logger   =   Context . ActorOf < LogActor >(); \n\n     // if any child, e.g. the logger above. throws an exception \n     // apply the rules below \n     // e.g. Restart the child, if 10 exceptions occur in 30 seconds or \n     // less, then stop the actor \n     protected   override   SupervisorStrategy   SupervisorStrategy () \n     { \n         return   new   OneForOneStrategy (   //or AllForOneStrategy \n             maxNumberOfRetries :   10 , \n             duration :   TimeSpan . FromSeconds ( 30 ), \n             decider :   Decider . From ( x   => \n             { \n                 //Maybe we consider ArithmeticException to not be application critical \n                 //so we just ignore the error and keep going. \n                 if   ( x   is   ArithmeticException )   return   Directive . Resume ; \n                 //Error that we cannot recover from, stop the failing actor \n                 else   if   ( x   is   NotSupportedException )   return   Directive . Stop ; \n                 //In all other cases, just restart the failing actor \n                 else   return   Directive . Restart ; \n              })); \n     }  }  // Example of long-running operation in an Actor - PipeTo / Become / Stash  /*  If you stick a long-running operation inside your Receive method then your actors will be unable to process any messages, including system messages, until that operation finishes. And if it\u2019s possible that the operation will never finish, it\u2019s possible to deadlock your actor.  The solution to this is simple: you need to encapsulate any long-running I/O-bound or CPU-bound operations inside a Task and make it possible to cancel that task from within the actor.  Here\u2019s an example of how you can use behavior switching, stashing, and control messages to do this.  https://petabridge.com/blog/akka-actors-finite-state-machines-switchable-behavior/  */  public   class   FooActor   :   ReceiveActor ,   IWithUnboundedStash  { \n\n     private   Task   _runningTask ; \n     private   CancellationTokenSource   _cancel ; \n\n     public   IStash   Stash   { get ;   set ;} \n\n     public   FooActor (){ \n         _cancel   =   new   CancellationTokenSource (); \n         Ready (); \n     } \n\n     private   void   Ready (){ \n         Receive < Start >( s   =>   { \n             var   self   =   Self ;   // closure \n             _runningTask   =   Task . Run (()   =>   { \n                 // ... work \n             },   _cancel . Token ). ContinueWith ( x   => \n             { \n                 if ( x . IsCancelled   ||   x . IsFaulted ) \n                     return   new   Failed (); \n                 return   new   Finished (); \n             },   TaskContinuationOptions . ExecuteSynchronously ) \n             . PipeTo ( self ); \n\n             // switch behavior \n             Become ( Working ); \n         }) \n     } \n\n     private   void   Working (){ \n         Receive < Cancel >( cancel   =>   { \n             _cancel . Cancel ();   // cancel work \n             BecomeReady (); \n         }); \n         Receive < Failed >( f   =>   BecomeReady ()); \n         Receive < Finished >( f   =>   BecomeReady ()); \n         ReceiveAny ( o   =>   Stash . Stash ()); \n     } \n\n     private   void   BecomeReady (){ \n         _cancel   =   new   CancellationTokenSource (); \n         Stash . UnstashAll (); \n         Become ( Ready ); \n     }  }",
            "title": "Akka.NET Examples"
        },
        {
            "location": "/dotNET/C#/",
            "text": "C# Cheatsheets\n\u00b6\n\n\nQuick Reference\n\n\nCheatsheet\n\n\nC# 6.0 / 7.0 - what is new\n\u00b6\n\n\nReadonly properties\n\u00b6\n\n\npublic\n \nstring\n \nFirstName\n \n{\n \nget\n;\n \nprivate\n \nset\n;\n \n}\n  \n// private set is accessible from the entire class\n\n\n\npublic\n \nstring\n \nLastName\n \n{\n \nget\n;\n \n}\n \n// accessible only in constructor\n\n\n\npublic\n \nICollection\n<\ndouble\n>\n \nGrades\n \n{\n \nget\n;\n \n}\n \n=\n \nnew\n \nList\n<\ndouble\n>();\n \n// property initializer\n\n\n\n\n\n\nExpression-bodied function members\n\u00b6\n\n\npublic\n \noverride\n \nstring\n \nToString\n()\n \n=>\n \n\"Hi!\"\n;\n\n\n\n\n\n\nUsing static\n\u00b6\n\n\nusing\n \nstatic\n \nSystem\n.\nString\n;\n\n\n// also common: \n\n\n// using static System.Math;\n\n\n// using static System.Linq.Enumerable;\n\n\n\nif\n \n(\nIsNullOrWhiteSpace\n(\nlastName\n))\n\n  \nthrow\n \nnew\n \nArgumentException\n(\nmessage\n:\n \n\"Cannot be blank\"\n,\n \nparamName\n:\n \nnameof\n(\nlastName\n));\n\n\n\n\n\n\nNull checking\n\u00b6\n\n\nvar\n \nfirst\n \n=\n \nperson\n?.\nFirstName\n;\n\n\nfirst\n \n=\n \nperson\n?.\nFirstName\n \n??\n \n\"Unspecified\"\n;\n\n\n\n// preferred event handing in C# 6:\n\n\nthis\n.\nSomethingHappened\n?.\nInvoke\n(\nthis\n,\n \neventArgs\n);\n\n\n\n\n\n\nString interpolation\n\u00b6\n\n\npublic\n \nstring\n \nGetFormattedGradePoint\n()\n \n=>\n \n$\n\"Name: {LastName}, {FirstName}. G.P.A: {Grades.Average():F2}\"\n;\n\n\n\n\n\n\nException Filters\n\u00b6\n\n\npublic\n \nstatic\n \nasync\n \nTask\n<\nstring\n>\n \nMakeRequest\n()\n\n\n{\n\n  \nvar\n \nclient\n \n=\n \nnew\n \nSystem\n.\nNet\n.\nHttp\n.\nHttpClient\n();\n\n  \nvar\n \nstreamTask\n \n=\n \nclient\n.\nGetStringAsync\n(\n\"https://localHost:10000\"\n);\n\n  \ntry\n \n  \n{\n\n    \nvar\n \nresponseText\n \n=\n \nawait\n \nstreamTask\n;\n\n    \nreturn\n \nresponseText\n;\n\n  \n}\n \n  \ncatch\n \n(\nSystem\n.\nNet\n.\nHttp\n.\nHttpRequestException\n \ne\n)\n \nwhen\n \n(\ne\n.\nMessage\n.\nContains\n(\n\"301\"\n))\n\n  \n{\n\n  \nreturn\n \n\"Site Moved\"\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\nList and dict initializers\n\u00b6\n\n\nprivate\n \nList\n<\nstring\n>\n \nmessages\n \n=\n \nnew\n \nList\n<\nstring\n>\n\n\n{\n\n  \n\"Page not Found\"\n,\n\n  \n\"Page moved, but left a forwarding address.\"\n,\n\n  \n\"The web server can't come out to play today.\"\n\n\n};\n\n\n\nprivate\n \nDictionary\n<\nint\n,\n \nstring\n>\n \nwebErrors\n \n=\n \nnew\n \nDictionary\n<\nint\n,\n \nstring\n>\n\n\n{\n\n\n  [404]\n \n=\n \n\"Page not Found\"\n,\n\n\n  [302]\n \n=\n \n\"Page moved, but left a forwarding address.\"\n,\n\n\n  [500]\n \n=\n \n\"The web server can't come out to play today.\"\n\n\n};\n\n\n\n\n\n\nOut variables\n\u00b6\n\n\nif\n \n(\nint\n.\nTryParse\n(\ninput\n,\n \nout\n \nint\n \nresult\n))\n\n    \nWriteLine\n(\nresult\n);\n\n\nelse\n\n    \nWriteLine\n(\n\"Could not parse input\"\n);\n\n\n\n\n\n\nTuples\n\u00b6\n\n\nvar\n \nletters\n \n=\n \n(\n\"a\"\n,\n \n\"b\"\n);\n\n\n\n(\nstring\n \nAlpha\n,\n \nstring\n \nBeta\n)\n \nnamedLetters\n \n=\n \n(\n\"a\"\n,\n \n\"b\"\n);\n\n\nvar\n \nalphabetStart\n \n=\n \n(\nAlpha\n:\n \n\"a\"\n,\n \nBeta\n:\n \n\"b\"\n);\n\n\n\npublic\n \nclass\n \nPoint\n\n\n{\n\n  \npublic\n \nPoint\n(\ndouble\n \nx\n,\n \ndouble\n \ny\n)\n\n  \n{\n\n  \nthis\n.\nX\n \n=\n \nx\n;\n\n  \nthis\n.\nY\n \n=\n \ny\n;\n\n  \n}\n\n\n  \npublic\n \ndouble\n \nX\n \n{\n \nget\n;\n \n}\n\n  \npublic\n \ndouble\n \nY\n \n{\n \nget\n;\n \n}\n\n\n  \n// Deconstruct method\n\n  \npublic\n \nvoid\n \nDeconstruct\n(\nout\n \ndouble\n \nx\n,\n \nout\n \ndouble\n \ny\n)\n\n  \n{\n\n  \nx\n \n=\n \nthis\n.\nX\n;\n\n  \ny\n \n=\n \nthis\n.\nY\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\nRef return values\n\u00b6\n\n\npublic\n \nstatic\n \nref\n \nint\n \nFind3\n(\nint\n[,]\n \nmatrix\n,\n \nFunc\n<\nint\n,\n \nbool\n>\n \npredicate\n)\n\n\n{\n\n  \nfor\n \n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n<\n \nmatrix\n.\nGetLength\n(\n0\n);\n \ni\n++)\n\n             \nfor\n \n(\nint\n \nj\n \n=\n \n0\n;\n \nj\n \n<\n \nmatrix\n.\nGetLength\n(\n1\n);\n \nj\n++)\n\n                      \nif\n \n(\npredicate\n(\nmatrix\n[\ni\n,\n \nj\n]))\n\n                          \nreturn\n \nref\n \nmatrix\n[\ni\n,\n \nj\n];\n\n  \nthrow\n \nnew\n \nInvalidOperationException\n(\n\"Not found\"\n);\n\n\n}\n\n\nref\n \nvar\n \nitem\n \n=\n \nref\n \nMatrixSearch\n.\nFind3\n(\nmatrix\n,\n \n(\nval\n)\n \n=>\n \nval\n \n==\n \n42\n);\n\n\nConsole\n.\nWriteLine\n(\nitem\n);\n\n\nitem\n \n=\n \n24\n;\n\n\nConsole\n.\nWriteLine\n(\nmatrix\n[\n4\n,\n \n2\n]);\n\n\n\n\n\n\nLocal functions\n\u00b6\n\n\npublic\n \nstatic\n \nIEnumerable\n<\nchar\n>\n \nAlphabetSubset3\n(\nchar\n \nstart\n,\n \nchar\n \nend\n)\n\n\n{\n\n    \nif\n \n((\nstart\n \n<\n \n'a'\n)\n \n||\n \n(\nstart\n \n>\n \n'z'\n))\n\n        \nthrow\n \nnew\n \nArgumentOutOfRangeException\n(\nparamName\n:\n \nnameof\n(\nstart\n),\n \nmessage\n:\n \n\"start must be a letter\"\n);\n\n    \nif\n \n((\nend\n \n<\n \n'a'\n)\n \n||\n \n(\nend\n \n>\n \n'z'\n))\n\n        \nthrow\n \nnew\n \nArgumentOutOfRangeException\n(\nparamName\n:\n \nnameof\n(\nend\n),\n \nmessage\n:\n \n\"end must be a letter\"\n);\n\n    \nif\n \n(\nend\n \n<=\n \nstart\n)\n\n        \nthrow\n \nnew\n \nArgumentException\n(\n$\n\"{nameof(end)} must be greater than {nameof(start)}\"\n);\n\n    \nreturn\n \nalphabetSubsetImplementation\n();\n\n\n    \nIEnumerable\n<\nchar\n>\n \nalphabetSubsetImplementation\n()\n\n    \n{\n\n        \nfor\n \n(\nvar\n \nc\n \n=\n \nstart\n;\n \nc\n \n<\n \nend\n;\n \nc\n++)\n\n            \nyield\n \nreturn\n \nc\n;\n\n    \n}\n\n\n}",
            "title": "C# Cheatsheet"
        },
        {
            "location": "/dotNET/C#/#c-cheatsheets",
            "text": "Quick Reference  Cheatsheet",
            "title": "C# Cheatsheets"
        },
        {
            "location": "/dotNET/C#/#c-60-70-what-is-new",
            "text": "",
            "title": "C# 6.0 / 7.0 - what is new"
        },
        {
            "location": "/dotNET/C#/#readonly-properties",
            "text": "public   string   FirstName   {   get ;   private   set ;   }    // private set is accessible from the entire class  public   string   LastName   {   get ;   }   // accessible only in constructor  public   ICollection < double >   Grades   {   get ;   }   =   new   List < double >();   // property initializer",
            "title": "Readonly properties"
        },
        {
            "location": "/dotNET/C#/#expression-bodied-function-members",
            "text": "public   override   string   ToString ()   =>   \"Hi!\" ;",
            "title": "Expression-bodied function members"
        },
        {
            "location": "/dotNET/C#/#using-static",
            "text": "using   static   System . String ;  // also common:   // using static System.Math;  // using static System.Linq.Enumerable;  if   ( IsNullOrWhiteSpace ( lastName )) \n   throw   new   ArgumentException ( message :   \"Cannot be blank\" ,   paramName :   nameof ( lastName ));",
            "title": "Using static"
        },
        {
            "location": "/dotNET/C#/#null-checking",
            "text": "var   first   =   person ?. FirstName ;  first   =   person ?. FirstName   ??   \"Unspecified\" ;  // preferred event handing in C# 6:  this . SomethingHappened ?. Invoke ( this ,   eventArgs );",
            "title": "Null checking"
        },
        {
            "location": "/dotNET/C#/#string-interpolation",
            "text": "public   string   GetFormattedGradePoint ()   =>   $ \"Name: {LastName}, {FirstName}. G.P.A: {Grades.Average():F2}\" ;",
            "title": "String interpolation"
        },
        {
            "location": "/dotNET/C#/#exception-filters",
            "text": "public   static   async   Task < string >   MakeRequest ()  { \n   var   client   =   new   System . Net . Http . HttpClient (); \n   var   streamTask   =   client . GetStringAsync ( \"https://localHost:10000\" ); \n   try  \n   { \n     var   responseText   =   await   streamTask ; \n     return   responseText ; \n   }  \n   catch   ( System . Net . Http . HttpRequestException   e )   when   ( e . Message . Contains ( \"301\" )) \n   { \n   return   \"Site Moved\" ; \n   }  }",
            "title": "Exception Filters"
        },
        {
            "location": "/dotNET/C#/#list-and-dict-initializers",
            "text": "private   List < string >   messages   =   new   List < string >  { \n   \"Page not Found\" , \n   \"Page moved, but left a forwarding address.\" , \n   \"The web server can't come out to play today.\"  };  private   Dictionary < int ,   string >   webErrors   =   new   Dictionary < int ,   string >  {    [404]   =   \"Page not Found\" ,    [302]   =   \"Page moved, but left a forwarding address.\" ,    [500]   =   \"The web server can't come out to play today.\"  };",
            "title": "List and dict initializers"
        },
        {
            "location": "/dotNET/C#/#out-variables",
            "text": "if   ( int . TryParse ( input ,   out   int   result )) \n     WriteLine ( result );  else \n     WriteLine ( \"Could not parse input\" );",
            "title": "Out variables"
        },
        {
            "location": "/dotNET/C#/#tuples",
            "text": "var   letters   =   ( \"a\" ,   \"b\" );  ( string   Alpha ,   string   Beta )   namedLetters   =   ( \"a\" ,   \"b\" );  var   alphabetStart   =   ( Alpha :   \"a\" ,   Beta :   \"b\" );  public   class   Point  { \n   public   Point ( double   x ,   double   y ) \n   { \n   this . X   =   x ; \n   this . Y   =   y ; \n   } \n\n   public   double   X   {   get ;   } \n   public   double   Y   {   get ;   } \n\n   // Deconstruct method \n   public   void   Deconstruct ( out   double   x ,   out   double   y ) \n   { \n   x   =   this . X ; \n   y   =   this . Y ; \n   }  }",
            "title": "Tuples"
        },
        {
            "location": "/dotNET/C#/#ref-return-values",
            "text": "public   static   ref   int   Find3 ( int [,]   matrix ,   Func < int ,   bool >   predicate )  { \n   for   ( int   i   =   0 ;   i   <   matrix . GetLength ( 0 );   i ++) \n              for   ( int   j   =   0 ;   j   <   matrix . GetLength ( 1 );   j ++) \n                       if   ( predicate ( matrix [ i ,   j ])) \n                           return   ref   matrix [ i ,   j ]; \n   throw   new   InvalidOperationException ( \"Not found\" );  }  ref   var   item   =   ref   MatrixSearch . Find3 ( matrix ,   ( val )   =>   val   ==   42 );  Console . WriteLine ( item );  item   =   24 ;  Console . WriteLine ( matrix [ 4 ,   2 ]);",
            "title": "Ref return values"
        },
        {
            "location": "/dotNET/C#/#local-functions",
            "text": "public   static   IEnumerable < char >   AlphabetSubset3 ( char   start ,   char   end )  { \n     if   (( start   <   'a' )   ||   ( start   >   'z' )) \n         throw   new   ArgumentOutOfRangeException ( paramName :   nameof ( start ),   message :   \"start must be a letter\" ); \n     if   (( end   <   'a' )   ||   ( end   >   'z' )) \n         throw   new   ArgumentOutOfRangeException ( paramName :   nameof ( end ),   message :   \"end must be a letter\" ); \n     if   ( end   <=   start ) \n         throw   new   ArgumentException ( $ \"{nameof(end)} must be greater than {nameof(start)}\" ); \n     return   alphabetSubsetImplementation (); \n\n     IEnumerable < char >   alphabetSubsetImplementation () \n     { \n         for   ( var   c   =   start ;   c   <   end ;   c ++) \n             yield   return   c ; \n     }  }",
            "title": "Local functions"
        },
        {
            "location": "/dotNET/Multithreading/",
            "text": "Advanced .NET Threading\n\u00b6\n\n\nAdvanced .NET Threading, Part 1: Thread Fundamentals\n\n\nAdvanced .NET Threading, Part 2: Compute-Bound Async Operations\n\n\nAdvanced .NET Threading, Part 3: I/O-Bound Async Operations\n\n\nAdvanced .NET Threading, Part 4: Thread Synchronization Primitives\n\n\nAdvanced .NET Threading, Part 5: Thread Synchronization Locks",
            "title": ".NET Multithreading"
        },
        {
            "location": "/dotNET/Multithreading/#advanced-net-threading",
            "text": "Advanced .NET Threading, Part 1: Thread Fundamentals  Advanced .NET Threading, Part 2: Compute-Bound Async Operations  Advanced .NET Threading, Part 3: I/O-Bound Async Operations  Advanced .NET Threading, Part 4: Thread Synchronization Primitives  Advanced .NET Threading, Part 5: Thread Synchronization Locks",
            "title": "Advanced .NET Threading"
        },
        {
            "location": "/dotNET/WPF/",
            "text": "Useful Links\n\u00b6\n\n\nWPF tutorial\n\n\nWPF Documentation\n\n\nWPF Samples\n\n\nWPF Tools\n\n\nApplication\n\u00b6\n\n\n<Application\n \nx:Class=\n\"ExpenseIt.App\"\n\n     \nxmlns=\n\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"\n\n     \nxmlns:x=\n\"http://schemas.microsoft.com/winfx/2006/xaml\"\n\n     \nStartupUri=\n\"MainWindow.xaml\"\n>\n\n    \n<Application.Resources>\n\n    \n</Application.Resources>\n\n\n</Application>\n\n\n\n\n\n\nCommands\n\u00b6\n\n\nWPF Commands\n\n\nWPF provides a set of predefined commands. The command library consists of the following classes: ApplicationCommands, NavigationCommands, MediaCommands, EditingCommands, and the ComponentCommands.\n\n\n<StackPanel>\n\n\n\n<StackPanel.ContextMenu>\n\n    \n<ContextMenu>\n\n      \n<MenuItem\n \nCommand=\n\"ApplicationCommands.Properties\"\n \n/>\n\n    \n</ContextMenu>\n\n\n</StackPanel.ContextMenu>\n\n  \n<Menu>\n\n    \n<MenuItem\n \nCommand=\n\"ApplicationCommands.Paste\"\n \n/>\n\n  \n</Menu>\n\n  \n<TextBox\n \n/></StackPanel>\n\n\n\n<Window.InputBindings>\n\n  \n<KeyBinding\n \nKey=\n\"B\"\n\n              \nModifiers=\n\"Control\"\n \n              \nCommand=\n\"ApplicationCommands.Open\"\n \n/></Window.InputBindings>\n\n\n\n\n\n\nPage\n\u00b6\n\n\n<Page\n\n  \nxmlns=\n\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"\n\n  \nxmlns:x=\n\"http://schemas.microsoft.com/winfx/2006/xaml\"\n\n  \nx:Class=\n\"ExampleNamespace.ExampleCode\"\n\n  \n>\n\n  \n<StackPanel>\n\n    \n<Button>\nButton 1\n</Button>\n\n    \n<Button>\nButton 2\n</Button>\n\n    \n<Button>\nButton 3\n</Button>\n\n  \n</StackPanel>\n\n\n</Page>\n\n\n\n\n\n\nStyles\n\u00b6\n\n\n<Window.Resources>\n<!--A Style that affects all TextBlocks-->\n\n\n<Style\n \nTargetType=\n\"TextBlock\"\n>\n\n  \n<Setter\n \nProperty=\n\"HorizontalAlignment\"\n \nValue=\n\"Center\"\n \n/>\n\n  \n<Setter\n \nProperty=\n\"FontFamily\"\n \nValue=\n\"Comic Sans MS\"\n/>\n\n  \n<Setter\n \nProperty=\n\"FontSize\"\n \nValue=\n\"14\"\n/></Style></Window.Resources>\n\n\n<!--A Style that extends the previous TextBlock Style--><!--This is a \"named style\" with an x:Key of TitleText-->\n\n\n<Style\n \nBasedOn=\n\"{StaticResource {x:Type TextBlock}}\"\n\n       \nTargetType=\n\"TextBlock\"\n\n       \nx:Key=\n\"TitleText\"\n>\n\n  \n<Setter\n \nProperty=\n\"FontSize\"\n \nValue=\n\"26\"\n/>\n\n  \n<Setter\n \nProperty=\n\"Foreground\"\n>\n\n  \n<Setter.Value>\n\n      \n<LinearGradientBrush\n \nStartPoint=\n\"0.5,0\"\n \nEndPoint=\n\"0.5,1\"\n>\n\n        \n<LinearGradientBrush.GradientStops>\n\n          \n<GradientStop\n \nOffset=\n\"0.0\"\n \nColor=\n\"#90DDDD\"\n \n/>\n\n          \n<GradientStop\n \nOffset=\n\"1.0\"\n \nColor=\n\"#5BFFFF\"\n \n/>\n\n        \n</LinearGradientBrush.GradientStops>\n\n      \n</LinearGradientBrush>\n\n    \n</Setter.Value>\n\n  \n</Setter></Style>\n\n\n\n\n\n\nTriggers\n\u00b6\n\n\n<Style\n \nx:Key=\n\"SpecialButton\"\n \nTargetType=\n\"{x:Type Button}\"\n>\n\n  \n<Style.Triggers>\n\n    \n<Trigger\n \nProperty=\n\"Button.IsMouseOver\"\n \nValue=\n\"true\"\n>\n\n      \n<Setter\n \nProperty =\n \n\"Background\"\n \nValue=\n\"Red\"\n/>\n\n    \n</Trigger>\n\n    \n<Trigger\n \nProperty=\n\"Button.IsPressed\"\n \nValue=\n\"true\"\n>\n\n      \n<Setter\n \nProperty =\n \n\"Foreground\"\n \nValue=\n\"Green\"\n/>\n\n    \n</Trigger>\n\n  \n</Style.Triggers></Style>",
            "title": "WPF"
        },
        {
            "location": "/dotNET/WPF/#useful-links",
            "text": "WPF tutorial  WPF Documentation  WPF Samples  WPF Tools",
            "title": "Useful Links"
        },
        {
            "location": "/dotNET/WPF/#application",
            "text": "<Application   x:Class= \"ExpenseIt.App\" \n      xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" \n      xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" \n      StartupUri= \"MainWindow.xaml\" > \n     <Application.Resources> \n     </Application.Resources>  </Application>",
            "title": "Application"
        },
        {
            "location": "/dotNET/WPF/#commands",
            "text": "WPF Commands  WPF provides a set of predefined commands. The command library consists of the following classes: ApplicationCommands, NavigationCommands, MediaCommands, EditingCommands, and the ComponentCommands.  <StackPanel>  <StackPanel.ContextMenu> \n     <ContextMenu> \n       <MenuItem   Command= \"ApplicationCommands.Properties\"   /> \n     </ContextMenu>  </StackPanel.ContextMenu> \n   <Menu> \n     <MenuItem   Command= \"ApplicationCommands.Paste\"   /> \n   </Menu> \n   <TextBox   /></StackPanel>  <Window.InputBindings> \n   <KeyBinding   Key= \"B\" \n               Modifiers= \"Control\"  \n               Command= \"ApplicationCommands.Open\"   /></Window.InputBindings>",
            "title": "Commands"
        },
        {
            "location": "/dotNET/WPF/#page",
            "text": "<Page \n   xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" \n   xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" \n   x:Class= \"ExampleNamespace.ExampleCode\" \n   > \n   <StackPanel> \n     <Button> Button 1 </Button> \n     <Button> Button 2 </Button> \n     <Button> Button 3 </Button> \n   </StackPanel>  </Page>",
            "title": "Page"
        },
        {
            "location": "/dotNET/WPF/#styles",
            "text": "<Window.Resources> <!--A Style that affects all TextBlocks-->  <Style   TargetType= \"TextBlock\" > \n   <Setter   Property= \"HorizontalAlignment\"   Value= \"Center\"   /> \n   <Setter   Property= \"FontFamily\"   Value= \"Comic Sans MS\" /> \n   <Setter   Property= \"FontSize\"   Value= \"14\" /></Style></Window.Resources>  <!--A Style that extends the previous TextBlock Style--><!--This is a \"named style\" with an x:Key of TitleText-->  <Style   BasedOn= \"{StaticResource {x:Type TextBlock}}\" \n        TargetType= \"TextBlock\" \n        x:Key= \"TitleText\" > \n   <Setter   Property= \"FontSize\"   Value= \"26\" /> \n   <Setter   Property= \"Foreground\" > \n   <Setter.Value> \n       <LinearGradientBrush   StartPoint= \"0.5,0\"   EndPoint= \"0.5,1\" > \n         <LinearGradientBrush.GradientStops> \n           <GradientStop   Offset= \"0.0\"   Color= \"#90DDDD\"   /> \n           <GradientStop   Offset= \"1.0\"   Color= \"#5BFFFF\"   /> \n         </LinearGradientBrush.GradientStops> \n       </LinearGradientBrush> \n     </Setter.Value> \n   </Setter></Style>",
            "title": "Styles"
        },
        {
            "location": "/dotNET/WPF/#triggers",
            "text": "<Style   x:Key= \"SpecialButton\"   TargetType= \"{x:Type Button}\" > \n   <Style.Triggers> \n     <Trigger   Property= \"Button.IsMouseOver\"   Value= \"true\" > \n       <Setter   Property =   \"Background\"   Value= \"Red\" /> \n     </Trigger> \n     <Trigger   Property= \"Button.IsPressed\"   Value= \"true\" > \n       <Setter   Property =   \"Foreground\"   Value= \"Green\" /> \n     </Trigger> \n   </Style.Triggers></Style>",
            "title": "Triggers"
        }
    ]
}