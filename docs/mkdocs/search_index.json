{
    "docs": [
        {
            "location": "/",
            "text": "You will find here a collection of cheatsheets, code templates and snippets that I have collected over the years...\n\n\nGiven that they were created for my own use, these notes are often very terse and dense. Thank you for your patience, while I am slowly improving their readability. I also have hundreds more to move to GitHub Pages :-) \n\n\nIn the meanwhile, feel free to use as you wish. Please email me suggestions and corrections.",
            "title": "Home"
        },
        {
            "location": "/Big_Data/Hadoop_Ecosystem/",
            "text": "Hadoop Ecosystem\n\u00b6\n\n\nHadoop is not a single product, but rather a software family. Its common components consist of the following:\n\n\n\n\nPig, a scripting language used to quickly write MapReduce code to handle unstructured sources \n\n\nHive, used to facilitate structure for the data \n\n\nHCatalog, used to provide inter-operatability between these internal systems \n\n\nHBase, which is essentially a database built on top of Hadoop \n\n\nHDFS, the actual file system for hadoop.\n\n\nApache Mahout\n\n\nPackaging for Hadoop: \nBigTop\n\n\n\n\nHadoop structures data using Hive, but can handle unstructured data easily using Pig.\n\n\nHadoop and Mongo\n\u00b6\n\n\n\n\nHadoop and MongoDB\n\n\nHadoop and MongoDB Use Cases\n\n\n\n\nAWS EMR\n\u00b6\n\n\nAmazon EMR Best Practices\n\n\nAmazon EMR includes\n\n\n\n\nGanglia\n\n\nHadoop\n\n\nHBase\n\n\nHCatalog\n\n\nHive\n\n\nHue\n\n\nMahout\n\n\nOozie\n\n\nPhoenix\n\n\nPig\n\n\nPrest0\n\n\nSpark\n\n\nSqoop\n\n\nTez\n\n\nZeppelin\n\n\nZooKeeper",
            "title": "Hadoop Ecosystem"
        },
        {
            "location": "/Big_Data/Hadoop_Ecosystem/#hadoop-ecosystem",
            "text": "Hadoop is not a single product, but rather a software family. Its common components consist of the following:   Pig, a scripting language used to quickly write MapReduce code to handle unstructured sources   Hive, used to facilitate structure for the data   HCatalog, used to provide inter-operatability between these internal systems   HBase, which is essentially a database built on top of Hadoop   HDFS, the actual file system for hadoop.  Apache Mahout  Packaging for Hadoop:  BigTop   Hadoop structures data using Hive, but can handle unstructured data easily using Pig.",
            "title": "Hadoop Ecosystem"
        },
        {
            "location": "/Big_Data/Hadoop_Ecosystem/#hadoop-and-mongo",
            "text": "Hadoop and MongoDB  Hadoop and MongoDB Use Cases",
            "title": "Hadoop and Mongo"
        },
        {
            "location": "/Big_Data/Hadoop_Ecosystem/#aws-emr",
            "text": "Amazon EMR Best Practices  Amazon EMR includes   Ganglia  Hadoop  HBase  HCatalog  Hive  Hue  Mahout  Oozie  Phoenix  Pig  Prest0  Spark  Sqoop  Tez  Zeppelin  ZooKeeper",
            "title": "AWS EMR"
        },
        {
            "location": "/Big_Data/Spark/",
            "text": "Spark Basics\n\u00b6\n\n\n\n\nMain Web Site\n\n\nApache Spark on Wikipedia\n\n\n\n\nUseful Links\n\u00b6\n\n\n\n\nAmpcamp big data bootcamp\n\n\nRDDs Simplified\n\n\nElasticsearch and Apache Lucene for Apache Spark and MLlib\n\n\nSpark on AWS\n\n\nRunning Apache Spark on AWS\n\n\nRunning Apache Spark EMR and EC2 scripts on AWS with read write S3\n\n\nSpark on EMR - How to Submit a Spark Application with EMR Steps\n\n\nDatabricks Reference Apps\n\n\nIntroduction to Apache Spark with Examples and Use Cases\n\n\n\n\nSpark on a local machine\n\u00b6\n\n\nInstall\n\u00b6\n\n\nTo run Spark interactively in a Python interpreter, use \nbin/pyspark\n:\n\n\n./bin/pyspark --master local\n[\n2\n]\n\n\n\n\n\n\nSubmit jobs\n\u00b6\n\n\n./bin/spark-submit examples/src/main/python/pi.py \n10\n\n\n\n\n\n\nSpark and MongoDB\n\u00b6\n\n\n\n\nUsing MongoDB with Apache Spark\n\n\nMongoDB Spark connector\n\n\n\n\nSpark and NLP\n\u00b6\n\n\n\n\nDictionary Based Annotation at Scale with Spark, SolrTextTagger and OpenNLP\n\n\n\n\nHere is a complete set of example on how to use DL4J (Deep Learning for Java) that uses UIMA on the SPARK platform\n\n\nDeep Learning for Java\n\n\nand in the following project the use of CTAKES UIMA module from within the Spark framework\n\n\nNatural Language Processing with Apache Spark\n\n\nGraphX\n\u00b6\n\n\n\n\nGraphx programming guide\n\n\n\n\nSpark on AWS EMR\n\u00b6\n\n\nSpark on AWS EMR\n\n\nCreate a Cluster With Spark\n\u00b6\n\n\nTo launch a cluster with Spark installed using the console\n\n\nThe following procedure creates a cluster with Spark installed.\n\n\n\n\nOpen the Amazon EMR console at \nhttps://console.aws.amazon.com/elasticmapreduce/\n.\n\n\nChoose Create cluster to use Quick Create.\n\n\nFor the Software Configuration field, choose Amazon Release Version emr-5.0.0 or later.\n\n\nIn the Select Applications field, choose either All Applications or Spark.\n\n\nSelect other options as necessary and then choose Create cluster.NoteTo configure Spark when you are creating the cluster, see Configure Spark.\n\n\n\n\n\n\n\n\nTo launch a cluster with Spark installed using the AWS CLI\n\n\nCreate the cluster with the following command:\n\n\naws emr create-cluster --name \n\"Spark cluster\"\n --release-label emr-5.0.0 --applications \nName\n=\nSpark \n\\ \n--ec2-attributes \nKeyName\n=\nmyKey --instance-type m3.xlarge --instance-count \n3\n --use-default-roles\n\n\n\n\n\nNote: For Windows, replace the above Linux line continuation character () with the caret (^).\n\n\nWith the AWS CLI\n\u00b6\n\n\nSimple cluster:\n\n\naws emr create-cluster --name \n\"Spark cluster\"\n --release-label  --applications \nName\n=\nSpark \n\\\n\n--ec2-attributes \nKeyName\n=\nmyKey --instance-type m3.xlarge --instance-count \n3\n --use-default-roles\n\n\n\n\n\nWith config file:\n\n\naws emr create-cluster --release-label --applications \nName\n=\nSpark \n\\\n\n--instance-type m3.xlarge --instance-count \n3\n --configurations https://s3.amazonaws.com/mybucket/myfolder/myConfig.json\n\n\n\n\n\nmyConfig.json:\n\n\n[\n\n  \n{\n\n    \n\"Classification\"\n:\n \n\"spark\"\n,\n\n    \n\"Properties\"\n:\n \n{\n\n      \n\"maximizeResourceAllocation\"\n:\n \n\"true\"\n\n    \n}\n\n  \n}\n\n\n]\n\n\n\n\n\n\nWith Spot instances:\n\n\naws emr create-cluster --name \n\"Spot cluster\"\n --release-label emr-5.0.0 --applications \nName\n=\nSpark \n\\\n\n--use-default-roles --ec2-attributes \nKeyName\n=\nmyKey \n\\\n\n--instance-groups \nInstanceGroupType\n=\nMASTER,InstanceType\n=\nm3.xlarge,InstanceCount\n=\n1\n,BidPrice\n=\n0\n.25 \n\\\n\n\nInstanceGroupType\n=\nCORE,BidPrice\n=\n0\n.03,InstanceType\n=\nm3.xlarge,InstanceCount\n=\n2\n\n\n\n# InstanceGroupType=TASK,BidPrice=0.10,InstanceType=m3.xlarge,InstanceCount=3\n\n\n\n\n\n\nIn Java:\n\n\n// start Spark on EMR in java\n\n\nAmazonElasticMapReduceClient\n \nemr\n \n=\n \nnew\n \nAmazonElasticMapReduceClient\n(\ncredentials\n);\n\n\nApplication\n \nsparkApp\n \n=\n \nnew\n \nApplication\n()\n \n.\nwithName\n(\n\"Spark\"\n);\n\n\nApplications\n \nmyApps\n \n=\n \nnew\n \nApplications\n();\n\n\nmyApps\n.\nadd\n(\nsparkApp\n);\n\n\nRunJobFlowRequest\n \nrequest\n \n=\n \nnew\n \nRunJobFlowRequest\n()\n \n.\nwithName\n(\n\"Spark Cluster\"\n)\n \n.\nwithApplications\n(\nmyApps\n)\n \n.\nwithReleaseLabel\n(\n\"\"\n)\n \n.\nwithInstances\n(\nnew\n \nJobFlowInstancesConfig\n()\n \n.\nwithEc2KeyName\n(\n\"myKeyName\"\n)\n \n.\nwithInstanceCount\n(\n1\n)\n \n.\nwithKeepJobFlowAliveWhenNoSteps\n(\ntrue\n)\n \n.\nwithMasterInstanceType\n(\n\"m3.xlarge\"\n)\n \n.\nwithSlaveInstanceType\n(\n\"m3.xlarge\"\n)\n \n);\n \nRunJobFlowResult\n \nresult\n \n=\n \nemr\n.\nrunJobFlow\n(\nrequest\n);\n\n\n\n\n\n\nConnect to the Master Node Using SSH\n\n\nTo connect to the master node using SSH, you need the public DNS name of the master node and your Amazon EC2 key pair private key. The Amazon EC2 key pair private key is specified when you launch the cluster.\n\n\nTo retrieve the public DNS name of the master node using the AWS CLI\n\n\n\n\n\n\nTo retrieve the cluster identifier, type the following command.aws emr list-clustersThe output lists your clusters including the cluster IDs. Note the cluster ID for the cluster to which you are connecting.\"Status\": {     \"Timeline\": {         \"ReadyDateTime\": 1408040782.374,         \"CreationDateTime\": 1408040501.213     },     \"State\": \"WAITING\",     \"StateChangeReason\": {         \"Message\": \"Waiting after step completed\"     } }, \"NormalizedInstanceHours\": 4,\"Id\": \"j-2AL4XXXXXX5T9\", \"Name\": \"My cluster\"\n\n\n\n\n\n\nTo list the cluster instances including the master public DNS name for the cluster, type one of the following commands. Replace j-2AL4XXXXXX5T9 with the cluster ID returned by the previous command.aws emr list-instances --cluster-id j-2AL4XXXXXX5T9Or:aws emr describe-clusters --cluster-id j-2AL4XXXXXX5T9\n\n\n\n\n\n\nView Web Interfaces Hosted on Amazon EMR Clusters\n\u00b6\n\n\n\n\n\n\nView Web Interfaces Hosted on Amazon EMR Clusters\n\n\n\n\n\n\nYARN ResourceManager: \nhttp://master-public-dns-name:8088\n\n\n\n\nYARN NodeManager: \nhttp://slave-public-dns-name:8042\n\n\nHadoop HDFS NameNode: \nhttp://master-public-dns-name:50070\n\n\nHadoop HDFS DataNode: \nhttp://slave-public-dns-name:50075\n\n\nSpark HistoryServer: \nhttp://master-public-dns-name:18080\n\n\nZeppelin: \nhttp://master-public-dns-name:8890\n\n\nHue: \nhttp://master-public-dns-name:8888\n\n\nGanglia: \nhttp://master-public-dns-name/ganglia\n\n\nHBase UI: \nhttp://master-public-dns-name:16010\n\n\n\n\nLaunching Applications with spark-submit\n\u00b6\n\n\n[Launching applications with spark submit] ( https://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit )\n\n\nApache Zeppelin\n\u00b6\n\n\nConnect to Zeppelin using the same SSH tunneling method to connect to other web servers on the master node. Zeppelin server is found at port 8890.\n\n\nZeppelin\n\n\nDataFrames API\n\u00b6\n\n\nDataFrame operations: \n- printSchema()\n- select()\n- show()\n- count()\n- groupBy()\n- sum()\n- limit()\n- orderBy()\n- filter()\n- withColumnRenamed()\n- join()\n- withColumn()\n\n\n// In the Regular Expression below:\n\n\n// ^  - Matches beginning of line\n\n\n// .* - Matches any characters, except newline\n\n\n\npagecountsEnWikipediaArticlesOnlyDF\n\n \n.\nfilter\n(\n$\n\"article\"\n.\nrlike\n(\n\"\"\"^Apache_.*\"\"\"\n))\n\n \n.\norderBy\n(\n$\n\"requests\"\n.\ndesc\n)\n\n \n.\nshow\n()\n \n// By default, show will return 20 rows\n\n\n\n// Import the sql functions package, which includes statistical functions like sum, max, min, avg, etc.\n\n\nimport\n \norg.apache.spark.sql.functions._\n\n\n\npagecountsEnWikipediaDF\n.\ngroupBy\n(\n\"project\"\n).\nsum\n().\nshow\n()\n\n\n\n\n\n\nColumns\n\u00b6\n\n\nA new column is constructed based on the input columns present in a dataframe:\n\n\ndf\n(\n\"columnName\"\n)\n \n// On a specific DataFrame.\n\n\ncol\n(\n\"columnName\"\n)\n \n// A generic column no yet associated with a DataFrame.\n\n\ncol\n(\n\"columnName.field\"\n)\n \n// Extracting a struct field\n\n\ncol\n(\n\"`a.column.with.dots`\"\n)\n \n// Escape `.` in column names.\n\n\n$\n\"columnName\"\n \n// Scala short hand for a named column.\n\n\nexpr\n(\n\"a + 1\"\n)\n \n// A column that is constructed from a parsed SQL Expression.\n\n\nlit\n(\n\"abc\"\n)\n \n// A column that produces a literal (constant) value.\n\n\n\n\n\n\nColumn objects can be composed to form complex expressions:\n\n\n$\n\"a\"\n \n+\n \n1\n\n\n$\n\"a\"\n \n===\n \n$\n\"b\"\n\n\n\n\n\n\nFile Read\n\u00b6\n\n\nCSV - Create a DataFrame with the anticipated structure\n\n\nval\n \nclickstreamDF\n \n=\n \nsqlContext\n.\nread\n.\nformat\n(\n\"com.databricks.spark.csv\"\n)\n\n  \n.\noption\n(\n\"header\"\n,\n \n\"true\"\n)\n\n  \n.\noption\n(\n\"delimiter\"\n,\n \n\"\\\\t\"\n)\n\n  \n.\noption\n(\n\"mode\"\n,\n \n\"PERMISSIVE\"\n)\n\n  \n.\noption\n(\n\"inferSchema\"\n,\n \n\"true\"\n)\n\n  \n.\nload\n(\n\"dbfs:///databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed\"\n)\n\n\n\n\n\n\nPARQUET - To create Dataset[Row] using SparkSession\n\n\nval\n \npeople\n \n=\n \nspark\n.\nread\n.\nparquet\n(\n\"...\"\n)\n\n\nval\n \ndepartment\n \n=\n \nspark\n.\nread\n.\nparquet\n(\n\"...\"\n)\n\n\n\npeople\n.\nfilter\n(\n\"age > 30\"\n)\n\n  \n.\njoin\n(\ndepartment\n,\n \npeople\n(\n\"deptId\"\n)\n \n===\n \ndepartment\n(\n\"id\"\n))\n\n  \n.\ngroupBy\n(\ndepartment\n(\n\"name\"\n),\n \n\"gender\"\n)\n\n  \n.\nagg\n(\navg\n(\npeople\n(\n\"salary\"\n)),\n \nmax\n(\npeople\n(\n\"age\"\n)))\n\n\n\n\n\n\nRepartitioning / Caching\n\u00b6\n\n\nval\n \nclickstreamNoIDs8partDF\n \n=\n \nclickstreamNoIDsDF\n.\nrepartition\n(\n8\n)\n\n\nclickstreamNoIDs8partDF\n.\nregisterTempTable\n(\n\"Clickstream\"\n)\n\n\nsqlContext\n.\ncacheTable\n(\n\"Clickstream\"\n)\n\n\n\n\n\n\nAn ideal partition size in Spark is about 50 MB - 200 MB.\nThe cache gets stored in Project Tungsten binary compressed columnar format.",
            "title": "Spark"
        },
        {
            "location": "/Big_Data/Spark/#spark-basics",
            "text": "Main Web Site  Apache Spark on Wikipedia",
            "title": "Spark Basics"
        },
        {
            "location": "/Big_Data/Spark/#useful-links",
            "text": "Ampcamp big data bootcamp  RDDs Simplified  Elasticsearch and Apache Lucene for Apache Spark and MLlib  Spark on AWS  Running Apache Spark on AWS  Running Apache Spark EMR and EC2 scripts on AWS with read write S3  Spark on EMR - How to Submit a Spark Application with EMR Steps  Databricks Reference Apps  Introduction to Apache Spark with Examples and Use Cases",
            "title": "Useful Links"
        },
        {
            "location": "/Big_Data/Spark/#spark-on-a-local-machine",
            "text": "",
            "title": "Spark on a local machine"
        },
        {
            "location": "/Big_Data/Spark/#install",
            "text": "To run Spark interactively in a Python interpreter, use  bin/pyspark :  ./bin/pyspark --master local [ 2 ]",
            "title": "Install"
        },
        {
            "location": "/Big_Data/Spark/#submit-jobs",
            "text": "./bin/spark-submit examples/src/main/python/pi.py  10",
            "title": "Submit jobs"
        },
        {
            "location": "/Big_Data/Spark/#spark-and-mongodb",
            "text": "Using MongoDB with Apache Spark  MongoDB Spark connector",
            "title": "Spark and MongoDB"
        },
        {
            "location": "/Big_Data/Spark/#spark-and-nlp",
            "text": "Dictionary Based Annotation at Scale with Spark, SolrTextTagger and OpenNLP   Here is a complete set of example on how to use DL4J (Deep Learning for Java) that uses UIMA on the SPARK platform  Deep Learning for Java  and in the following project the use of CTAKES UIMA module from within the Spark framework  Natural Language Processing with Apache Spark",
            "title": "Spark and NLP"
        },
        {
            "location": "/Big_Data/Spark/#graphx",
            "text": "Graphx programming guide",
            "title": "GraphX"
        },
        {
            "location": "/Big_Data/Spark/#spark-on-aws-emr",
            "text": "Spark on AWS EMR",
            "title": "Spark on AWS EMR"
        },
        {
            "location": "/Big_Data/Spark/#create-a-cluster-with-spark",
            "text": "To launch a cluster with Spark installed using the console  The following procedure creates a cluster with Spark installed.   Open the Amazon EMR console at  https://console.aws.amazon.com/elasticmapreduce/ .  Choose Create cluster to use Quick Create.  For the Software Configuration field, choose Amazon Release Version emr-5.0.0 or later.  In the Select Applications field, choose either All Applications or Spark.  Select other options as necessary and then choose Create cluster.NoteTo configure Spark when you are creating the cluster, see Configure Spark.     To launch a cluster with Spark installed using the AWS CLI  Create the cluster with the following command:  aws emr create-cluster --name  \"Spark cluster\"  --release-label emr-5.0.0 --applications  Name = Spark  \\  --ec2-attributes  KeyName = myKey --instance-type m3.xlarge --instance-count  3  --use-default-roles  Note: For Windows, replace the above Linux line continuation character () with the caret (^).",
            "title": "Create a Cluster With Spark"
        },
        {
            "location": "/Big_Data/Spark/#with-the-aws-cli",
            "text": "Simple cluster:  aws emr create-cluster --name  \"Spark cluster\"  --release-label  --applications  Name = Spark  \\ \n--ec2-attributes  KeyName = myKey --instance-type m3.xlarge --instance-count  3  --use-default-roles  With config file:  aws emr create-cluster --release-label --applications  Name = Spark  \\ \n--instance-type m3.xlarge --instance-count  3  --configurations https://s3.amazonaws.com/mybucket/myfolder/myConfig.json  myConfig.json:  [ \n   { \n     \"Classification\" :   \"spark\" , \n     \"Properties\" :   { \n       \"maximizeResourceAllocation\" :   \"true\" \n     } \n   }  ]   With Spot instances:  aws emr create-cluster --name  \"Spot cluster\"  --release-label emr-5.0.0 --applications  Name = Spark  \\ \n--use-default-roles --ec2-attributes  KeyName = myKey  \\ \n--instance-groups  InstanceGroupType = MASTER,InstanceType = m3.xlarge,InstanceCount = 1 ,BidPrice = 0 .25  \\  InstanceGroupType = CORE,BidPrice = 0 .03,InstanceType = m3.xlarge,InstanceCount = 2  # InstanceGroupType=TASK,BidPrice=0.10,InstanceType=m3.xlarge,InstanceCount=3   In Java:  // start Spark on EMR in java  AmazonElasticMapReduceClient   emr   =   new   AmazonElasticMapReduceClient ( credentials );  Application   sparkApp   =   new   Application ()   . withName ( \"Spark\" );  Applications   myApps   =   new   Applications ();  myApps . add ( sparkApp );  RunJobFlowRequest   request   =   new   RunJobFlowRequest ()   . withName ( \"Spark Cluster\" )   . withApplications ( myApps )   . withReleaseLabel ( \"\" )   . withInstances ( new   JobFlowInstancesConfig ()   . withEc2KeyName ( \"myKeyName\" )   . withInstanceCount ( 1 )   . withKeepJobFlowAliveWhenNoSteps ( true )   . withMasterInstanceType ( \"m3.xlarge\" )   . withSlaveInstanceType ( \"m3.xlarge\" )   );   RunJobFlowResult   result   =   emr . runJobFlow ( request );   Connect to the Master Node Using SSH  To connect to the master node using SSH, you need the public DNS name of the master node and your Amazon EC2 key pair private key. The Amazon EC2 key pair private key is specified when you launch the cluster.  To retrieve the public DNS name of the master node using the AWS CLI    To retrieve the cluster identifier, type the following command.aws emr list-clustersThe output lists your clusters including the cluster IDs. Note the cluster ID for the cluster to which you are connecting.\"Status\": {     \"Timeline\": {         \"ReadyDateTime\": 1408040782.374,         \"CreationDateTime\": 1408040501.213     },     \"State\": \"WAITING\",     \"StateChangeReason\": {         \"Message\": \"Waiting after step completed\"     } }, \"NormalizedInstanceHours\": 4,\"Id\": \"j-2AL4XXXXXX5T9\", \"Name\": \"My cluster\"    To list the cluster instances including the master public DNS name for the cluster, type one of the following commands. Replace j-2AL4XXXXXX5T9 with the cluster ID returned by the previous command.aws emr list-instances --cluster-id j-2AL4XXXXXX5T9Or:aws emr describe-clusters --cluster-id j-2AL4XXXXXX5T9",
            "title": "With the AWS CLI"
        },
        {
            "location": "/Big_Data/Spark/#view-web-interfaces-hosted-on-amazon-emr-clusters",
            "text": "View Web Interfaces Hosted on Amazon EMR Clusters    YARN ResourceManager:  http://master-public-dns-name:8088   YARN NodeManager:  http://slave-public-dns-name:8042  Hadoop HDFS NameNode:  http://master-public-dns-name:50070  Hadoop HDFS DataNode:  http://slave-public-dns-name:50075  Spark HistoryServer:  http://master-public-dns-name:18080  Zeppelin:  http://master-public-dns-name:8890  Hue:  http://master-public-dns-name:8888  Ganglia:  http://master-public-dns-name/ganglia  HBase UI:  http://master-public-dns-name:16010",
            "title": "View Web Interfaces Hosted on Amazon EMR Clusters"
        },
        {
            "location": "/Big_Data/Spark/#launching-applications-with-spark-submit",
            "text": "[Launching applications with spark submit] ( https://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit )",
            "title": "Launching Applications with spark-submit"
        },
        {
            "location": "/Big_Data/Spark/#apache-zeppelin",
            "text": "Connect to Zeppelin using the same SSH tunneling method to connect to other web servers on the master node. Zeppelin server is found at port 8890.  Zeppelin",
            "title": "Apache Zeppelin"
        },
        {
            "location": "/Big_Data/Spark/#dataframes-api",
            "text": "DataFrame operations: \n- printSchema()\n- select()\n- show()\n- count()\n- groupBy()\n- sum()\n- limit()\n- orderBy()\n- filter()\n- withColumnRenamed()\n- join()\n- withColumn()  // In the Regular Expression below:  // ^  - Matches beginning of line  // .* - Matches any characters, except newline  pagecountsEnWikipediaArticlesOnlyDF \n  . filter ( $ \"article\" . rlike ( \"\"\"^Apache_.*\"\"\" )) \n  . orderBy ( $ \"requests\" . desc ) \n  . show ()   // By default, show will return 20 rows  // Import the sql functions package, which includes statistical functions like sum, max, min, avg, etc.  import   org.apache.spark.sql.functions._  pagecountsEnWikipediaDF . groupBy ( \"project\" ). sum (). show ()",
            "title": "DataFrames API"
        },
        {
            "location": "/Big_Data/Spark/#columns",
            "text": "A new column is constructed based on the input columns present in a dataframe:  df ( \"columnName\" )   // On a specific DataFrame.  col ( \"columnName\" )   // A generic column no yet associated with a DataFrame.  col ( \"columnName.field\" )   // Extracting a struct field  col ( \"`a.column.with.dots`\" )   // Escape `.` in column names.  $ \"columnName\"   // Scala short hand for a named column.  expr ( \"a + 1\" )   // A column that is constructed from a parsed SQL Expression.  lit ( \"abc\" )   // A column that produces a literal (constant) value.   Column objects can be composed to form complex expressions:  $ \"a\"   +   1  $ \"a\"   ===   $ \"b\"",
            "title": "Columns"
        },
        {
            "location": "/Big_Data/Spark/#file-read",
            "text": "CSV - Create a DataFrame with the anticipated structure  val   clickstreamDF   =   sqlContext . read . format ( \"com.databricks.spark.csv\" ) \n   . option ( \"header\" ,   \"true\" ) \n   . option ( \"delimiter\" ,   \"\\\\t\" ) \n   . option ( \"mode\" ,   \"PERMISSIVE\" ) \n   . option ( \"inferSchema\" ,   \"true\" ) \n   . load ( \"dbfs:///databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed\" )   PARQUET - To create Dataset[Row] using SparkSession  val   people   =   spark . read . parquet ( \"...\" )  val   department   =   spark . read . parquet ( \"...\" )  people . filter ( \"age > 30\" ) \n   . join ( department ,   people ( \"deptId\" )   ===   department ( \"id\" )) \n   . groupBy ( department ( \"name\" ),   \"gender\" ) \n   . agg ( avg ( people ( \"salary\" )),   max ( people ( \"age\" )))",
            "title": "File Read"
        },
        {
            "location": "/Big_Data/Spark/#repartitioning-caching",
            "text": "val   clickstreamNoIDs8partDF   =   clickstreamNoIDsDF . repartition ( 8 )  clickstreamNoIDs8partDF . registerTempTable ( \"Clickstream\" )  sqlContext . cacheTable ( \"Clickstream\" )   An ideal partition size in Spark is about 50 MB - 200 MB.\nThe cache gets stored in Project Tungsten binary compressed columnar format.",
            "title": "Repartitioning / Caching"
        },
        {
            "location": "/Cloud/AWS/",
            "text": "AWS Services Overview\n\u00b6\n\n\nBasic Services\n\u00b6\n\n\n\n\nCompute: EC2 (autoscaling, ELB load balancing)\n\n\nNetworking / Security: VPC (security groups), IAM (users/groups/application roles)\n\n\n\n\nStorage\n\u00b6\n\n\n\n\nS3: secure, scalable object-level storage, static web site hosting...\n\n\nGlacier: long-term storage\n\n\nEBS: block-level storage (for EC2 instances)\n\n\n\n\nDatabases\n\u00b6\n\n\n\n\nRDS: relational databases (MySQL, PostgreSQL, MSSQL, MariaDB, Aurora...)\n\n\nDynamoDB: scalable NoSQL database backed by solid-state drives\n\n\n\n\nAnalytics\n\u00b6\n\n\n\n\nRedShift: PostgreSQL-based columnstore OLAP database that uses SQL. MPP architecture.\n\n\nEMR: Hadoop cluster (Hive, Pig, HBase, Spark...).\n\n\n\n\nETL / ELT / Batch Processing\n\u00b6\n\n\n\n\nGlue\n\n\nBatch\n\n\nData Pipeline: orchestrate data transfers between S3, DynamoDB, Redshift\n\n\n\n\nApplication Services\n\u00b6\n\n\n\n\nNotifications: SNS (alerts by email, SMS...), SES (bulk email)\n\n\nQueue: SQS (async message queues for component decoupling) \n\n\nWorkflows, State Machine as a Service: AWS Step Functions, SWF (task-oriented workflows - complicated)\n\n\nDocument Search: ElasticSearch, CloudSearch\n\n\n\n\nMonitoring\n\u00b6\n\n\n\n\nCloudwatch (monitor services and instances e.g. CPU utilization, etc...)\n\n\nCloudTrail (monitor API calls)\n\n\n\n\nInfrastructure Deployment / Automation\n\u00b6\n\n\n\n\nElastic Beanstalk (simple, mostly web or Linux worker)\n\n\nCloudFormation (JSON / YAML templates - more difficult, but many existing templates)\n\n\nOpsWork (higher level than CloudFormation, uses non-native components - Chef-based)\n\n\n\n\nDesktop in the Cloud\n\u00b6\n\n\n\n\nWorkSpaces\n\n\n\n\nDetails\n\u00b6\n\n\nTools\n\u00b6\n\n\n\n\n\n\nUnix tools on Windows: \nCygwin\n\n\n\n\n\n\nPutty SSH client for Windows \ndoc\n\n\n\n\n\n\nDownload and install PuTTY \nlink\n. Be sure to install the entire suite.\n\n\n\n\nStart PuTTYgen (for example, from the Start menu, click All Programs > PuTTY > PuTTYgen).\n\n\nUnder Type of key to generate, select SSH-2 RSA.\n\n\nLoad the .pem file (private key) downloaded from the console (in \"credentials\" folder) \n\n\n\n\nSave private key\n\n\n\n\n\n\nAWS \ncommand line interface\n\n\n\n\n\n\nAWS toolkit for Visual Studio\n\n\n\n\n\n\nAWS tools for PowerShell\n\n\n\n\n\n\nAWS EC2\n\u00b6\n\n\n\n\nLog onto instance with \nPutty SSH\n\n\n\n\nlogin as: ec2-user (Amazon Linux) or: ubuntu\n\n\nBash shell documentation\n\n\n\n\n\n\nUse a shell script to configure the instance \nlink\n\n\n\n\n\n\nUser data: You can specify user data to configure an instance during launch, or to run a configuration script. To attach a file, select the \"As file\" option and browse for the file to attach.\n\n\n\n\n\n\nAWS S3\n\u00b6\n\n\nGUI tools to upload / manage files:\n\n\n\n\nAWS Console\n\n\nS3 Browser\n\n\nCloudBerry\n\n\n\n\nCommand-line s3 clients:\n\n\n\n\nAWS command line (see above)\n\n\nS3 command line tools\n\n\n\n\nRedshift\n\u00b6\n\n\n1) Use Case\n\n\n\n\nLarge-scale SQL analytical database\n\n\nQuerying in Redshift is FAST\n\n\nFull SQL compared to \nHiveQL\n\n\nRedshift isn\u2019t a complete replacement for a Hadoop system (no streaming, no text processing)\n\n\n\n\n2) Tools\n\n\n\n\ninstall SQL \ntool\n\n\nor \nAginity\n\n\nMicrosoft \nSSDT\n\n\n\n\n3) Get data into Redshift:\n\n\n\n\nCOPY from S3 (delimited text files)\n\n\nCOPY from DynamoDB (NoSQL datastore)\n\n\nJDBC/ODBC transactions (not efficient for bulk loading)\n\n\n\n\nTables have \u2018keys\u2019 that define how the data is split across slices. The recommended practice is to split based upon commonly-joined columns, so that joined data resides on the same slice, thus avoiding the need to move data between systems.\n\n\n4) Examples:\n\n\nCOPY\n \ntable1\n \nFROM\n \n's3://bucket1/'\n \ncredentials\n \n'aws_access_key_id=abc;aws_secret_access_key=xyz'\n \ndelimiter\n \n'|'\n \ngzip\n \nremovequotes\n \ntruncatecolumns\n \nmaxerror\n \n1000\n\n\nSELECT\n \nDISTINCT\n \nfield1\n \nFROM\n \ntable1\n\n\nSELECT\n \nCOUNT\n(\nDISTINCT\n \nfield2\n)\n \nFROM\n \ntable1\n\n\n\n\n\n\nEMR\n\u00b6\n\n\n\n\nEMR FAQs\n\n\nExtract, Transform, and Load (ETL) Data with Amazon EMR\n\n\nEMR article\n\n\n\n\nSWF\n\u00b6\n\n\nThe Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components. In Amazon SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing intertask dependencies, scheduling, and concurrency in accordance with the logical flow of the application. Amazon SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state.\n\n\nWhen using Amazon SWF, you implement workers to perform tasks. These workers can run either on cloud infrastructure, such as Amazon Elastic Compute Cloud (Amazon EC2), or on your own premises. You can create tasks that are long-running, or that may fail, time out, or require restarts\u2014or that may complete with varying throughput and latency. Amazon SWF stores tasks and assigns them to workers when they are ready, tracks their progress, and maintains their state, including details on their completion. To coordinate tasks, you write a program that gets the latest state of each task from Amazon SWF and uses it to initiate subsequent tasks. Amazon SWF maintains an application's execution state durably so that the application is resilient to failures in individual components. With Amazon SWF, you can implement, deploy, scale, and modify these application components independently.\n\n\nAmazon SWF offers capabilities to support a variety of application requirements. It is suitable for a range of use cases that require coordination of tasks, including media processing, web application back-ends, business process workflows, and analytics pipelines.",
            "title": "AWS"
        },
        {
            "location": "/Cloud/AWS/#aws-services-overview",
            "text": "",
            "title": "AWS Services Overview"
        },
        {
            "location": "/Cloud/AWS/#basic-services",
            "text": "Compute: EC2 (autoscaling, ELB load balancing)  Networking / Security: VPC (security groups), IAM (users/groups/application roles)",
            "title": "Basic Services"
        },
        {
            "location": "/Cloud/AWS/#storage",
            "text": "S3: secure, scalable object-level storage, static web site hosting...  Glacier: long-term storage  EBS: block-level storage (for EC2 instances)",
            "title": "Storage"
        },
        {
            "location": "/Cloud/AWS/#databases",
            "text": "RDS: relational databases (MySQL, PostgreSQL, MSSQL, MariaDB, Aurora...)  DynamoDB: scalable NoSQL database backed by solid-state drives",
            "title": "Databases"
        },
        {
            "location": "/Cloud/AWS/#analytics",
            "text": "RedShift: PostgreSQL-based columnstore OLAP database that uses SQL. MPP architecture.  EMR: Hadoop cluster (Hive, Pig, HBase, Spark...).",
            "title": "Analytics"
        },
        {
            "location": "/Cloud/AWS/#etl-elt-batch-processing",
            "text": "Glue  Batch  Data Pipeline: orchestrate data transfers between S3, DynamoDB, Redshift",
            "title": "ETL / ELT / Batch Processing"
        },
        {
            "location": "/Cloud/AWS/#application-services",
            "text": "Notifications: SNS (alerts by email, SMS...), SES (bulk email)  Queue: SQS (async message queues for component decoupling)   Workflows, State Machine as a Service: AWS Step Functions, SWF (task-oriented workflows - complicated)  Document Search: ElasticSearch, CloudSearch",
            "title": "Application Services"
        },
        {
            "location": "/Cloud/AWS/#monitoring",
            "text": "Cloudwatch (monitor services and instances e.g. CPU utilization, etc...)  CloudTrail (monitor API calls)",
            "title": "Monitoring"
        },
        {
            "location": "/Cloud/AWS/#infrastructure-deployment-automation",
            "text": "Elastic Beanstalk (simple, mostly web or Linux worker)  CloudFormation (JSON / YAML templates - more difficult, but many existing templates)  OpsWork (higher level than CloudFormation, uses non-native components - Chef-based)",
            "title": "Infrastructure Deployment / Automation"
        },
        {
            "location": "/Cloud/AWS/#desktop-in-the-cloud",
            "text": "WorkSpaces",
            "title": "Desktop in the Cloud"
        },
        {
            "location": "/Cloud/AWS/#details",
            "text": "",
            "title": "Details"
        },
        {
            "location": "/Cloud/AWS/#tools",
            "text": "Unix tools on Windows:  Cygwin    Putty SSH client for Windows  doc    Download and install PuTTY  link . Be sure to install the entire suite.   Start PuTTYgen (for example, from the Start menu, click All Programs > PuTTY > PuTTYgen).  Under Type of key to generate, select SSH-2 RSA.  Load the .pem file (private key) downloaded from the console (in \"credentials\" folder)    Save private key    AWS  command line interface    AWS toolkit for Visual Studio    AWS tools for PowerShell",
            "title": "Tools"
        },
        {
            "location": "/Cloud/AWS/#aws-ec2",
            "text": "Log onto instance with  Putty SSH   login as: ec2-user (Amazon Linux) or: ubuntu  Bash shell documentation    Use a shell script to configure the instance  link    User data: You can specify user data to configure an instance during launch, or to run a configuration script. To attach a file, select the \"As file\" option and browse for the file to attach.",
            "title": "AWS EC2"
        },
        {
            "location": "/Cloud/AWS/#aws-s3",
            "text": "GUI tools to upload / manage files:   AWS Console  S3 Browser  CloudBerry   Command-line s3 clients:   AWS command line (see above)  S3 command line tools",
            "title": "AWS S3"
        },
        {
            "location": "/Cloud/AWS/#redshift",
            "text": "1) Use Case   Large-scale SQL analytical database  Querying in Redshift is FAST  Full SQL compared to  HiveQL  Redshift isn\u2019t a complete replacement for a Hadoop system (no streaming, no text processing)   2) Tools   install SQL  tool  or  Aginity  Microsoft  SSDT   3) Get data into Redshift:   COPY from S3 (delimited text files)  COPY from DynamoDB (NoSQL datastore)  JDBC/ODBC transactions (not efficient for bulk loading)   Tables have \u2018keys\u2019 that define how the data is split across slices. The recommended practice is to split based upon commonly-joined columns, so that joined data resides on the same slice, thus avoiding the need to move data between systems.  4) Examples:  COPY   table1   FROM   's3://bucket1/'   credentials   'aws_access_key_id=abc;aws_secret_access_key=xyz'   delimiter   '|'   gzip   removequotes   truncatecolumns   maxerror   1000  SELECT   DISTINCT   field1   FROM   table1  SELECT   COUNT ( DISTINCT   field2 )   FROM   table1",
            "title": "Redshift"
        },
        {
            "location": "/Cloud/AWS/#emr",
            "text": "EMR FAQs  Extract, Transform, and Load (ETL) Data with Amazon EMR  EMR article",
            "title": "EMR"
        },
        {
            "location": "/Cloud/AWS/#swf",
            "text": "The Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components. In Amazon SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing intertask dependencies, scheduling, and concurrency in accordance with the logical flow of the application. Amazon SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state.  When using Amazon SWF, you implement workers to perform tasks. These workers can run either on cloud infrastructure, such as Amazon Elastic Compute Cloud (Amazon EC2), or on your own premises. You can create tasks that are long-running, or that may fail, time out, or require restarts\u2014or that may complete with varying throughput and latency. Amazon SWF stores tasks and assigns them to workers when they are ready, tracks their progress, and maintains their state, including details on their completion. To coordinate tasks, you write a program that gets the latest state of each task from Amazon SWF and uses it to initiate subsequent tasks. Amazon SWF maintains an application's execution state durably so that the application is resilient to failures in individual components. With Amazon SWF, you can implement, deploy, scale, and modify these application components independently.  Amazon SWF offers capabilities to support a variety of application requirements. It is suitable for a range of use cases that require coordination of tasks, including media processing, web application back-ends, business process workflows, and analytics pipelines.",
            "title": "SWF"
        },
        {
            "location": "/Cloud/Serverless/",
            "text": "Serverless Cheatsheet\n\u00b6\n\n\nServerless home page\n\n\nInstall\n\u00b6\n\n\nnpm install -g serverless\n\n\n\n\n\nExamples\n\u00b6\n\n\nServerless Examples\n\n\nServerless Starter\n\n\nPython example\n\n\nC# example\n\n\nCheatsheet\n\u00b6\n\n\n\n\nCreate a Service:\n\n\n\n\n# NodeJS\n\nserverless create -p \n[\nSERVICE NAME\n]\n -t aws-nodejs\n\n\n# C#\n\nserverless create --path serverlessCSharp --template aws-csharp\n\n\n\n\n\n\n\nInstall a Service\n\n\n\n\nThis is a convenience method to install a pre-made Serverless Service locally by downloading the Github repo and unzipping it.\n\n\nserverless install -u \n[\nGITHUB URL OF SERVICE\n]\n\n\n\n\n\n\n\n\nDeploy All\n\n\n\n\nUse this when you have made changes to your Functions, Events or Resources in \nserverless.yml\n or you simply want to deploy all changes within your Service at the same time.\n\n\nserverless deploy -s \n[\nSTAGE NAME\n]\n -r \n[\nREGION NAME\n]\n -v\n\n\n\n\n\n\n\nDeploy Function\n\n\n\n\nUse this to quickly overwrite your AWS Lambda code on AWS, allowing you to develop faster.\n\n\nserverless deploy \nfunction\n -f \n[\nFUNCTION NAME\n]\n -s \n[\nSTAGE NAME\n]\n -r \n[\nREGION NAME\n]\n\n\n\n\n\n\n\n\nInvoke Function\n\n\n\n\nInvokes an AWS Lambda Function on AWS and returns logs.\n\n\nserverless invoke -f \n[\nFUNCTION NAME\n]\n -s \n[\nSTAGE NAME\n]\n -r \n[\nREGION NAME\n]\n -l\n\n\n\n\n\n\n\nStreaming Logs\n\n\n\n\nOpen up a separate tab in your console and stream all logs for a specific Function using this command.\n\n\nserverless logs -f \n[\nFUNCTION NAME\n]\n -s \n[\nSTAGE NAME\n]\n -r \n[\nREGION NAME\n]",
            "title": "Serverless"
        },
        {
            "location": "/Cloud/Serverless/#serverless-cheatsheet",
            "text": "Serverless home page",
            "title": "Serverless Cheatsheet"
        },
        {
            "location": "/Cloud/Serverless/#install",
            "text": "npm install -g serverless",
            "title": "Install"
        },
        {
            "location": "/Cloud/Serverless/#examples",
            "text": "Serverless Examples  Serverless Starter  Python example  C# example",
            "title": "Examples"
        },
        {
            "location": "/Cloud/Serverless/#cheatsheet",
            "text": "Create a Service:   # NodeJS \nserverless create -p  [ SERVICE NAME ]  -t aws-nodejs # C# \nserverless create --path serverlessCSharp --template aws-csharp   Install a Service   This is a convenience method to install a pre-made Serverless Service locally by downloading the Github repo and unzipping it.  serverless install -u  [ GITHUB URL OF SERVICE ]    Deploy All   Use this when you have made changes to your Functions, Events or Resources in  serverless.yml  or you simply want to deploy all changes within your Service at the same time.  serverless deploy -s  [ STAGE NAME ]  -r  [ REGION NAME ]  -v   Deploy Function   Use this to quickly overwrite your AWS Lambda code on AWS, allowing you to develop faster.  serverless deploy  function  -f  [ FUNCTION NAME ]  -s  [ STAGE NAME ]  -r  [ REGION NAME ]    Invoke Function   Invokes an AWS Lambda Function on AWS and returns logs.  serverless invoke -f  [ FUNCTION NAME ]  -s  [ STAGE NAME ]  -r  [ REGION NAME ]  -l   Streaming Logs   Open up a separate tab in your console and stream all logs for a specific Function using this command.  serverless logs -f  [ FUNCTION NAME ]  -s  [ STAGE NAME ]  -r  [ REGION NAME ]",
            "title": "Cheatsheet"
        },
        {
            "location": "/Data_Science/Data_Manipulation/",
            "text": "Pandas\n\u00b6\n\n\n\n\nPandas cheatsheet\n\n\nPandas cheatsheet 2\n\n\n\n\nData Wrangling with .NET\n\u00b6\n\n\nQuora: Which is the best machine learning library for .NET?\n\n\nDeedle- Exploratory data library for .NET\n\n\nDeedle is an easy to use library for data and time series manipulation and for scientific programming. It supports working with structured data frames, ordered and unordered data, as well as time series. Deedle is designed to work well for exploratory programming using F# and C# interactive console, but can be also used in efficient compiled .NET code.\n\n\nThe library implements a wide range of operations for data manipulation including advanced indexing and slicing, joining and aligning data, handling of missing values, grouping and aggregation, statistics and more.\n\n\nAccord.NET Framework\n\n\nAccord.NET provides statistical analysis, machine learning, image processing and computer vision methods for .NET applications. The Accord.NET Framework extends the popular AForge.NET with new features, adding to a more complete environment for scientific computing in .NET.",
            "title": "Data Manipulation"
        },
        {
            "location": "/Data_Science/Data_Manipulation/#pandas",
            "text": "Pandas cheatsheet  Pandas cheatsheet 2",
            "title": "Pandas"
        },
        {
            "location": "/Data_Science/Data_Manipulation/#data-wrangling-with-net",
            "text": "Quora: Which is the best machine learning library for .NET?  Deedle- Exploratory data library for .NET  Deedle is an easy to use library for data and time series manipulation and for scientific programming. It supports working with structured data frames, ordered and unordered data, as well as time series. Deedle is designed to work well for exploratory programming using F# and C# interactive console, but can be also used in efficient compiled .NET code.  The library implements a wide range of operations for data manipulation including advanced indexing and slicing, joining and aligning data, handling of missing values, grouping and aggregation, statistics and more.  Accord.NET Framework  Accord.NET provides statistical analysis, machine learning, image processing and computer vision methods for .NET applications. The Accord.NET Framework extends the popular AForge.NET with new features, adding to a more complete environment for scientific computing in .NET.",
            "title": "Data Wrangling with .NET"
        },
        {
            "location": "/Data_Science/Data_Visualization/",
            "text": "Basics\n\u00b6\n\n\nData visualization - Wikipedia\n\n\n19 Tools for Data Visualization Projects\n\n\n22 free tools for data visualization and analysis - Computerworld\n\n\n22 free tools for data visualization and analysis\n\n\nJavaScript libraries / APIs\n\u00b6\n\n\nD3.js - Data-Driven Documents\n\n\nD3 provides many built-in reusable functions and function factories, such as graphical primitives for area, line and pie charts.\n\n\nD3 building blocks\n\n\nC3.js\n\n\nGoogle Charts\n\n\nTools\n\u00b6\n\n\nplot.ly\n\n\nTableau\n\n\nQlik\n\n\nQuadrigram",
            "title": "Data Visualization"
        },
        {
            "location": "/Data_Science/Data_Visualization/#basics",
            "text": "Data visualization - Wikipedia  19 Tools for Data Visualization Projects  22 free tools for data visualization and analysis - Computerworld  22 free tools for data visualization and analysis",
            "title": "Basics"
        },
        {
            "location": "/Data_Science/Data_Visualization/#javascript-libraries-apis",
            "text": "D3.js - Data-Driven Documents  D3 provides many built-in reusable functions and function factories, such as graphical primitives for area, line and pie charts.  D3 building blocks  C3.js  Google Charts",
            "title": "JavaScript libraries / APIs"
        },
        {
            "location": "/Data_Science/Data_Visualization/#tools",
            "text": "plot.ly  Tableau  Qlik  Quadrigram",
            "title": "Tools"
        },
        {
            "location": "/Data_Science/Machine_Learning/",
            "text": "Useful Links\n\u00b6\n\n\nDataTau\n\n\nDatasets\n\u00b6\n\n\n\n\nUC Irvine Machine Learning Repository\n\n\nQuandl\n\n\nKaggle\n\n\n\n\nAlgorithms\n\u00b6\n\n\nRestricted Boltzmann Machines\n\u00b6\n\n\nA Beginner's Tutorial for Restricted Boltzmann Machines - Deeplearning4j- Open-source, Distributed Deep Learning for the JVM\n\n\nEmbedding\n\u00b6\n\n\nt-distributed stochastic neighbor embedding - Wikipedia\n\n\nReinforcement Learning\n\u00b6\n\n\nLecture 10 Reinforcement Learning I\n\n\nPyBrain\n\n\nPyBrain - a simple neural networks library in Python\n\n\nCyBrain\n\n\nML Plaforms\n\u00b6\n\n\nPalladium\n\n\nGUI tools\n\u00b6\n\n\n\n\nOrange\n\n\nProvides a design tool for visual programming allowing you to connect together data preparation, algorithms, and result evaluation\ntogether to create machine learning \u201cprograms\u201d. Provides over 100 widgets for the environment and also provides a Python API and library for\nintegrating into your application.\n\n\n\n\n\n\nWeka explorer\n\n\nA graphical machine learning workbench. It provides an explorer that you can use to prepare data, run algorithms and review results. It\nalso provides an experimenter where you can perform the same tasks in a controlled environment and design a batch of algorithm runs that could\nrun for an extended period of time and then review the results. Finally, it also provides a data flow interface where you can plug algorithms\ntogether like a flow diagram. Under the covers you can use Weka as a Java library and write programs that make use of the algorithms.\n\n\n\n\n\n\nBigML\n\n\nA web service where you can upload your data, prepare it and run algorithms on it. It provides clean and easy to use interfaces for\nconfiguring algorithms (decision trees) and reviewing the results. The best feature of this service is that it is all in the cloud, meaning that\nall you need is a web browser to get started. It also provides an API so that if you like it you can build an application around it.\n\n\n\n\n\n\n\n\nTutorials\n\u00b6\n\n\n\n\nmachinelearningmastery.com\n\n\nKaggle\n\n\n\n\nBooks\n\u00b6\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\n\nBoosting Foundations and Algorithms",
            "title": "Machine Learning"
        },
        {
            "location": "/Data_Science/Machine_Learning/#useful-links",
            "text": "DataTau",
            "title": "Useful Links"
        },
        {
            "location": "/Data_Science/Machine_Learning/#datasets",
            "text": "UC Irvine Machine Learning Repository  Quandl  Kaggle",
            "title": "Datasets"
        },
        {
            "location": "/Data_Science/Machine_Learning/#algorithms",
            "text": "",
            "title": "Algorithms"
        },
        {
            "location": "/Data_Science/Machine_Learning/#restricted-boltzmann-machines",
            "text": "A Beginner's Tutorial for Restricted Boltzmann Machines - Deeplearning4j- Open-source, Distributed Deep Learning for the JVM",
            "title": "Restricted Boltzmann Machines"
        },
        {
            "location": "/Data_Science/Machine_Learning/#embedding",
            "text": "t-distributed stochastic neighbor embedding - Wikipedia",
            "title": "Embedding"
        },
        {
            "location": "/Data_Science/Machine_Learning/#reinforcement-learning",
            "text": "Lecture 10 Reinforcement Learning I  PyBrain  PyBrain - a simple neural networks library in Python  CyBrain",
            "title": "Reinforcement Learning"
        },
        {
            "location": "/Data_Science/Machine_Learning/#ml-plaforms",
            "text": "Palladium",
            "title": "ML Plaforms"
        },
        {
            "location": "/Data_Science/Machine_Learning/#gui-tools",
            "text": "Orange  Provides a design tool for visual programming allowing you to connect together data preparation, algorithms, and result evaluation\ntogether to create machine learning \u201cprograms\u201d. Provides over 100 widgets for the environment and also provides a Python API and library for\nintegrating into your application.    Weka explorer  A graphical machine learning workbench. It provides an explorer that you can use to prepare data, run algorithms and review results. It\nalso provides an experimenter where you can perform the same tasks in a controlled environment and design a batch of algorithm runs that could\nrun for an extended period of time and then review the results. Finally, it also provides a data flow interface where you can plug algorithms\ntogether like a flow diagram. Under the covers you can use Weka as a Java library and write programs that make use of the algorithms.    BigML  A web service where you can upload your data, prepare it and run algorithms on it. It provides clean and easy to use interfaces for\nconfiguring algorithms (decision trees) and reviewing the results. The best feature of this service is that it is all in the cloud, meaning that\nall you need is a web browser to get started. It also provides an API so that if you like it you can build an application around it.",
            "title": "GUI tools"
        },
        {
            "location": "/Data_Science/Machine_Learning/#tutorials",
            "text": "machinelearningmastery.com  Kaggle",
            "title": "Tutorials"
        },
        {
            "location": "/Data_Science/Machine_Learning/#books",
            "text": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction  Boosting Foundations and Algorithms",
            "title": "Books"
        },
        {
            "location": "/Databases/Mongodb/",
            "text": "Import from CSV\n\u00b6\n\n\nmongoimport --db users --collection contacts --type csv --headerline --file contacts.csv\n\n\n\n\n\nSpecifying \n--headerline\n instructs mongoimport to determine the name of the fields using the first line in the CSV file.\nUse the \n--ignoreBlanks\n option to ignore blank fields. For CSV and TSV imports, this option provides the desired functionality in most cases, because it avoids inserting fields with null values into your collection.\n\n\nMongoImport documentation\n\n\nPrint from a Cursor\n\u00b6\n\n\nmyCursor\n.\nforEach\n(\nprintjson\n);\n\n\n\n// or\n\n\nwhile\n \n(\nmyCollection\n.\nhasNext\n())\n \n{\n\n   \nprintjson\n(\nmyCollection\n.\nnext\n());\n\n\n}\n \n\n\n\n\n\nAggregation Tips\n\u00b6\n\n\n// lowercase a string \n\n\n{\n \n$project\n:\n \n{\n \n\"address\"\n:\n \n{\n \n$toLower\n:\n \n\"$address\"\n \n}\n \n}\n \n},\n\n\n\n// extract field within embedded document\n\n\n{\n \n$project\n:\n \n{\n \n\"experience.location\"\n:\n \n1\n \n}\n \n},\n\n\n\n// flatten \n\n\n{\n \n$unwind\n:\n \n\"$experience\"\n},\n\n\n{\n \n$group\n:\n \n{\n \n_id\n:\n \n\"$_id\"\n,\n \nlocs\n:\n \n{\n \n$push\n:\n \n{\n \n$ifNull\n:\n \n[\n \n\"$experience.location\"\n,\n \n\"undefined\"\n \n]\n \n}\n \n}\n \n}\n \n}\n\n\n\n// output a collection\n\n\n{\n \n$out\n:\n \n\"myCollection2\"\n \n}\n\n\n\n// get unique values \n\n\n{\n \n$group\n:\n \n{\n \n_id\n:\n \n\"$fulladdress\"\n \n}\n \n}\n\n\n\n\n\n\nMake a Copy\n\u00b6\n\n\nDon't use copyTo - it is fully blocking... and deprecated in 3.x\n\n\n\n\nUse the Aggregation framework:\n\n\n\n\ndb\n \n=\n \ndb\n.\ngetSiblingDB\n(\n\"myDB\"\n);\n \n// set current db for $out\n\n\nvar\n \nmyCollection\n \n=\n \ndb\n.\ngetCollection\n(\n\"myCollection\"\n);\n\n\n\n// project if needed, get uniques if needed, create a new collection\n\n\nmyCollection\n.\naggregate\n([{\n \n$project\n:\n{\n \n\"fulladdress\"\n:\n \n1\n \n}\n \n},{\n \n$group\n:\n{\n \n_id\n:\n \n\"$fulladdress\"\n \n}\n \n},{\n \n$out\n:\n \n\"outputCollection\"\n \n}],\n \n{\n \nallowDiskUse\n:\ntrue\n \n});\n \n\n\n\n\n\n\n\nOr use bulk update:\n\n\n\n\nvar\n \noutputColl\n \n=\n \ndb\n.\ngetCollection\n(\n \n\"outputCollection\"\n \n);\n\n\nvar\n \noutputBulk\n \n=\n \noutputColl\n.\ninitializeUnorderedBulkOp\n();\n\n\nmyCollection\n.\nfind\n(\n \n{},\n \n{\n \n\"fulladdress\"\n:\n \n1\n \n}\n \n).\nforEach\n(\n \nfunction\n(\ndoc\n)\n \n{\n\n     \noutputBulk\n.\ninsert\n(\ndoc\n);\n\n\n});\n\n\noutputBulk\n.\nexecute\n();\n\n\n\n\n\n\nLonger Example\n\u00b6\n\n\nAdd a count field to all records\n\n\nfunction\n \ngatherStats\n()\n \n{\n   \n    \nvar\n \nstart\n \n=\n \nDate\n.\nnow\n();\n\n\n    \nvar\n \ninputDB\n \n=\n \ndb\n.\ngetSiblingDB\n(\n\"inputDB\"\n);\n\n    \nvar\n \ninputColl\n \n=\n \ninputDB\n.\ngetCollection\n(\n\"inputColl\"\n);\n\n\n    \n// debug: inputColl.find( {} ).limit(2).forEach(printjson);  \n\n\n    \noutputDB\n \n=\n \ndb\n.\ngetSiblingDB\n(\n\"outputDB\"\n);\n \n    \ndb\n \n=\n \noutputDB\n;\n \n// set current database for the next aggregate step\n\n\n    \n// create temporary collection with count\n\n    \ninputColl\n.\naggregate\n(\n  \n[\n \n    \n{\n \n$group\n:\n \n{\n \n_id\n:\n \n{\n \n$toLower\n:\n \n\"$address\"\n \n},\n \ncount\n:\n \n{\n \n$sum\n:\n \n1\n \n}\n \n}\n \n},\n \n    \n{\n \n$sort\n:\n \n{\n \n\"count\"\n:\n \n-\n1\n \n}\n \n},\n\n    \n{\n \n$limit\n:\n \n100000\n \n},\n                 \n// limit to 100k addresses with highest count  \n\n    \n{\n \n$out\n:\n \n\"stats\"\n \n}\n\n    \n],\n  \n{\n \nallowDiskUse\n:\n \ntrue\n \n}\n \n);\n       \n// returns { _id, count } where _id is the address\n\n\n    \nvar\n \nstatsColl\n \n=\n \noutputDB\n.\ngetCollection\n(\n\"stats\"\n);\n\n\n   \n// create output collection\n\n    \nvar\n \noutputColl\n \n=\n \noutputDB\n.\ngetCollection\n(\n\"outputColl\"\n);\n \n    \nvar\n \noutputBulk\n \n=\n \noutputColl\n.\ninitializeUnorderedBulkOp\n();\n \n    \nvar\n \ncounter\n \n=\n \n0\n;\n \n\n    \nvar\n \ninputCursor\n \n=\n \ninputColl\n.\nfind\n(\n \n{},\n \n{}\n \n);\n \n    \ninputCursor\n.\nforEach\n(\n \nfunction\n(\ndoc\n)\n \n{\n \n        \nvar\n \nstatDoc\n \n=\n \nstatsColl\n.\nfindOne\n(\n \n{\n \n_id\n:\n \ndoc\n.\naddress\n \n}\n \n);\n\n        \nif\n \n(\nstatDoc\n)\n \n{\n\n            \ndoc\n.\ncount\n \n=\n \nstatDoc\n.\ncount\n;\n\n            \noutputBulk\n.\ninsert\n(\ndoc\n);\n\n            \ncounter\n++\n;\n  \n            \nif\n \n(\n \ncounter\n \n%\n \n1000\n \n==\n \n0\n \n)\n \n{\n\n                    \noutputBulk\n.\nexecute\n();\n\n                    \n// you have to reset\n\n                    \noutputBulk\n \n=\n \noutputColl\n.\ninitializeUnorderedBulkOp\n();\n \n                \n}\n\n            \n}\n\n        \n}\n\n    \n);\n\n\n    \nif\n \n(\n \ncounter\n \n%\n \n1000\n \n>\n \n0\n \n)\n\n        \noutputBulk\n.\nexecute\n();\n\n\n\n    \n// print the results\n\n    \noutputColl\n.\nfind\n({}).\nsort\n({\ncount\n:\n \n-\n1\n}).\nforEach\n(\nprintjson\n);\n \n\n    \nvar\n \nend\n \n=\n \nDate\n.\nnow\n();\n\n    \nvar\n \nduration\n \n=\n \n(\nend\n \n-\n \nstart\n)\n/\n1000\n;\n\n    \nprintjson\n(\n\"Duration: \"\n \n+\n \nduration\n \n+\n \n\" seconds\"\n);\n\n\n    \nprintjson\n(\n\" | DONE | \"\n);\n\n\n}\n\n\n\ngatherStats\n();\n\n\n\n\n\n\nAlternatively move data to memory:\n\n\n    \nvar\n \nstatsDict\n \n=\n \n{};\n \n// or better Object.create(null);    \n\n    \nstatsColl\n.\nfind\n({}).\nforEach\n(\n \nfunction\n(\ndoc\n)\n \n{\n \nstatsDict\n[\ndoc\n.\n_id\n]\n \n=\n \ndoc\n.\ncount\n \n}\n \n);\n\n\n    \n// could also use: var statsArray = statsCursor.toArray();\n\n\n    \ninputCursor\n.\nforEach\n(\n \nfunction\n(\ndoc\n)\n \n{\n\n        \nif\n \n(\ndoc\n.\naddress\n \nin\n \nstatsDict\n)\n\n        \n{\n \n            \ndoc\n[\n\"count\"\n]\n \n=\n \nstatsDict\n[\ndoc\n.\naddress\n];\n \n            \noutputBulk\n.\ninsert\n(\ndoc\n);\n \n        \n}\n\n    \n});\n\n    \noutputBulk\n.\nexecute\n();",
            "title": "Mongodb"
        },
        {
            "location": "/Databases/Mongodb/#import-from-csv",
            "text": "mongoimport --db users --collection contacts --type csv --headerline --file contacts.csv  Specifying  --headerline  instructs mongoimport to determine the name of the fields using the first line in the CSV file.\nUse the  --ignoreBlanks  option to ignore blank fields. For CSV and TSV imports, this option provides the desired functionality in most cases, because it avoids inserting fields with null values into your collection.  MongoImport documentation",
            "title": "Import from CSV"
        },
        {
            "location": "/Databases/Mongodb/#print-from-a-cursor",
            "text": "myCursor . forEach ( printjson );  // or  while   ( myCollection . hasNext ())   { \n    printjson ( myCollection . next ());  }",
            "title": "Print from a Cursor"
        },
        {
            "location": "/Databases/Mongodb/#aggregation-tips",
            "text": "// lowercase a string   {   $project :   {   \"address\" :   {   $toLower :   \"$address\"   }   }   },  // extract field within embedded document  {   $project :   {   \"experience.location\" :   1   }   },  // flatten   {   $unwind :   \"$experience\" },  {   $group :   {   _id :   \"$_id\" ,   locs :   {   $push :   {   $ifNull :   [   \"$experience.location\" ,   \"undefined\"   ]   }   }   }   }  // output a collection  {   $out :   \"myCollection2\"   }  // get unique values   {   $group :   {   _id :   \"$fulladdress\"   }   }",
            "title": "Aggregation Tips"
        },
        {
            "location": "/Databases/Mongodb/#make-a-copy",
            "text": "Don't use copyTo - it is fully blocking... and deprecated in 3.x   Use the Aggregation framework:   db   =   db . getSiblingDB ( \"myDB\" );   // set current db for $out  var   myCollection   =   db . getCollection ( \"myCollection\" );  // project if needed, get uniques if needed, create a new collection  myCollection . aggregate ([{   $project : {   \"fulladdress\" :   1   }   },{   $group : {   _id :   \"$fulladdress\"   }   },{   $out :   \"outputCollection\"   }],   {   allowDiskUse : true   });     Or use bulk update:   var   outputColl   =   db . getCollection (   \"outputCollection\"   );  var   outputBulk   =   outputColl . initializeUnorderedBulkOp ();  myCollection . find (   {},   {   \"fulladdress\" :   1   }   ). forEach (   function ( doc )   { \n      outputBulk . insert ( doc );  });  outputBulk . execute ();",
            "title": "Make a Copy"
        },
        {
            "location": "/Databases/Mongodb/#longer-example",
            "text": "Add a count field to all records  function   gatherStats ()   {    \n     var   start   =   Date . now (); \n\n     var   inputDB   =   db . getSiblingDB ( \"inputDB\" ); \n     var   inputColl   =   inputDB . getCollection ( \"inputColl\" ); \n\n     // debug: inputColl.find( {} ).limit(2).forEach(printjson);   \n\n     outputDB   =   db . getSiblingDB ( \"outputDB\" );  \n     db   =   outputDB ;   // set current database for the next aggregate step \n\n     // create temporary collection with count \n     inputColl . aggregate (    [  \n     {   $group :   {   _id :   {   $toLower :   \"$address\"   },   count :   {   $sum :   1   }   }   },  \n     {   $sort :   {   \"count\" :   - 1   }   }, \n     {   $limit :   100000   },                   // limit to 100k addresses with highest count   \n     {   $out :   \"stats\"   } \n     ],    {   allowDiskUse :   true   }   );         // returns { _id, count } where _id is the address \n\n     var   statsColl   =   outputDB . getCollection ( \"stats\" ); \n\n    // create output collection \n     var   outputColl   =   outputDB . getCollection ( \"outputColl\" );  \n     var   outputBulk   =   outputColl . initializeUnorderedBulkOp ();  \n     var   counter   =   0 ;  \n\n     var   inputCursor   =   inputColl . find (   {},   {}   );  \n     inputCursor . forEach (   function ( doc )   {  \n         var   statDoc   =   statsColl . findOne (   {   _id :   doc . address   }   ); \n         if   ( statDoc )   { \n             doc . count   =   statDoc . count ; \n             outputBulk . insert ( doc ); \n             counter ++ ;   \n             if   (   counter   %   1000   ==   0   )   { \n                     outputBulk . execute (); \n                     // you have to reset \n                     outputBulk   =   outputColl . initializeUnorderedBulkOp ();  \n                 } \n             } \n         } \n     ); \n\n     if   (   counter   %   1000   >   0   ) \n         outputBulk . execute (); \n\n\n     // print the results \n     outputColl . find ({}). sort ({ count :   - 1 }). forEach ( printjson );  \n\n     var   end   =   Date . now (); \n     var   duration   =   ( end   -   start ) / 1000 ; \n     printjson ( \"Duration: \"   +   duration   +   \" seconds\" ); \n\n     printjson ( \" | DONE | \" );  }  gatherStats ();   Alternatively move data to memory:       var   statsDict   =   {};   // or better Object.create(null);     \n     statsColl . find ({}). forEach (   function ( doc )   {   statsDict [ doc . _id ]   =   doc . count   }   ); \n\n     // could also use: var statsArray = statsCursor.toArray(); \n\n     inputCursor . forEach (   function ( doc )   { \n         if   ( doc . address   in   statsDict ) \n         {  \n             doc [ \"count\" ]   =   statsDict [ doc . address ];  \n             outputBulk . insert ( doc );  \n         } \n     }); \n     outputBulk . execute ();",
            "title": "Longer Example"
        },
        {
            "location": "/Databases/SQL/",
            "text": "SQL Cheatsheet\n\u00b6\n\n\nDML: SELECT\n\u00b6\n\n\nFilter\n:\n\n\nSELECT\n \nLastName\n,\n \nFirstName\n,\n \nAddress\n \nFROM\n \nPersons\n\n\nWHERE\n \nAddress\n \nIS\n \nNULL\n\n\n\n\n\n\nLike\n:\n\n\nSELECT\n \n*\n \nFROM\n \nCustomers\n\n\nWHERE\n \nCity\n \nLIKE\n \n's%'\n;\n\n\n\nSELECT\n \n*\n \nFROM\n \nCustomers\n\n\nWHERE\n \nCountry\n \nLIKE\n \n'%land%'\n;\n\n\n\nSELECT\n \n*\n \nFROM\n \nCustomers\n\n\nWHERE\n \nCountry\n \nNOT\n \nLIKE\n \n'%land%'\n;\n\n\n\n\n\n\nSort\n:\n\n\nSELECT\n \n*\n \nFROM\n \nCustomers\n\n\nORDER\n \nBY\n \nCountry\n \nDESC\n;\n\n\n\nSELECT\n \n*\n \nFROM\n \nCustomers\n\n\nORDER\n \nBY\n \nCountry\n,\n \nCustomerName\n;\n\n\n\n\n\n\nLimit\n:\n\n\nSELECT\n \nTOP\n \nnumber\n|\npercent\n \ncolumn_name\n(\ns\n)\n\n\nFROM\n \ntable_name\n;\n\n\n\n-- Examples:\n\n\nSELECT\n \nTOP\n \n2\n \n*\n \nFROM\n \nCustomers\n;\n\n\n\nSELECT\n \nTOP\n \n50\n \nPERCENT\n \n*\n \nFROM\n \nCustomers\n;\n\n\n\n\n\n\nOracle Syntax\n:\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n\n\nFROM\n \ntable_name\n\n\nWHERE\n \nROWNUM\n \n<=\n \nnumber\n;\n\n\n\n\n\n\nJoins\n:\n\n\nSELECT\n \nCustomers\n.\nCustomerName\n,\n \nOrders\n.\nOrderID\n\n\nFROM\n \nCustomers\n\n\nFULL\n \nOUTER\n \nJOIN\n \nOrders\n\n\nON\n \nCustomers\n.\nCustomerID\n \n=\n \nOrders\n.\nCustomerID\n\n\nORDER\n \nBY\n \nCustomers\n.\nCustomerName\n;\n\n\n\n\n\n\nUnion\n:\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable1\n\n\nUNION\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable2\n;\n\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable1\n\n\nUNION\n \nALL\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable2\n;\n\n\n\n\n\n\nSelect Into\n:\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n\n\nINTO\n \nnewtable\n \n[\nIN\n \nexternaldb\n]\n\n\nFROM\n \ntable1\n;\n\n\n\n\n\n\nFormula\n:\n\n\nSELECT\n \nProductName\n,\n \nUnitPrice\n*\n(\nUnitsInStock\n+\nISNULL\n(\nUnitsOnOrder\n,\n0\n))\n\n\nFROM\n \nProducts\n\n\n\n\n\n\nDML: INSERT\n\u00b6\n\n\nINSERT\n \nINTO\n \ntable_name\n\n\nVALUES\n \n(\nvalue1\n,\nvalue2\n,\nvalue3\n,...);\n\n\n\nINSERT\n \nINTO\n \ntable_name\n \n(\ncolumn1\n,\ncolumn2\n,\ncolumn3\n,...)\n\n\nVALUES\n \n(\nvalue1\n,\nvalue2\n,\nvalue3\n,...);\n\n\n\n-- Example:\n\n\n\nINSERT\n \nINTO\n \nCustomers\n \n(\nCustomerName\n,\n \nCity\n,\n \nCountry\n)\n\n\nVALUES\n \n(\n'Cardinal'\n,\n \n'Stavanger'\n,\n \n'Norway'\n);\n\n\n\n\n\n\nInsert from select\n:\n\n\nINSERT\n \nINTO\n \ntable2\n(\ncolumn_name\n(\ns\n))\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n\n\nFROM\n \ntable1\n;\n\n\n\n-- Example:\n\n\n\nINSERT\n \nINTO\n \nCustomers\n \n(\nCustomerName\n,\n \nCountry\n)\n\n\nSELECT\n \nSupplierName\n,\n \nCountry\n \nFROM\n \nSuppliers\n\n\nWHERE\n \nCountry\n=\n'Germany'\n;\n\n\n\n\n\n\nDML: UPDATE\n\u00b6\n\n\nUPDATE\n \ntable_name\n\n\nSET\n \ncolumn1\n=\nvalue1\n,\ncolumn2\n=\nvalue2\n,...\n\n\nWHERE\n \nsome_column\n=\nsome_value\n;\n\n\n\n-- Example:\n\n\n\nUPDATE\n \nCustomers\n\n\nSET\n \nContactName\n=\n'Alfred Schmidt'\n,\n \nCity\n=\n'Hamburg'\n\n\nWHERE\n \nCustomerName\n=\n'Alfreds Futterkiste'\n;\n\n\n\n\n\n\nDML: DELETE\n\u00b6\n\n\nDELETE\n \nFROM\n \ntable_name\n\n\nWHERE\n \nsome_column\n=\nsome_value\n;\n\n\n\nDELETE\n \nFROM\n \nCustomers\n\n\nWHERE\n \nCustomerName\n=\n'Alfreds Futterkiste'\n \nAND\n \nContactName\n=\n'Maria Anders'\n;\n\n\n\n\n\n\nDatabases\n\u00b6\n\n\nCREATE\n \nDATABASE\n \nmy_db\n;\n\n\n\nDROP\n \nDATABASE\n \nmy_db\n;\n\n\n\n\n\n\nTables\n\u00b6\n\n\nCreate\n:\n\n\nCREATE\n \nTABLE\n \ntable_name\n\n\n(\n\n\ncolumn_name1\n \ndata_type\n(\nsize\n),\n\n\ncolumn_name2\n \ndata_type\n(\nsize\n),\n\n\ncolumn_name3\n \ndata_type\n(\nsize\n),\n\n\n....\n\n\n);\n\n\n\nCREATE\n \nTABLE\n \ntable_name\n\n\n(\n\n\ncolumn_name1\n \ndata_type\n(\nsize\n)\n \nconstraint_name\n,\n\n\ncolumn_name2\n \ndata_type\n(\nsize\n)\n \nconstraint_name\n,\n\n\ncolumn_name3\n \ndata_type\n(\nsize\n)\n \nconstraint_name\n,\n\n\n....\n\n\n);\n\n\n\n\n\n\n-- Examples\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n \nUNIQUE\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n)\n\n\n)\n\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n),\n\n\nCONSTRAINT\n \nuc_PersonID\n \nUNIQUE\n \n(\nP_Id\n,\n \nLastName\n)\n\n\n)\n\n\n\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nADD\n \nCONSTRAINT\n \nuc_PersonID\n \nUNIQUE\n \n(\nP_Id\n,\nLastName\n)\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nDROP\n \nCONSTRAINT\n \nuc_PersonID\n\n\n\n\n\n\nTemporary Table\n:\n\n\nCREATE\n \nTABLE\n \n#\nMyTempTable\n \n(\ncola\n \nINT\n \nPRIMARY\n \nKEY\n);\n\n\nINSERT\n \nINTO\n \n#\nMyTempTable\n \nVALUES\n \n(\n1\n);\n\n\n\n\n\n\nDrop / Truncate\n:\n\n\nDROP\n \nTABLE\n \ntable_name\n\n\n\nTRUNCATE\n \nTABLE\n \ntable_name\n\n\n\n\n\n\nPRIMARY KEY constraint\n\u00b6\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n \nPRIMARY\n \nKEY\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n)\n\n\n)\n\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n),\n\n\nCONSTRAINT\n \nPK_PersonID\n \nPRIMARY\n \nKEY\n \n(\nP_Id\n,\nLastName\n)\n\n\n)\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nADD\n \nCONSTRAINT\n \nPK_PersonID\n \nPRIMARY\n \nKEY\n \n(\nP_Id\n,\nLastName\n)\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nDROP\n \nCONSTRAINT\n \nPK_PersonID\n\n\n\n\n\n\nFOREIGN KEY constraints\n\u00b6\n\n\nCREATE\n \nTABLE\n \nOrders\n\n\n(\n\n\nO_Id\n \nint\n \nNOT\n \nNULL\n \nPRIMARY\n \nKEY\n,\n\n\nOrderNo\n \nint\n \nNOT\n \nNULL\n,\n\n\nP_Id\n \nint\n \nFOREIGN\n \nKEY\n \nREFERENCES\n \nPersons\n(\nP_Id\n)\n\n\n)\n\n\n\nCREATE\n \nTABLE\n \nOrders\n\n\n(\n\n\nO_Id\n \nint\n \nNOT\n \nNULL\n,\n\n\nOrderNo\n \nint\n \nNOT\n \nNULL\n,\n\n\nP_Id\n \nint\n,\n\n\nPRIMARY\n \nKEY\n \n(\nO_Id\n),\n\n\nCONSTRAINT\n \nFK_PerOrders\n \nFOREIGN\n \nKEY\n \n(\nP_Id\n)\n\n\nREFERENCES\n \nPersons\n(\nP_Id\n)\n\n\n)\n\n\n\n\n\n\nALTER\n \nTABLE\n \nOrders\n\n\nADD\n \nFOREIGN\n \nKEY\n \n(\nP_Id\n)\n\n\nREFERENCES\n \nPersons\n(\nP_Id\n)\n\n\n\nALTER\n \nTABLE\n \nOrders\n\n\nADD\n \nCONSTRAINT\n \nfk_PerOrders\n\n\nFOREIGN\n \nKEY\n \n(\nP_Id\n)\n\n\nREFERENCES\n \nPersons\n(\nP_Id\n)\n\n\n\nALTER\n \nTABLE\n \nOrders\n\n\nDROP\n \nCONSTRAINT\n \nfk_PerOrders\n\n\n\n\n\n\nCHECK Constraints\n\u00b6\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n \nCHECK\n \n(\nP_Id\n>\n0\n),\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n)\n\n\n)\n\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nP_Id\n \nint\n \nNOT\n \nNULL\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n),\n\n\nCONSTRAINT\n \nchk_Person\n \nCHECK\n \n(\nP_Id\n>\n0\n \nAND\n \nCity\n=\n'Sandnes'\n)\n\n\n)\n\n\n\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nADD\n \nCONSTRAINT\n \nCHK_Person\n \nCHECK\n \n(\nP_Id\n>\n0\n \nAND\n \nCity\n=\n'Sandnes'\n)\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nDROP\n \nCONSTRAINT\n \nCHK_Person\n\n\n\n\n\n\nDEFAULT Constraints\n\u00b6\n\n\nCREATE\n \nTABLE\n \nOrders\n\n\n(\n\n\nO_Id\n \nint\n \nNOT\n \nNULL\n,\n\n\nOrderNo\n \nint\n \nNOT\n \nNULL\n,\n\n\nP_Id\n \nint\n,\n\n\nOrderDate\n \ndate\n \nDEFAULT\n \nGETDATE\n()\n\n\n)\n\n\n\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nALTER\n \nCOLUMN\n \nCity\n \nSET\n \nDEFAULT\n \n'SEATTLE'\n\n\n\nALTER\n \nTABLE\n \nPersons\n\n\nALTER\n \nCOLUMN\n \nCity\n \nDROP\n \nDEFAULT\n\n\n\n\n\n\nIndex\n\u00b6\n\n\nCREATE\n \nUNIQUE\n \nINDEX\n \nindex_name\n\n\nON\n \ntable_name\n \n(\ncolumn_name\n)\n\n\n\nCREATE\n \nINDEX\n \nindex_name\n\n\nON\n \ntable_name\n \n(\ncolumn_name1\n,\n \ncol_name2\n)\n\n\n\n-- Example:\n\n\n\nCREATE\n \nINDEX\n \nPIndex\n\n\nON\n \nPersons\n \n(\nLastName\n,\n \nFirstName\n)\n\n\n\n\n\n\nDROP\n \nINDEX\n \ntable_name\n.\nindex_name\n\n\n\n-- Example:\n\n\n\nDROP\n \nINDEX\n \nIX_ProductVendor_BusinessEntityID\n\n    \nON\n \nPurchasing\n.\nProductVendor\n;\n\n\n\n\n\n\nAdd / drop / alter column in table\n\u00b6\n\n\nALTER\n \nTABLE\n \ntable_name\n\n\nADD\n \ncolumn_name\n \ndatatype\n\n\n\nALTER\n \nTABLE\n \ntable_name\n\n\nDROP\n \nCOLUMN\n \ncolumn_name\n\n\n\nALTER\n \nTABLE\n \ntable_name\n\n\nALTER\n \nCOLUMN\n \ncolumn_name\n \ndatatype\n\n\n\n\n\n\nAutoincrement\n\u00b6\n\n\nCREATE\n \nTABLE\n \nPersons\n\n\n(\n\n\nID\n \nint\n \nIDENTITY\n(\n1\n,\n1\n)\n \nPRIMARY\n \nKEY\n,\n\n\nLastName\n \nvarchar\n(\n255\n)\n \nNOT\n \nNULL\n,\n\n\nFirstName\n \nvarchar\n(\n255\n),\n\n\nAddress\n \nvarchar\n(\n255\n),\n\n\nCity\n \nvarchar\n(\n255\n)\n\n\n)\n\n\n\n\n\n\nExample:\n\n\nCREATE\n \nTABLE\n \ndbo\n.\nPurchaseOrderDetail\n\n\n(\n\n    \nPurchaseOrderID\n \nint\n \nNOT\n \nNULL\n\n        \nREFERENCES\n \nPurchasing\n.\nPurchaseOrderHeader\n(\nPurchaseOrderID\n),\n\n    \nLineNumber\n \nsmallint\n \nNOT\n \nNULL\n,\n\n    \nProductID\n \nint\n \nNULL\n\n        \nREFERENCES\n \nProduction\n.\nProduct\n(\nProductID\n),\n\n    \nUnitPrice\n \nmoney\n \nNULL\n,\n\n    \nOrderQty\n \nsmallint\n \nNULL\n,\n\n    \nReceivedQty\n \nfloat\n \nNULL\n,\n\n    \nRejectedQty\n \nfloat\n \nNULL\n,\n\n    \nDueDate\n \ndatetime\n \nNULL\n,\n\n    \nrowguid\n \nuniqueidentifier\n \nROWGUIDCOL\n  \nNOT\n \nNULL\n\n        \nCONSTRAINT\n \nDF_PurchaseOrderDetail_rowguid\n \nDEFAULT\n \n(\nnewid\n()),\n\n    \nModifiedDate\n \ndatetime\n \nNOT\n \nNULL\n\n        \nCONSTRAINT\n \nDF_PurchaseOrderDetail_ModifiedDate\n \nDEFAULT\n \n(\ngetdate\n()),\n\n    \nLineTotal\n  \nAS\n \n((\nUnitPrice\n*\nOrderQty\n)),\n\n    \nStockedQty\n  \nAS\n \n((\nReceivedQty\n-\nRejectedQty\n)),\n\n    \nCONSTRAINT\n \nPK_PurchaseOrderDetail_PurchaseOrderID_LineNumber\n\n              \nPRIMARY\n \nKEY\n \nCLUSTERED\n \n(\nPurchaseOrderID\n,\n \nLineNumber\n)\n\n              \nWITH\n \n(\nIGNORE_DUP_KEY\n \n=\n \nOFF\n)\n\n\n)\n\n\nON\n \nPRIMARY\n;\n\n\n\n\n\n\nViews\n\u00b6\n\n\nCREATE\n \nVIEW\n \nview_name\n \nAS\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n\n\nFROM\n \ntable_name\n\n\nWHERE\n \ncondition\n\n\n\n\n\n\nCREATE\n \nOR\n \nREPLACE\n \nVIEW\n \nview_name\n \nAS\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n\n\nFROM\n \ntable_name\n\n\nWHERE\n \ncondition\n\n\n\n\n\n\nDROP\n \nVIEW\n \nview_name\n\n\n\n\n\n\nExamples\n:\n\n\nCREATE\n \nVIEW\n \n[\nProducts\n \nAbove\n \nAverage\n \nPrice\n]\n \nAS\n\n\nSELECT\n \nProductName\n,\nUnitPrice\n\n\nFROM\n \nProducts\n\n\nWHERE\n \nUnitPrice\n \n>\n \n(\nSELECT\n \nAVG\n(\nUnitPrice\n)\n \nFROM\n \nProducts\n)\n\n\n\nSELECT\n \n*\n \nFROM\n \n[\nProducts\n \nAbove\n \nAverage\n \nPrice\n]\n\n\n\n\n\n\nCREATE\n \nVIEW\n \n[\nCategory\n \nSales\n \nFor\n \n1997\n]\n \nAS\n\n\nSELECT\n \nDISTINCT\n \nCategoryName\n,\n \nSum\n(\nProductSales\n)\n \nAS\n \nCategorySales\n\n\nFROM\n \n[\nProduct\n \nSales\n \nfor\n \n1997\n]\n\n\nGROUP\n \nBY\n \nCategoryName\n\n\n\n\n\n\nDates\n\u00b6\n\n\nGETDATE\n()\n  \n-- Returns the current date and time\n\n\n\nDATEPART\n()\n \n-- Returns a single part of a date/time\n\n\n\nDATEADD\n()\n  \n-- Adds or subtracts a specified time interval from a date\n\n\n\nDATEDIFF\n()\n \n-- Returns the time between two dates\n\n\n\nCONVERT\n()\n  \n-- Displays date/time data in different formats\n\n\n\n\n\n\nExample\n:\n\n\nCREATE\n \nTABLE\n \nOrders\n\n\n(\n\n\nOrderId\n \nint\n \nNOT\n \nNULL\n \nPRIMARY\n \nKEY\n,\n\n\nProductName\n \nvarchar\n(\n50\n)\n \nNOT\n \nNULL\n,\n\n\nOrderDate\n \ndatetime\n \nNOT\n \nNULL\n \nDEFAULT\n \nGETDATE\n()\n\n\n)\n\n\n\nSELECT\n \nDATEPART\n(\nyyyy\n,\nOrderDate\n)\n \nAS\n \nOrderYear\n,\n\n\nDATEPART\n(\nmm\n,\nOrderDate\n)\n \nAS\n \nOrderMonth\n,\n\n\nDATEPART\n(\ndd\n,\nOrderDate\n)\n \nAS\n \nOrderDay\n,\n\n\nFROM\n \nOrders\n\n\nWHERE\n \nOrderId\n=\n1\n\n\n\nSELECT\n \nOrderId\n,\nDATEADD\n(\nday\n,\n45\n,\nOrderDate\n)\n \nAS\n \nOrderPayDate\n\n\nFROM\n \nOrders\n\n\n\nSELECT\n \nDATEDIFF\n(\nday\n,\n'2008-06-05'\n,\n'2008-08-05'\n)\n \nAS\n \nDiffDate\n\n\n\nCONVERT\n(\nVARCHAR\n(\n19\n),\nGETDATE\n())\n\n\nCONVERT\n(\nVARCHAR\n(\n10\n),\nGETDATE\n(),\n10\n)\n\n\nCONVERT\n(\nVARCHAR\n(\n10\n),\nGETDATE\n(),\n110\n)\n\n\n\n\n\n\nSQL Server Data Types\n\u00b6\n\n\nData type / Description / Storage\n\n\nchar(n)\n\nFixed width character string. Maximum 8,000 characters\nDefined width\n\n\nvarchar(n)\n\nVariable width character string. Maximum 8,000 characters\n2 bytes + number of chars\n\n\nvarchar(max)\n\nVariable width character string. Maximum 1,073,741,824 characters\n2 bytes + number of chars\n\n\ntext\n\nVariable width character string. Maximum 2GB of text data\n4 bytes + number of chars\n\n\nnchar\n\nFixed width Unicode string. Maximum 4,000 characters\nDefined width x 2\n\n\nnvarchar\n\nVariable width Unicode string. Maximum 4,000 characters\n\n\nnvarchar(max)\n\nVariable width Unicode string. Maximum 536,870,912 characters\n\n\nntext\n\nVariable width Unicode string. Maximum 2GB of text data\n\n\nbit\n\nAllows 0, 1, or NULL\n\n\nbinary(n)\n\nFixed width binary string. Maximum 8,000 bytes\n\n\nvarbinary\n\nVariable width binary string. Maximum 8,000 bytes\n\n\nvarbinary(max)\n\nVariable width binary string. Maximum 2GB\n\n\nimage\n\nVariable width binary string. Maximum 2GB\n\n\nNumber types\n\u00b6\n\n\ntinyint\n\nAllows whole numbers from 0 to 255\n1 byte\n\n\nsmallint\n\nAllows whole numbers between -32,768 and 32,767\n2 bytes\n\n\nint\n\nAllows whole numbers between -2,147,483,648 and 2,147,483,647\n4 bytes\n\n\nbigint\n\nAllows whole numbers between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807\n8 bytes\n\n\ndecimal(p,s)\n\nFixed precision and scale numbers.\nAllows numbers from -10^38 +1 to 10^38.\n\n\nThe p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0.\n5-17 bytes\n\n\nnumeric(p,s)\n\nFixed precision and scale numbers.\nAllows numbers from -10^38 +1 to 10^38.\n\n\nThe p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0.\n5-17 bytes\n\n\nsmallmoney\n\nMonetary data from -214,748.3648 to 214,748.3647\n4 bytes\n\n\nmoney\n\nMonetary data from -922,337,203,685,477.5808 to 922,337,203,685,477.5807\n8 bytes\n\n\nfloat(n)\n\nFloating precision number data from -1.79E + 308 to 1.79E + 308.\nThe n parameter indicates whether the field should hold 4 or 8 bytes. float(24) holds a 4-byte field and float(53) holds an 8-byte field. Default value of n is 53.\n4 or 8 bytes\n\n\nreal\n\nFloating precision number data from -3.40E + 38 to 3.40E + 38\n4 bytes\n\n\nDate types\n\u00b6\n\n\ndatetime\n\nFrom January 1, 1753 to December 31, 9999 with an accuracy of 3.33 milliseconds\n8 bytes\n\n\ndatetime2\n\nFrom January 1, 0001 to December 31, 9999 with an accuracy of 100 nanoseconds\n6-8 bytes\n\n\nsmalldatetime\n\nFrom January 1, 1900 to June 6, 2079 with an accuracy of 1 minute\n4 bytes\n\n\ndate\n\nStore a date only. From January 1, 0001 to December 31, 9999\n3 bytes\n\n\ntime\n\nStore a time only to an accuracy of 100 nanoseconds\n3-5 bytes\n\n\ndatetimeoffset\n\nThe same as datetime2 with the addition of a time zone offset\n8-10 bytes\n\n\ntimestamp\n\nStores a unique number that gets updated every time a row gets created or modified. The timestamp value is based upon an internal clock and does not correspond to real time. Each table may have only one timestamp variable\n\n\nOther data types\n\u00b6\n\n\nsql_variant\n\nStores up to 8,000 bytes of data of various data types, except text, ntext, and timestamp\n\n\nuniqueidentifier\n\nStores a globally unique identifier (GUID)\n\n\nxml\n\nStores XML formatted data. Maximum 2GB\n\n\ncursor\n\nStores a reference to a cursor used for database operations\n\n\ntable\n\nStores a result-set for later processing\n\n\nSQL Aggregate Functions\n\u00b6\n\n\nSQL aggregate functions return a single value, calculated from values in a column.\n\n\nUseful aggregate functions:\n\n\n\n\nAVG()\n   - Returns the average value\n\n\nCOUNT()\n - Returns the number of rows\n\n\nTOP 1\n   - Single sample\n\n\nMAX()\n   - Returns the largest value\n\n\nMIN()\n   - Returns the smallest value\n\n\nSUM()\n   - Returns the sum\n\n\n\n\nExamples:\n\n\nSELECT\n \nCOUNT\n(\nDISTINCT\n \ncolumn_name\n)\n \nFROM\n \ntable_name\n;\n\n\n\nSELECT\n \nTOP\n \n1\n \ncolumn_name\n \nFROM\n \ntable_name\n\n\nORDER\n \nBY\n \ncolumn_name\n \nDESC\n;\n\n\n\nSELECT\n \ncolumn_name\n,\n \naggregate_function\n(\ncolumn_name\n)\n\n\nFROM\n \ntable_name\n\n\nWHERE\n \ncolumn_name\n \noperator\n \nvalue\n\n\nGROUP\n \nBY\n \ncolumn_name\n;\n\n\n\nSELECT\n \nShippers\n.\nShipperName\n,\n \nCOUNT\n(\nOrders\n.\nOrderID\n)\n \nAS\n \nNumberOfOrders\n\n\nFROM\n \nOrders\n\n\nLEFT\n \nJOIN\n \nShippers\n\n\nON\n \nOrders\n.\nShipperID\n=\nShippers\n.\nShipperID\n\n\nGROUP\n \nBY\n \nShipperName\n;\n\n\n\nSELECT\n \ncolumn_name\n,\n \naggregate_function\n(\ncolumn_name\n)\n\n\nFROM\n \ntable_name\n\n\nWHERE\n \ncolumn_name\n \noperator\n \nvalue\n\n\nGROUP\n \nBY\n \ncolumn_name\n\n\nHAVING\n \naggregate_function\n(\ncolumn_name\n)\n \noperator\n \nvalue\n;\n\n\n\nSELECT\n \nEmployees\n.\nLastName\n,\n \nCOUNT\n(\nOrders\n.\nOrderID\n)\n \nAS\n \nNumberOfOrders\n\n\nFROM\n \nOrders\n\n\nINNER\n \nJOIN\n \nEmployees\n\n\nON\n \nOrders\n.\nEmployeeID\n=\nEmployees\n.\nEmployeeID\n)\n\n\nGROUP\n \nBY\n \nLastName\n\n\nHAVING\n \nCOUNT\n(\nOrders\n.\nOrderID\n)\n \n>\n \n10\n;\n\n\n\n\n\n\nSQL Scalar functions\n\u00b6\n\n\n\n\nConverts a field to upper case: SELECT UPPER(column_name) FROM table_name;\n\n\nConverts a field to lower case: SELECT LOWER(column_name) FROM table_name;\n\n\nMID() - Extract characters from a text field\n\n\nLEN() - Returns the length of a text field\n\n\nROUND() - Rounds a numeric field to the number of decimals specified\n\n\nNOW() - Returns the current system date and time\n\n\nFORMAT() - Formats how a field is to be displayed\n\n\n\n\nSELECT\n \nProductName\n,\n \nROUND\n(\nPrice\n,\n0\n)\n \nAS\n \nRoundedPrice\n\n\nFROM\n \nProducts\n;\n\n\n\n\n\n\nVariables\n\u00b6\n\n\nDECLARE\n \n@\nmyvar\n \nchar\n(\n20\n);\n\n\nSET\n \n@\nmyvar\n \n=\n \n'This is a test'\n;\n\n\nSELECT\n \n@\nmyvar\n;\n\n\n\n\n\n\nScalar Function\n\u00b6\n\n\nCREATE\n \nFUNCTION\n \nFunctionName\n\n\n(\n\n\n-- Add the parameters for the function here\n\n\n@\np1\n \nint\n\n\n)\n\n\nRETURNS\n \nint\n\n\nAS\n\n\nBEGIN\n\n\n-- Declare the return variable here\n\n\nDECLARE\n \n@\nResult\n \nint\n\n\n-- Add the T-SQL statements to compute the return value here\n\n\nSELECT\n \n@\nResult\n \n=\n \n@\np1\n\n\n\n-- Return the result of the function\n\n\nRETURN\n \n@\nResult\n\n\nEND\n\n\n\n\n\n\nTable Value Function\n\u00b6\n\n\nIF\n \nOBJECT_ID\n \n(\nN\n'dbo.EmployeeByID'\n \n)\n \nIS\n \nNOT\n \nNULL\n\n   \nDROP\n \nFUNCTION\n \ndbo\n.\nEmployeeByID\n\n\nGO\n\n\n\nCREATE\n \nFUNCTION\n \ndbo\n.\nEmployeeByID\n(\n@\nInEmpID\n \nint\n)\n\n\nRETURNS\n \n@\nretFindReports\n \nTABLE\n\n\n(\n\n    \n-- columns returned by the function\n\n    \nEmployeeID\n \nint\n \nNOT\n \nNULL\n,\n\n    \nName\n \nnvarchar\n(\n255\n \n)\n \nNOT\n \nNULL\n,\n\n    \nTitle\n \nnvarchar\n(\n50\n \n)\n \nNOT\n \nNULL\n,\n\n    \nEmployeeLevel\n \nint\n \nNOT\n \nNULL\n\n\n)\n\n\nAS\n\n\n-- body of the function\n\n\nBEGIN\n\n   \nWITH\n \nDirectReports\n(\nName\n \n,\n \nTitle\n \n,\n \nEmployeeID\n \n,\n \nEmployeeLevel\n \n,\n \nSort\n \n)\n \nAS\n\n    \n(\nSELECT\n \nCONVERT\n(\n \nvarchar\n(\n255\n \n),\n \nc\n \n.\nFirstName\n \n+\n \n' '\n \n+\n \nc\n.\nLastName\n \n),\n\n        \ne\n.\nTitle\n \n,\n\n        \ne\n.\nEmployeeID\n \n,\n\n        \n1\n \n,\n\n        \nCONVERT\n(\nvarchar\n \n(\n255\n),\n \nc\n.\n \nFirstName\n \n+\n \n' '\n \n+\n \nc\n \n.\nLastName\n)\n\n     \nFROM\n \nHumanResources\n.\nEmployee\n \nAS\n \ne\n\n          \nJOIN\n \nPerson\n.\nContact\n \nAS\n \nc\n \nON\n \ne\n.\nContactID\n \n=\n \nc\n.\nContactID\n\n     \nWHERE\n \ne\n.\nEmployeeID\n \n=\n \n@\nInEmpID\n\n   \nUNION\n \nALL\n\n     \nSELECT\n \nCONVERT\n \n(\nvarchar\n(\n \n255\n),\n \nREPLICATE\n \n(\n \n'| '\n \n,\n \nEmployeeLevel\n)\n \n+\n\n        \nc\n.\nFirstName\n \n+\n \n' '\n \n+\n \nc\n.\n \nLastName\n),\n\n        \ne\n.\nTitle\n \n,\n\n        \ne\n.\nEmployeeID\n \n,\n\n        \nEmployeeLevel\n \n+\n \n1\n,\n\n        \nCONVERT\n \n(\n \nvarchar\n(\n255\n \n),\n \nRTRIM\n \n(\nSort\n)\n \n+\n \n'| '\n \n+\n \nFirstName\n \n+\n \n' '\n \n+\n\n                 \nLastName\n)\n\n     \nFROM\n \nHumanResources\n.\nEmployee\n \nas\n \ne\n\n          \nJOIN\n \nPerson\n.\nContact\n \nAS\n \nc\n \nON\n \ne\n.\nContactID\n \n=\n \nc\n.\nContactID\n\n          \nJOIN\n \nDirectReports\n \nAS\n \nd\n \nON\n \ne\n.\n \nManagerID\n \n=\n \nd\n.\n \nEmployeeID\n\n    \n)\n\n   \n-- copy the required columns to the result of the function\n\n\n   \nINSERT\n \n@\nretFindReports\n\n   \nSELECT\n \nEmployeeID\n,\n \nName\n,\n \nTitle\n,\n \nEmployeeLevel\n\n     \nFROM\n \nDirectReports\n\n   \nORDER\n \nBY\n \nSort\n\n   \nRETURN\n\n\nEND\n\n\nGO\n\n\n\n\n\n\nStored Procedure\n\u00b6\n\n\nCREATE\n \nPROCEDURE\n \nProcedureName\n\n        \n-- Add the parameters for the stored procedure here\n\n        \n@\np1\n \nint\n \n=\n \n0\n \n,\n\n        \n@\np2\n \nint\n \n=\n \n0\n\n\nAS\n\n\nBEGIN\n\n        \n-- SET NOCOUNT ON added to prevent extra result sets from\n\n        \n-- interfering with SELECT statements.\n\n        \nSET\n \nNOCOUNT\n \nON\n;\n\n\n    \n-- Insert statements for procedure here\n\n        \nSELECT\n \n@\np1\n \n,\n \n@\np2\n\n\nEND\n\n\nGO\n\n\n\n\n\n\nSelf-join\n\u00b6\n\n\nQ. Here's the data in a table 'orders'\n\n\ncustomer_id order_id order_day\n123        27424624    25Dec2011\n123        89690900    25Dec2010\n797        12131323    25Dec2010\n876        67145419    15Dec2011\n\n\n\n\n\nCould you give me SQL for customers who placed orders on both the days, 25th Dec 2010 and 25th Dec 2011?\n\n\n    \nSELECT\n \no\n.\ncustomer_id\n,\n \no\n.\norder_day\n\n    \nFROM\n \norders\n \nAS\n \no\n\n    \nINNER\n \nJOIN\n \norders\n \nAS\n \no1\n\n    \nON\n \no\n.\ncustomer_id\n \n=\n \no1\n.\ncustomer_id\n\n    \nWHERE\n \n...",
            "title": "SQL"
        },
        {
            "location": "/Databases/SQL/#sql-cheatsheet",
            "text": "",
            "title": "SQL Cheatsheet"
        },
        {
            "location": "/Databases/SQL/#dml-select",
            "text": "Filter :  SELECT   LastName ,   FirstName ,   Address   FROM   Persons  WHERE   Address   IS   NULL   Like :  SELECT   *   FROM   Customers  WHERE   City   LIKE   's%' ;  SELECT   *   FROM   Customers  WHERE   Country   LIKE   '%land%' ;  SELECT   *   FROM   Customers  WHERE   Country   NOT   LIKE   '%land%' ;   Sort :  SELECT   *   FROM   Customers  ORDER   BY   Country   DESC ;  SELECT   *   FROM   Customers  ORDER   BY   Country ,   CustomerName ;   Limit :  SELECT   TOP   number | percent   column_name ( s )  FROM   table_name ;  -- Examples:  SELECT   TOP   2   *   FROM   Customers ;  SELECT   TOP   50   PERCENT   *   FROM   Customers ;   Oracle Syntax :  SELECT   column_name ( s )  FROM   table_name  WHERE   ROWNUM   <=   number ;   Joins :  SELECT   Customers . CustomerName ,   Orders . OrderID  FROM   Customers  FULL   OUTER   JOIN   Orders  ON   Customers . CustomerID   =   Orders . CustomerID  ORDER   BY   Customers . CustomerName ;   Union :  SELECT   column_name ( s )   FROM   table1  UNION  SELECT   column_name ( s )   FROM   table2 ;  SELECT   column_name ( s )   FROM   table1  UNION   ALL  SELECT   column_name ( s )   FROM   table2 ;   Select Into :  SELECT   column_name ( s )  INTO   newtable   [ IN   externaldb ]  FROM   table1 ;   Formula :  SELECT   ProductName ,   UnitPrice * ( UnitsInStock + ISNULL ( UnitsOnOrder , 0 ))  FROM   Products",
            "title": "DML: SELECT"
        },
        {
            "location": "/Databases/SQL/#dml-insert",
            "text": "INSERT   INTO   table_name  VALUES   ( value1 , value2 , value3 ,...);  INSERT   INTO   table_name   ( column1 , column2 , column3 ,...)  VALUES   ( value1 , value2 , value3 ,...);  -- Example:  INSERT   INTO   Customers   ( CustomerName ,   City ,   Country )  VALUES   ( 'Cardinal' ,   'Stavanger' ,   'Norway' );   Insert from select :  INSERT   INTO   table2 ( column_name ( s ))  SELECT   column_name ( s )  FROM   table1 ;  -- Example:  INSERT   INTO   Customers   ( CustomerName ,   Country )  SELECT   SupplierName ,   Country   FROM   Suppliers  WHERE   Country = 'Germany' ;",
            "title": "DML: INSERT"
        },
        {
            "location": "/Databases/SQL/#dml-update",
            "text": "UPDATE   table_name  SET   column1 = value1 , column2 = value2 ,...  WHERE   some_column = some_value ;  -- Example:  UPDATE   Customers  SET   ContactName = 'Alfred Schmidt' ,   City = 'Hamburg'  WHERE   CustomerName = 'Alfreds Futterkiste' ;",
            "title": "DML: UPDATE"
        },
        {
            "location": "/Databases/SQL/#dml-delete",
            "text": "DELETE   FROM   table_name  WHERE   some_column = some_value ;  DELETE   FROM   Customers  WHERE   CustomerName = 'Alfreds Futterkiste'   AND   ContactName = 'Maria Anders' ;",
            "title": "DML: DELETE"
        },
        {
            "location": "/Databases/SQL/#databases",
            "text": "CREATE   DATABASE   my_db ;  DROP   DATABASE   my_db ;",
            "title": "Databases"
        },
        {
            "location": "/Databases/SQL/#tables",
            "text": "Create :  CREATE   TABLE   table_name  (  column_name1   data_type ( size ),  column_name2   data_type ( size ),  column_name3   data_type ( size ),  ....  );  CREATE   TABLE   table_name  (  column_name1   data_type ( size )   constraint_name ,  column_name2   data_type ( size )   constraint_name ,  column_name3   data_type ( size )   constraint_name ,  ....  );   -- Examples  CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL   UNIQUE ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 )  )  CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 ),  CONSTRAINT   uc_PersonID   UNIQUE   ( P_Id ,   LastName )  )   ALTER   TABLE   Persons  ADD   CONSTRAINT   uc_PersonID   UNIQUE   ( P_Id , LastName )  ALTER   TABLE   Persons  DROP   CONSTRAINT   uc_PersonID   Temporary Table :  CREATE   TABLE   # MyTempTable   ( cola   INT   PRIMARY   KEY );  INSERT   INTO   # MyTempTable   VALUES   ( 1 );   Drop / Truncate :  DROP   TABLE   table_name  TRUNCATE   TABLE   table_name",
            "title": "Tables"
        },
        {
            "location": "/Databases/SQL/#primary-key-constraint",
            "text": "CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL   PRIMARY   KEY ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 )  )  CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 ),  CONSTRAINT   PK_PersonID   PRIMARY   KEY   ( P_Id , LastName )  )  ALTER   TABLE   Persons  ADD   CONSTRAINT   PK_PersonID   PRIMARY   KEY   ( P_Id , LastName )  ALTER   TABLE   Persons  DROP   CONSTRAINT   PK_PersonID",
            "title": "PRIMARY KEY constraint"
        },
        {
            "location": "/Databases/SQL/#foreign-key-constraints",
            "text": "CREATE   TABLE   Orders  (  O_Id   int   NOT   NULL   PRIMARY   KEY ,  OrderNo   int   NOT   NULL ,  P_Id   int   FOREIGN   KEY   REFERENCES   Persons ( P_Id )  )  CREATE   TABLE   Orders  (  O_Id   int   NOT   NULL ,  OrderNo   int   NOT   NULL ,  P_Id   int ,  PRIMARY   KEY   ( O_Id ),  CONSTRAINT   FK_PerOrders   FOREIGN   KEY   ( P_Id )  REFERENCES   Persons ( P_Id )  )   ALTER   TABLE   Orders  ADD   FOREIGN   KEY   ( P_Id )  REFERENCES   Persons ( P_Id )  ALTER   TABLE   Orders  ADD   CONSTRAINT   fk_PerOrders  FOREIGN   KEY   ( P_Id )  REFERENCES   Persons ( P_Id )  ALTER   TABLE   Orders  DROP   CONSTRAINT   fk_PerOrders",
            "title": "FOREIGN KEY constraints"
        },
        {
            "location": "/Databases/SQL/#check-constraints",
            "text": "CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL   CHECK   ( P_Id > 0 ),  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 )  )  CREATE   TABLE   Persons  (  P_Id   int   NOT   NULL ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 ),  CONSTRAINT   chk_Person   CHECK   ( P_Id > 0   AND   City = 'Sandnes' )  )   ALTER   TABLE   Persons  ADD   CONSTRAINT   CHK_Person   CHECK   ( P_Id > 0   AND   City = 'Sandnes' )  ALTER   TABLE   Persons  DROP   CONSTRAINT   CHK_Person",
            "title": "CHECK Constraints"
        },
        {
            "location": "/Databases/SQL/#default-constraints",
            "text": "CREATE   TABLE   Orders  (  O_Id   int   NOT   NULL ,  OrderNo   int   NOT   NULL ,  P_Id   int ,  OrderDate   date   DEFAULT   GETDATE ()  )   ALTER   TABLE   Persons  ALTER   COLUMN   City   SET   DEFAULT   'SEATTLE'  ALTER   TABLE   Persons  ALTER   COLUMN   City   DROP   DEFAULT",
            "title": "DEFAULT Constraints"
        },
        {
            "location": "/Databases/SQL/#index",
            "text": "CREATE   UNIQUE   INDEX   index_name  ON   table_name   ( column_name )  CREATE   INDEX   index_name  ON   table_name   ( column_name1 ,   col_name2 )  -- Example:  CREATE   INDEX   PIndex  ON   Persons   ( LastName ,   FirstName )   DROP   INDEX   table_name . index_name  -- Example:  DROP   INDEX   IX_ProductVendor_BusinessEntityID \n     ON   Purchasing . ProductVendor ;",
            "title": "Index"
        },
        {
            "location": "/Databases/SQL/#add-drop-alter-column-in-table",
            "text": "ALTER   TABLE   table_name  ADD   column_name   datatype  ALTER   TABLE   table_name  DROP   COLUMN   column_name  ALTER   TABLE   table_name  ALTER   COLUMN   column_name   datatype",
            "title": "Add / drop / alter column in table"
        },
        {
            "location": "/Databases/SQL/#autoincrement",
            "text": "CREATE   TABLE   Persons  (  ID   int   IDENTITY ( 1 , 1 )   PRIMARY   KEY ,  LastName   varchar ( 255 )   NOT   NULL ,  FirstName   varchar ( 255 ),  Address   varchar ( 255 ),  City   varchar ( 255 )  )   Example:  CREATE   TABLE   dbo . PurchaseOrderDetail  ( \n     PurchaseOrderID   int   NOT   NULL \n         REFERENCES   Purchasing . PurchaseOrderHeader ( PurchaseOrderID ), \n     LineNumber   smallint   NOT   NULL , \n     ProductID   int   NULL \n         REFERENCES   Production . Product ( ProductID ), \n     UnitPrice   money   NULL , \n     OrderQty   smallint   NULL , \n     ReceivedQty   float   NULL , \n     RejectedQty   float   NULL , \n     DueDate   datetime   NULL , \n     rowguid   uniqueidentifier   ROWGUIDCOL    NOT   NULL \n         CONSTRAINT   DF_PurchaseOrderDetail_rowguid   DEFAULT   ( newid ()), \n     ModifiedDate   datetime   NOT   NULL \n         CONSTRAINT   DF_PurchaseOrderDetail_ModifiedDate   DEFAULT   ( getdate ()), \n     LineTotal    AS   (( UnitPrice * OrderQty )), \n     StockedQty    AS   (( ReceivedQty - RejectedQty )), \n     CONSTRAINT   PK_PurchaseOrderDetail_PurchaseOrderID_LineNumber \n               PRIMARY   KEY   CLUSTERED   ( PurchaseOrderID ,   LineNumber ) \n               WITH   ( IGNORE_DUP_KEY   =   OFF )  )  ON   PRIMARY ;",
            "title": "Autoincrement"
        },
        {
            "location": "/Databases/SQL/#views",
            "text": "CREATE   VIEW   view_name   AS  SELECT   column_name ( s )  FROM   table_name  WHERE   condition   CREATE   OR   REPLACE   VIEW   view_name   AS  SELECT   column_name ( s )  FROM   table_name  WHERE   condition   DROP   VIEW   view_name   Examples :  CREATE   VIEW   [ Products   Above   Average   Price ]   AS  SELECT   ProductName , UnitPrice  FROM   Products  WHERE   UnitPrice   >   ( SELECT   AVG ( UnitPrice )   FROM   Products )  SELECT   *   FROM   [ Products   Above   Average   Price ]   CREATE   VIEW   [ Category   Sales   For   1997 ]   AS  SELECT   DISTINCT   CategoryName ,   Sum ( ProductSales )   AS   CategorySales  FROM   [ Product   Sales   for   1997 ]  GROUP   BY   CategoryName",
            "title": "Views"
        },
        {
            "location": "/Databases/SQL/#dates",
            "text": "GETDATE ()    -- Returns the current date and time  DATEPART ()   -- Returns a single part of a date/time  DATEADD ()    -- Adds or subtracts a specified time interval from a date  DATEDIFF ()   -- Returns the time between two dates  CONVERT ()    -- Displays date/time data in different formats   Example :  CREATE   TABLE   Orders  (  OrderId   int   NOT   NULL   PRIMARY   KEY ,  ProductName   varchar ( 50 )   NOT   NULL ,  OrderDate   datetime   NOT   NULL   DEFAULT   GETDATE ()  )  SELECT   DATEPART ( yyyy , OrderDate )   AS   OrderYear ,  DATEPART ( mm , OrderDate )   AS   OrderMonth ,  DATEPART ( dd , OrderDate )   AS   OrderDay ,  FROM   Orders  WHERE   OrderId = 1  SELECT   OrderId , DATEADD ( day , 45 , OrderDate )   AS   OrderPayDate  FROM   Orders  SELECT   DATEDIFF ( day , '2008-06-05' , '2008-08-05' )   AS   DiffDate  CONVERT ( VARCHAR ( 19 ), GETDATE ())  CONVERT ( VARCHAR ( 10 ), GETDATE (), 10 )  CONVERT ( VARCHAR ( 10 ), GETDATE (), 110 )",
            "title": "Dates"
        },
        {
            "location": "/Databases/SQL/#sql-server-data-types",
            "text": "Data type / Description / Storage  char(n) \nFixed width character string. Maximum 8,000 characters\nDefined width  varchar(n) \nVariable width character string. Maximum 8,000 characters\n2 bytes + number of chars  varchar(max) \nVariable width character string. Maximum 1,073,741,824 characters\n2 bytes + number of chars  text \nVariable width character string. Maximum 2GB of text data\n4 bytes + number of chars  nchar \nFixed width Unicode string. Maximum 4,000 characters\nDefined width x 2  nvarchar \nVariable width Unicode string. Maximum 4,000 characters  nvarchar(max) \nVariable width Unicode string. Maximum 536,870,912 characters  ntext \nVariable width Unicode string. Maximum 2GB of text data  bit \nAllows 0, 1, or NULL  binary(n) \nFixed width binary string. Maximum 8,000 bytes  varbinary \nVariable width binary string. Maximum 8,000 bytes  varbinary(max) \nVariable width binary string. Maximum 2GB  image \nVariable width binary string. Maximum 2GB",
            "title": "SQL Server Data Types"
        },
        {
            "location": "/Databases/SQL/#number-types",
            "text": "tinyint \nAllows whole numbers from 0 to 255\n1 byte  smallint \nAllows whole numbers between -32,768 and 32,767\n2 bytes  int \nAllows whole numbers between -2,147,483,648 and 2,147,483,647\n4 bytes  bigint \nAllows whole numbers between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807\n8 bytes  decimal(p,s) \nFixed precision and scale numbers.\nAllows numbers from -10^38 +1 to 10^38.  The p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0.\n5-17 bytes  numeric(p,s) \nFixed precision and scale numbers.\nAllows numbers from -10^38 +1 to 10^38.  The p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0.\n5-17 bytes  smallmoney \nMonetary data from -214,748.3648 to 214,748.3647\n4 bytes  money \nMonetary data from -922,337,203,685,477.5808 to 922,337,203,685,477.5807\n8 bytes  float(n) \nFloating precision number data from -1.79E + 308 to 1.79E + 308.\nThe n parameter indicates whether the field should hold 4 or 8 bytes. float(24) holds a 4-byte field and float(53) holds an 8-byte field. Default value of n is 53.\n4 or 8 bytes  real \nFloating precision number data from -3.40E + 38 to 3.40E + 38\n4 bytes",
            "title": "Number types"
        },
        {
            "location": "/Databases/SQL/#date-types",
            "text": "datetime \nFrom January 1, 1753 to December 31, 9999 with an accuracy of 3.33 milliseconds\n8 bytes  datetime2 \nFrom January 1, 0001 to December 31, 9999 with an accuracy of 100 nanoseconds\n6-8 bytes  smalldatetime \nFrom January 1, 1900 to June 6, 2079 with an accuracy of 1 minute\n4 bytes  date \nStore a date only. From January 1, 0001 to December 31, 9999\n3 bytes  time \nStore a time only to an accuracy of 100 nanoseconds\n3-5 bytes  datetimeoffset \nThe same as datetime2 with the addition of a time zone offset\n8-10 bytes  timestamp \nStores a unique number that gets updated every time a row gets created or modified. The timestamp value is based upon an internal clock and does not correspond to real time. Each table may have only one timestamp variable",
            "title": "Date types"
        },
        {
            "location": "/Databases/SQL/#other-data-types",
            "text": "sql_variant \nStores up to 8,000 bytes of data of various data types, except text, ntext, and timestamp  uniqueidentifier \nStores a globally unique identifier (GUID)  xml \nStores XML formatted data. Maximum 2GB  cursor \nStores a reference to a cursor used for database operations  table \nStores a result-set for later processing",
            "title": "Other data types"
        },
        {
            "location": "/Databases/SQL/#sql-aggregate-functions",
            "text": "SQL aggregate functions return a single value, calculated from values in a column.  Useful aggregate functions:   AVG()    - Returns the average value  COUNT()  - Returns the number of rows  TOP 1    - Single sample  MAX()    - Returns the largest value  MIN()    - Returns the smallest value  SUM()    - Returns the sum   Examples:  SELECT   COUNT ( DISTINCT   column_name )   FROM   table_name ;  SELECT   TOP   1   column_name   FROM   table_name  ORDER   BY   column_name   DESC ;  SELECT   column_name ,   aggregate_function ( column_name )  FROM   table_name  WHERE   column_name   operator   value  GROUP   BY   column_name ;  SELECT   Shippers . ShipperName ,   COUNT ( Orders . OrderID )   AS   NumberOfOrders  FROM   Orders  LEFT   JOIN   Shippers  ON   Orders . ShipperID = Shippers . ShipperID  GROUP   BY   ShipperName ;  SELECT   column_name ,   aggregate_function ( column_name )  FROM   table_name  WHERE   column_name   operator   value  GROUP   BY   column_name  HAVING   aggregate_function ( column_name )   operator   value ;  SELECT   Employees . LastName ,   COUNT ( Orders . OrderID )   AS   NumberOfOrders  FROM   Orders  INNER   JOIN   Employees  ON   Orders . EmployeeID = Employees . EmployeeID )  GROUP   BY   LastName  HAVING   COUNT ( Orders . OrderID )   >   10 ;",
            "title": "SQL Aggregate Functions"
        },
        {
            "location": "/Databases/SQL/#sql-scalar-functions",
            "text": "Converts a field to upper case: SELECT UPPER(column_name) FROM table_name;  Converts a field to lower case: SELECT LOWER(column_name) FROM table_name;  MID() - Extract characters from a text field  LEN() - Returns the length of a text field  ROUND() - Rounds a numeric field to the number of decimals specified  NOW() - Returns the current system date and time  FORMAT() - Formats how a field is to be displayed   SELECT   ProductName ,   ROUND ( Price , 0 )   AS   RoundedPrice  FROM   Products ;",
            "title": "SQL Scalar functions"
        },
        {
            "location": "/Databases/SQL/#variables",
            "text": "DECLARE   @ myvar   char ( 20 );  SET   @ myvar   =   'This is a test' ;  SELECT   @ myvar ;",
            "title": "Variables"
        },
        {
            "location": "/Databases/SQL/#scalar-function",
            "text": "CREATE   FUNCTION   FunctionName  (  -- Add the parameters for the function here  @ p1   int  )  RETURNS   int  AS  BEGIN  -- Declare the return variable here  DECLARE   @ Result   int  -- Add the T-SQL statements to compute the return value here  SELECT   @ Result   =   @ p1  -- Return the result of the function  RETURN   @ Result  END",
            "title": "Scalar Function"
        },
        {
            "location": "/Databases/SQL/#table-value-function",
            "text": "IF   OBJECT_ID   ( N 'dbo.EmployeeByID'   )   IS   NOT   NULL \n    DROP   FUNCTION   dbo . EmployeeByID  GO  CREATE   FUNCTION   dbo . EmployeeByID ( @ InEmpID   int )  RETURNS   @ retFindReports   TABLE  ( \n     -- columns returned by the function \n     EmployeeID   int   NOT   NULL , \n     Name   nvarchar ( 255   )   NOT   NULL , \n     Title   nvarchar ( 50   )   NOT   NULL , \n     EmployeeLevel   int   NOT   NULL  )  AS  -- body of the function  BEGIN \n    WITH   DirectReports ( Name   ,   Title   ,   EmployeeID   ,   EmployeeLevel   ,   Sort   )   AS \n     ( SELECT   CONVERT (   varchar ( 255   ),   c   . FirstName   +   ' '   +   c . LastName   ), \n         e . Title   , \n         e . EmployeeID   , \n         1   , \n         CONVERT ( varchar   ( 255 ),   c .   FirstName   +   ' '   +   c   . LastName ) \n      FROM   HumanResources . Employee   AS   e \n           JOIN   Person . Contact   AS   c   ON   e . ContactID   =   c . ContactID \n      WHERE   e . EmployeeID   =   @ InEmpID \n    UNION   ALL \n      SELECT   CONVERT   ( varchar (   255 ),   REPLICATE   (   '| '   ,   EmployeeLevel )   + \n         c . FirstName   +   ' '   +   c .   LastName ), \n         e . Title   , \n         e . EmployeeID   , \n         EmployeeLevel   +   1 , \n         CONVERT   (   varchar ( 255   ),   RTRIM   ( Sort )   +   '| '   +   FirstName   +   ' '   + \n                  LastName ) \n      FROM   HumanResources . Employee   as   e \n           JOIN   Person . Contact   AS   c   ON   e . ContactID   =   c . ContactID \n           JOIN   DirectReports   AS   d   ON   e .   ManagerID   =   d .   EmployeeID \n     ) \n    -- copy the required columns to the result of the function \n\n    INSERT   @ retFindReports \n    SELECT   EmployeeID ,   Name ,   Title ,   EmployeeLevel \n      FROM   DirectReports \n    ORDER   BY   Sort \n    RETURN  END  GO",
            "title": "Table Value Function"
        },
        {
            "location": "/Databases/SQL/#stored-procedure",
            "text": "CREATE   PROCEDURE   ProcedureName \n         -- Add the parameters for the stored procedure here \n         @ p1   int   =   0   , \n         @ p2   int   =   0  AS  BEGIN \n         -- SET NOCOUNT ON added to prevent extra result sets from \n         -- interfering with SELECT statements. \n         SET   NOCOUNT   ON ; \n\n     -- Insert statements for procedure here \n         SELECT   @ p1   ,   @ p2  END  GO",
            "title": "Stored Procedure"
        },
        {
            "location": "/Databases/SQL/#self-join",
            "text": "Q. Here's the data in a table 'orders'  customer_id order_id order_day\n123        27424624    25Dec2011\n123        89690900    25Dec2010\n797        12131323    25Dec2010\n876        67145419    15Dec2011  Could you give me SQL for customers who placed orders on both the days, 25th Dec 2010 and 25th Dec 2011?       SELECT   o . customer_id ,   o . order_day \n     FROM   orders   AS   o \n     INNER   JOIN   orders   AS   o1 \n     ON   o . customer_id   =   o1 . customer_id \n     WHERE   ...",
            "title": "Self-join"
        },
        {
            "location": "/Deep_Learning/Keras/",
            "text": "Useful Links\n\u00b6\n\n\n\n\nKeras\n\n\n\n\nKeras Blog\n\n\n\n\n\n\nBuilding autoencoders in Keras\n\n\n\n\n\n\nmanifold-learning-and-autoencoders\n\n\n\n\nKeras tutorial for Kaggle 2nd Annual Data Science Bowl\n\n\nSupervised Sequence Labelling with Recurrent Neural Networks\n\n\nSequence Classification with LSTM Recurrent Neural Networks in Python with Keras",
            "title": "Keras"
        },
        {
            "location": "/Deep_Learning/Keras/#useful-links",
            "text": "Keras   Keras Blog    Building autoencoders in Keras    manifold-learning-and-autoencoders   Keras tutorial for Kaggle 2nd Annual Data Science Bowl  Supervised Sequence Labelling with Recurrent Neural Networks  Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras",
            "title": "Useful Links"
        },
        {
            "location": "/DevOps/CloudFormation/",
            "text": "DevOps Philosophy\n\u00b6\n\n\nWhy we use Terraform and not Chef, Puppet, Ansible, SaltStack, or CloudFormation\n\n\nTools\n\u00b6\n\n\n\n\nAWS CLI cloudformation\n\n\naws cloudformation validate-template\n\n\nboto3 cloudformation\n\n\n\n\nYAML\n\u00b6\n\n\n\n\nYAML Cheatsheet\n\n\nYAML Cheatsheet 2\n\n\n\n\nYAML notation for folded text: \n>\n\n\ndata\n:\n \n>\n\n   \nWrapped text\n\n   \nwill be folded\n\n   \ninto a single\n\n   \nparagraph\n\n\n   \nBlank lines denote\n\n   \nparagraph breaks\n\n\n\n\n\n\nSample Templates\n\u00b6\n\n\nTemplates for the US East (Northern Virginia) Region\n\n\nAWSlabs on GitHub\n\n- \nStartup kit templates\n\n- \nAWS CloudFormation Sample Templates\n\n\nCloudonaut Templates\n\n\nFree Templates for AWS CloudFormation (Cloudonaut)\n\n\nDeploying Microservices with Amazon ECS, AWS CloudFormation, and an Application Load Balancer\n\n\nTemplate Basics\n\u00b6\n\n\nTemplate Basics\n\n\nTemplate Anatomy\n\n\n---\n\n\nAWSTemplateFormatVersion\n:\n \n\"version\n \ndate\"\n\n\n\nDescription\n:\n\n  \nString\n\n\n\nMetadata\n:\n\n  \ntemplate metadata\n\n\n\nParameters\n:\n\n  \nset of parameters\n\n\n\nMappings\n:\n\n  \nset of mappings\n\n\n\nConditions\n:\n\n  \nset of conditions\n\n\n\nTransform\n:\n\n  \nset of transforms\n\n\n\nResources\n:\n\n  \nset of resources\n\n\n\nOutputs\n:\n\n  \nset of outputs\n\n\n\n\n\n\nWith examples:\n\n\n---\n\n\nAWSTemplateFormatVersion\n:\n \n\"2010-09-09\"\n\n\n\nDescription\n:\n \n>\n\n  \nHere are some\n\n  \ndetails about\n\n  \nthe template.\n\n\n\nMetadata\n:\n\n  \nInstances\n:\n\n    \nDescription\n:\n \n\"Information\n \nabout\n \nthe\n \ninstances\"\n\n  \nDatabases\n:\n \n    \nDescription\n:\n \n\"Information\n \nabout\n \nthe\n \ndatabases\"\n\n\n\nParameters\n:\n \n  \nInstanceTypeParameter\n:\n \n    \nType\n:\n \nString\n            \n# String, Number, List<Number>, CommaDelimitedList e.g. \"test,dev,prod\", or an AWS-specific types such as Amazon EC2 key pair names and VPC IDs.\n\n    \nDefault\n:\n \nt2.micro\n\n    \nAllowedValues\n:\n \n      \n-\n \nt2.micro\n\n      \n-\n \nm1.small\n\n    \nDescription\n:\n \nEnter t2.micro or m1.small. Default is t2.micro.\n\n    \n# AllowedPattern: \"[A-Za-z0-9]+\" # A regular expression that represents the patterns you want to allow for String types.\n\n    \n# ConstraintDescription: Malformed input-Parameter MyParameter must match pattern [A-Za-z0-9]+\n\n    \n# MinLength: 2  # for String\n\n    \n# MaxLength: 10\n\n    \n# MinValue: 0   # for Number types.\n\n    \n# MaxValue: 100 \n\n    \n# NoEcho: True\n\n\n\nMappings\n:\n \n  \nRegionMap\n:\n \n    \nus-east-1\n:\n \n      \n\"32\"\n:\n \n\"ami-6411e20d\"\n\n    \nus-west-1\n:\n \n      \n\"32\"\n:\n \n\"ami-c9c7978c\"\n\n    \neu-west-1\n:\n \n      \n\"32\"\n:\n \n\"ami-37c2f643\"\n\n    \nap-southeast-1\n:\n \n      \n\"32\"\n:\n \n\"ami-66f28c34\"\n\n    \nap-northeast-1\n:\n \n      \n\"32\"\n:\n \n\"ami-9c03a89d\"\n\n\n\nConditions\n:\n \n  \nCreateProdResources\n:\n \n!Equals\n \n[\n \n!Ref\n \nEnvType\n,\n \nprod\n \n]\n\n\n\nTransform\n:\n\n  \nset of transforms\n\n\n\nResources\n:\n\n  \nEc2Instance\n:\n\n      \nType\n:\n \nAWS::EC2::Instance\n\n      \nProperties\n:\n\n        \nInstanceType\n:\n\n          \nRef\n:\n \nInstanceTypeParameter\n  \n# reference to parameter above\n\n        \nImageId\n:\n \nami-2f726546\n\n\n\nOutputs\n:\n \n  \nVolumeId\n:\n \n    \nCondition\n:\n \nCreateProdResources\n\n    \nValue\n:\n \n      \n!Ref\n \nNewVolume\n\n\n\n\n\n\n\n\nThe Ref function can refer to input parameters that are specified at stack creation time.\n\n\n\n\nExamples\n\u00b6\n\n\nS3\n\u00b6\n\n\nResources\n:\n\n  \nHelloBucket\n:\n\n    \nType\n:\n \nAWS::S3::Bucket\n  \n# AWS::ProductIdentifier::ResourceType\n\n\n\n\n\n\nEC2\n\u00b6\n\n\nResources\n:\n\n  \nEc2Instance\n:\n\n    \nType\n:\n \nAWS::EC2::Instance\n\n    \nProperties\n:\n\n      \nSecurityGroups\n:\n\n      \n-\n \nRef\n:\n \nInstanceSecurityGroup\n\n      \nKeyName\n:\n \nmykey\n\n      \nImageId\n:\n \n''\n\n  \nInstanceSecurityGroup\n:\n\n    \nType\n:\n \nAWS::EC2::SecurityGroup\n\n    \nProperties\n:\n\n      \nGroupDescription\n:\n \nEnable SSH access via port 22\n\n      \nSecurityGroupIngress\n:\n\n      \n-\n \nIpProtocol\n:\n \ntcp\n\n        \nFromPort\n:\n \n'22'\n\n        \nToPort\n:\n \n'22'\n\n        \nCidrIp\n:\n \n0.0.0.0/0",
            "title": "CloudFormation"
        },
        {
            "location": "/DevOps/CloudFormation/#devops-philosophy",
            "text": "Why we use Terraform and not Chef, Puppet, Ansible, SaltStack, or CloudFormation",
            "title": "DevOps Philosophy"
        },
        {
            "location": "/DevOps/CloudFormation/#tools",
            "text": "AWS CLI cloudformation  aws cloudformation validate-template  boto3 cloudformation",
            "title": "Tools"
        },
        {
            "location": "/DevOps/CloudFormation/#yaml",
            "text": "YAML Cheatsheet  YAML Cheatsheet 2   YAML notation for folded text:  >  data :   > \n    Wrapped text \n    will be folded \n    into a single \n    paragraph \n\n    Blank lines denote \n    paragraph breaks",
            "title": "YAML"
        },
        {
            "location": "/DevOps/CloudFormation/#sample-templates",
            "text": "Templates for the US East (Northern Virginia) Region  AWSlabs on GitHub \n-  Startup kit templates \n-  AWS CloudFormation Sample Templates  Cloudonaut Templates  Free Templates for AWS CloudFormation (Cloudonaut)  Deploying Microservices with Amazon ECS, AWS CloudFormation, and an Application Load Balancer",
            "title": "Sample Templates"
        },
        {
            "location": "/DevOps/CloudFormation/#template-basics",
            "text": "Template Basics  Template Anatomy  ---  AWSTemplateFormatVersion :   \"version   date\"  Description : \n   String  Metadata : \n   template metadata  Parameters : \n   set of parameters  Mappings : \n   set of mappings  Conditions : \n   set of conditions  Transform : \n   set of transforms  Resources : \n   set of resources  Outputs : \n   set of outputs   With examples:  ---  AWSTemplateFormatVersion :   \"2010-09-09\"  Description :   > \n   Here are some \n   details about \n   the template.  Metadata : \n   Instances : \n     Description :   \"Information   about   the   instances\" \n   Databases :  \n     Description :   \"Information   about   the   databases\"  Parameters :  \n   InstanceTypeParameter :  \n     Type :   String              # String, Number, List<Number>, CommaDelimitedList e.g. \"test,dev,prod\", or an AWS-specific types such as Amazon EC2 key pair names and VPC IDs. \n     Default :   t2.micro \n     AllowedValues :  \n       -   t2.micro \n       -   m1.small \n     Description :   Enter t2.micro or m1.small. Default is t2.micro. \n     # AllowedPattern: \"[A-Za-z0-9]+\" # A regular expression that represents the patterns you want to allow for String types. \n     # ConstraintDescription: Malformed input-Parameter MyParameter must match pattern [A-Za-z0-9]+ \n     # MinLength: 2  # for String \n     # MaxLength: 10 \n     # MinValue: 0   # for Number types. \n     # MaxValue: 100  \n     # NoEcho: True  Mappings :  \n   RegionMap :  \n     us-east-1 :  \n       \"32\" :   \"ami-6411e20d\" \n     us-west-1 :  \n       \"32\" :   \"ami-c9c7978c\" \n     eu-west-1 :  \n       \"32\" :   \"ami-37c2f643\" \n     ap-southeast-1 :  \n       \"32\" :   \"ami-66f28c34\" \n     ap-northeast-1 :  \n       \"32\" :   \"ami-9c03a89d\"  Conditions :  \n   CreateProdResources :   !Equals   [   !Ref   EnvType ,   prod   ]  Transform : \n   set of transforms  Resources : \n   Ec2Instance : \n       Type :   AWS::EC2::Instance \n       Properties : \n         InstanceType : \n           Ref :   InstanceTypeParameter    # reference to parameter above \n         ImageId :   ami-2f726546  Outputs :  \n   VolumeId :  \n     Condition :   CreateProdResources \n     Value :  \n       !Ref   NewVolume    The Ref function can refer to input parameters that are specified at stack creation time.",
            "title": "Template Basics"
        },
        {
            "location": "/DevOps/CloudFormation/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/DevOps/CloudFormation/#s3",
            "text": "Resources : \n   HelloBucket : \n     Type :   AWS::S3::Bucket    # AWS::ProductIdentifier::ResourceType",
            "title": "S3"
        },
        {
            "location": "/DevOps/CloudFormation/#ec2",
            "text": "Resources : \n   Ec2Instance : \n     Type :   AWS::EC2::Instance \n     Properties : \n       SecurityGroups : \n       -   Ref :   InstanceSecurityGroup \n       KeyName :   mykey \n       ImageId :   '' \n   InstanceSecurityGroup : \n     Type :   AWS::EC2::SecurityGroup \n     Properties : \n       GroupDescription :   Enable SSH access via port 22 \n       SecurityGroupIngress : \n       -   IpProtocol :   tcp \n         FromPort :   '22' \n         ToPort :   '22' \n         CidrIp :   0.0.0.0/0",
            "title": "EC2"
        },
        {
            "location": "/DevOps/Docker/",
            "text": "Docker Cheatsheet\n\u00b6\n\n\nUseful Links\n\u00b6\n\n\nDocker Cheat Sheet\n\n\nDocker Documentation\n\n\nDocker Tutorials and Labs\n\n\nDocker + Jenkins\n\n\nDocker Hub\n\n\nConcepts\n\u00b6\n\n\nA Docker image is a read-only template. For example, an image could contain an Ubuntu operating system with Apache and your web application installed. Images are used to create Docker containers. Docker provides a simple way to build new images or update existing images, or you can download Docker images that other people have already created. Docker images are the buildcomponent of Docker.\n\n\nDocker registries hold images.\n\n\nCheatsheet\n\u00b6\n\n\nTo show only running containers use:\n\n\n$ docker ps\n\n\n\n\n\nTo show all containers use:\n\n\n$ docker ps -a\n\n\n\n\n\nShow last started container:\n\n\n$ docker ps -l\n\n\n\n\n\nDownload an image:\n\n\n$ docker pull centos\n\n\n\n\n\nCreate then start a container: \ndocker run [OPTIONS] IMAGE [COMMAND] [ARG...]\n\n    * \nDocker run reference\n\n\n$ docker run hello-world\n\n\n\n\n\nRun with interactive terminal (i = interactive t = terminal):\n\n\n$ docker run -it ubuntu bash\n\n\n\n\n\nStart then detach the container (daemonize):\n\n\n$ docker run -d -p8088:80 --name webserver nginx\n\n\n\n\n\nIf you want a transient container, \ndocker run --rm\n will remove the container after it stops.\n\n\nLooks inside the container (use \n-f\n to act like \ntail -f\n):\n\n\n$ docker logs <container name>\n\n\n\n\n\nStop container:\n\n\n$ docker stop <container name>   \n# container ID or name\n\n\n\n\n\n\nDelete container:\n\n\n$ docker rm <container name>\n\n\n\n\n\nTo check the environment:\n\n\n$ docker run -it alpine env \n\n\n\n\n\nDocker version / info:\n\n\n$ docker version\n$ docker info\n\n\n\n\n\nPort Mapping\n\u00b6\n\n\n-p 80:5000\n  would map port 80 on our local host to port 5000 inside our container.\n\n\n$ docker run -d -p \n80\n:5000 training/webapp python app.py\n\n\n\n\n\nFull format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort\n\n\n$ docker run -d -p \n127\n.0.0.1:80:5000 training/webapp python app.py\n\n\n\n\n\nBoth hostPort and containerPort can be specified as a range of ports. When specifying ranges for both, the number of container ports in the range must match the number of host ports in the range, for example: \n-p1234-1236:1234-1236/tcp\n\n\nThe \n-P\n flag tells Docker to map any required network ports inside our container to our host (using random ports).\n\n\n$ docker run -d -P training/webapp python app.py\n\n\n\n\n\nLinking\n\u00b6\n\n\n--link <name or id>:alias\n where \nname\n is the name of the container we\u2019re linking to and \nalias\n is an alias for the link name.\nThe \n--link\n flag also takes the form: \n--link <name or id>\n\n\n$ docker run -d --name myES -p \n9200\n:9200 -p \n9300\n:9300 elasticsearch\n$ docker run --name myK --link myES:elasticsearch  -p \n5601\n:5601 -d docker-kibana-sense\n\n\n\n\n\nNetworks\n\u00b6\n\n\n$ docker network ls\n\n\n\n\n\nFind out the container\u2019s IP address:\n\n\n$ docker network inspect bridge\n\n\n\n\n\nData Volumes\n\u00b6\n\n\nCreate a new volume inside a container at /webapp:\n\n\n$ docker run -d -P --name web -v /webapp training/webapp python app.py\n$ docker inspect web\n\n\n\n\n\nYou can also use the VOLUME instruction in a Dockerfile to add one or more new volumes to any container created from that image.\n\n\nMount the host directory, \n/src/webapp\n, into the container at \n/opt/webapp\n.\n\n\n$ docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n\n\n\n\n\nOn Windows, use:  \ndocker run -v /c/Users/<path>:/<container path> ...\n\n\nExample Dockerfile\n\u00b6\n\n\n$ vim Dockerfile\n\n\n\n\n\nFROM\n docker/whalesay:latest\n\n\nRUN\n apt-get -y update \n&&\n apt-get install -y fortunes\n\nCMD\n /usr/games/fortune -a | cowsay\n\n\n\n\n\n\n$ docker build -t docker-whale .",
            "title": "Docker"
        },
        {
            "location": "/DevOps/Docker/#docker-cheatsheet",
            "text": "",
            "title": "Docker Cheatsheet"
        },
        {
            "location": "/DevOps/Docker/#useful-links",
            "text": "Docker Cheat Sheet  Docker Documentation  Docker Tutorials and Labs  Docker + Jenkins  Docker Hub",
            "title": "Useful Links"
        },
        {
            "location": "/DevOps/Docker/#concepts",
            "text": "A Docker image is a read-only template. For example, an image could contain an Ubuntu operating system with Apache and your web application installed. Images are used to create Docker containers. Docker provides a simple way to build new images or update existing images, or you can download Docker images that other people have already created. Docker images are the buildcomponent of Docker.  Docker registries hold images.",
            "title": "Concepts"
        },
        {
            "location": "/DevOps/Docker/#cheatsheet",
            "text": "To show only running containers use:  $ docker ps  To show all containers use:  $ docker ps -a  Show last started container:  $ docker ps -l  Download an image:  $ docker pull centos  Create then start a container:  docker run [OPTIONS] IMAGE [COMMAND] [ARG...] \n    *  Docker run reference  $ docker run hello-world  Run with interactive terminal (i = interactive t = terminal):  $ docker run -it ubuntu bash  Start then detach the container (daemonize):  $ docker run -d -p8088:80 --name webserver nginx  If you want a transient container,  docker run --rm  will remove the container after it stops.  Looks inside the container (use  -f  to act like  tail -f ):  $ docker logs <container name>  Stop container:  $ docker stop <container name>    # container ID or name   Delete container:  $ docker rm <container name>  To check the environment:  $ docker run -it alpine env   Docker version / info:  $ docker version\n$ docker info",
            "title": "Cheatsheet"
        },
        {
            "location": "/DevOps/Docker/#port-mapping",
            "text": "-p 80:5000   would map port 80 on our local host to port 5000 inside our container.  $ docker run -d -p  80 :5000 training/webapp python app.py  Full format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort  $ docker run -d -p  127 .0.0.1:80:5000 training/webapp python app.py  Both hostPort and containerPort can be specified as a range of ports. When specifying ranges for both, the number of container ports in the range must match the number of host ports in the range, for example:  -p1234-1236:1234-1236/tcp  The  -P  flag tells Docker to map any required network ports inside our container to our host (using random ports).  $ docker run -d -P training/webapp python app.py",
            "title": "Port Mapping"
        },
        {
            "location": "/DevOps/Docker/#linking",
            "text": "--link <name or id>:alias  where  name  is the name of the container we\u2019re linking to and  alias  is an alias for the link name.\nThe  --link  flag also takes the form:  --link <name or id>  $ docker run -d --name myES -p  9200 :9200 -p  9300 :9300 elasticsearch\n$ docker run --name myK --link myES:elasticsearch  -p  5601 :5601 -d docker-kibana-sense",
            "title": "Linking"
        },
        {
            "location": "/DevOps/Docker/#networks",
            "text": "$ docker network ls  Find out the container\u2019s IP address:  $ docker network inspect bridge",
            "title": "Networks"
        },
        {
            "location": "/DevOps/Docker/#data-volumes",
            "text": "Create a new volume inside a container at /webapp:  $ docker run -d -P --name web -v /webapp training/webapp python app.py\n$ docker inspect web  You can also use the VOLUME instruction in a Dockerfile to add one or more new volumes to any container created from that image.  Mount the host directory,  /src/webapp , into the container at  /opt/webapp .  $ docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py  On Windows, use:   docker run -v /c/Users/<path>:/<container path> ...",
            "title": "Data Volumes"
        },
        {
            "location": "/DevOps/Docker/#example-dockerfile",
            "text": "$ vim Dockerfile  FROM  docker/whalesay:latest  RUN  apt-get -y update  &&  apt-get install -y fortunes CMD  /usr/games/fortune -a | cowsay   $ docker build -t docker-whale .",
            "title": "Example Dockerfile"
        },
        {
            "location": "/DevOps/Git/",
            "text": "Git Cheatsheets\n\u00b6\n\n\n\n\nGraphical git cheatsheet\n\n\nGit basic commands\n\n\nGit cheatsheet (visual)\n\n\nGit cheatsheet (interactive)\n\n\nGit full documentation\n\n\n\n\nRepo hosting:\n\n\n \nBitBucket\n\n\n \nGitHub\n \n\n\nCommon Commands\n\u00b6\n\n\n\n\nCreate a new Git repository in current directory:\n\n\n\n\ngit init\n\n\n\n\n\n\n\nOr create an empty Git repository in the specified directory:\n\n\n\n\ngit init <directory>\n\n\n\n\n\n\n\nOr copy an existing Git repository:\n\n\n\n\ngit clone <repo URL>\n\n\n\n\n\n\n\nClone the repository located at \n into the folder called \n on the local machine:\n\n\n\n\ngit clone <repo> <directory>\ngit clone username@host:/path/to/repository\n\n\n\n\n\n\n\nGlobal Configuration:\n\n\n\n\n$ git config --global user.name \n\"Firstname Lastname\"\n\n$ git config --global user.email \n\"your_email@youremail.com\"\n\n\n\n\n\n\n\n\nStage all changes in \n<file>\n for the next commit:\n\n\n\n\ngit add <file>\n\n\n\n\n\n\n\nOr stage all changes in \n<directory>\n for the next commit:\n\n\n\n\ngit add <directory>  \n# usually '.' for current directory\n\n\n\n\n\n\n\n\nCommit the staged snapshot to the project history:\n\n\n\n\ngit commit  \n# interactive   \n\ngit commit -m \n\"<message>\"\n\n\n\n\n\n\n\n\nOr add and commit all in one:\n\n\n\n\ngit commit -am \n\"message\"\n\n\n\n\n\n\n\n\nFix up the most recent commit (don't do that if shared history):\n\n\n\n\ngit commit --amend\n\n\n\n\n\n\n\nList which files are staged, unstaged, and untracked:\n\n\n\n\ngit status\ngit status -s  \n# short format\n\n\n\n\n\n\n\n\nShow file diff:\n\n\n\n\ngit diff           \n#  git diff by itself doesn\u2019t show all changes made since your last commit \u2013 only changes that are still unstaged.\n\ngit diff --staged  \n#  Shows file differences between staging and the last file version\n\n\n\n\n\n\n\n\nOpen GUI:\n\n\n\n\ngit gui\n\n\n\n\n\n\n\nDisplays committed snapshots:\n\n\n\n\ngit log -n <limit>\ngit log --graph --decorate --oneline\n\n\n\n\n\n\n\nChecking out commits, and checking out branches:\n\n\n\n\ngit checkout <commit>       \n#  Return to commit\n\ngit checkout master         \n#  Return to the master branch (or whatever branch we choose)\n\n\n\n\n\n\n\n\nCheck out a previous version of a file:\n\n\n\n\ngit checkout <commit> <file>    \n#  Check out the version of the file from the selected commit\n\ngit checkout HEAD hello.py      \n#  Check out the most recent version\n\n\n\n\n\n\nBranches\n\u00b6\n\n\nBranches are just pointers to commits.\n\n\n\n\nList all of the branches in your repository.  Also tell you what branch you're currently in ('*' branch):\n\n\n\n\ngit branch\n\n\n\n\n\n\n\nCreate a new branch called \n<branch>\n. \n\n\n\n\ngit branch <branch>\n\n\n\n\n\nThis does not check out the new branch. You need:\n\n\ngit checkout <existing-branch>\n\n\n\n\n\nOr direcly create-and-check out \n<new-branch>\n.\n\n\ngit checkout -b <new-branch>\n\n\n\n\n\n\n\nSafe delete the branch:\n\n\n\n\ngit branch -d <branch>\n\n\n\n\n\n\n\nMerge the specified branch into the current branch:\n\n\n\n\ngit merge <branch>\n\n\n\n\n\n\n\nUndo any undesired changes\n\n\n\n\nGenerate a new commit that undoes all of the changes introduced in \n<commit>\n, then apply it to the current branch\n\n\ngit revert <commit>\n\n\n\n\n\ngit revert\n undoes a single commit \u2014 it does not \u201crevert\u201d back to the previous state of a project by removing all subsequent commits.\n\n\n\n\nReset (dangerous method - erases history):\n\n\n\n\ngit reset\n\n\n\n\n\n\n\nList the remote connections you have to other repositories.\n\n\n\n\ngit remote -v\n\n\n\n\n\n\n\nCreate a new connection / delete a connection to a remote repository.\n\n\n\n\ngit remote add <name> <url>  \n# often \"origin\"\n\ngit remote rm <name>         \n# delete\n\n\n\n\n\n\n\n\nFetch the specified remote\u2019s copy of the current branch and immediately merge it into the local copy. This is the same as \ngit fetch <remote>\n followed by \ngit merge origin/<current-branch>\n.\n\n\n\n\ngit pull <remote>\n\n\n\n\n\n\n\nPut my changes on top of what everybody else has done. Ensure a linear history by preventing unnecessary merge commits.\n\n\n\n\ngit pull --rebase <remote>\n\n\n\n\n\n\n\nTransfer commits from your local repository to a remote repo.\n\n\n\n\ngit push <remote> <branch>\n\n\n\n\n\n\n\nPushes the current branch to the remote server and links the local branch to the remote so next time you can do \ngit pull\n or \ngit push\n.\n\n\n\n\ngit push -u origin <branch>\n\n\n\n\n\nTypical Workflows\n\u00b6\n\n\nClone a Repo\n\u00b6\n\n\n$ mkdir repos\n$ \ncd\n ~/repos\n$ git clone https://<url>\n$ ls -al  <repo dir>\n\n\n\n\n\nAdd a change in the working directory to the staging area\n\u00b6\n\n\n$ git status\n$ git add README\n\n\n\n\n\n-A\n, \n--all\n finds new files as well as staging modified content and removing files that are no longer in the working tree.\n\n\n$ git add -A\n$ git commit -m \n\"Add repo instructions\"\n\n$ git push -u origin master\n$ git pull\n$ ssh -p \n2222\n user@domain.com\n\n\n\n\n\nShort-lived topic branches\n\u00b6\n\n\n\n\nStart a new feature: \n\n\n\n\ngit checkout -b new-feature master\n\n\n\n\n\n\n\nEdit some files:\n\n\n\n\ngit add <file>\ngit commit -m \n\"Start a feature\"\n\n\n\n\n\n\n\n\nEdit some files\n\n\n\n\ngit add <file>\ngit commit -m \n\"Finish a feature\"\n\n\n\n\n\n\n\n\nMerge in the \nnew-feature\n branch\n\n\n\n\ngit checkout master\ngit merge new-feature\ngit branch -d new-feature\n\n\n\n\n\nPush and pull from a centralized repo\n\u00b6\n\n\n\n\nTo push the master branch to the central repo:\n\n\n\n\ngit push origin master\n\n\n\n\n\nIf local history has diverged from the central repository, Git will refuse the request.\n\n\ngit pull --rebase origin master\n\n\n\n\n\nSync my local repo with the remote repo\n\u00b6\n\n\ngit pull origin master\ngit add filename.xyz\ngit commit . -m \u201ccomment\u201d\ngit push origin master\n\n\n\n\n\nCreate a central Repo\n\u00b6\n\n\nThe \n--bare\n flag creates a repository that doesn\u2019t have a working directory, making it impossible to edit files and commit changes in that repository. Central repositories should always be created as bare repositories because pushing branches to a non-bare repository has the potential to overwrite changes.\n\n\n$ git init --bare foobar.git  \n$ git rev-parse --show-toplevel     \n# print top-level directory\n\n$ git rev-parse --git-dir           \n# print .git directory name",
            "title": "Git"
        },
        {
            "location": "/DevOps/Git/#git-cheatsheets",
            "text": "Graphical git cheatsheet  Git basic commands  Git cheatsheet (visual)  Git cheatsheet (interactive)  Git full documentation   Repo hosting:    BitBucket    GitHub",
            "title": "Git Cheatsheets"
        },
        {
            "location": "/DevOps/Git/#common-commands",
            "text": "Create a new Git repository in current directory:   git init   Or create an empty Git repository in the specified directory:   git init <directory>   Or copy an existing Git repository:   git clone <repo URL>   Clone the repository located at   into the folder called   on the local machine:   git clone <repo> <directory>\ngit clone username@host:/path/to/repository   Global Configuration:   $ git config --global user.name  \"Firstname Lastname\" \n$ git config --global user.email  \"your_email@youremail.com\"    Stage all changes in  <file>  for the next commit:   git add <file>   Or stage all changes in  <directory>  for the next commit:   git add <directory>   # usually '.' for current directory    Commit the staged snapshot to the project history:   git commit   # interactive    \ngit commit -m  \"<message>\"    Or add and commit all in one:   git commit -am  \"message\"    Fix up the most recent commit (don't do that if shared history):   git commit --amend   List which files are staged, unstaged, and untracked:   git status\ngit status -s   # short format    Show file diff:   git diff            #  git diff by itself doesn\u2019t show all changes made since your last commit \u2013 only changes that are still unstaged. \ngit diff --staged   #  Shows file differences between staging and the last file version    Open GUI:   git gui   Displays committed snapshots:   git log -n <limit>\ngit log --graph --decorate --oneline   Checking out commits, and checking out branches:   git checkout <commit>        #  Return to commit \ngit checkout master          #  Return to the master branch (or whatever branch we choose)    Check out a previous version of a file:   git checkout <commit> <file>     #  Check out the version of the file from the selected commit \ngit checkout HEAD hello.py       #  Check out the most recent version",
            "title": "Common Commands"
        },
        {
            "location": "/DevOps/Git/#branches",
            "text": "Branches are just pointers to commits.   List all of the branches in your repository.  Also tell you what branch you're currently in ('*' branch):   git branch   Create a new branch called  <branch> .    git branch <branch>  This does not check out the new branch. You need:  git checkout <existing-branch>  Or direcly create-and-check out  <new-branch> .  git checkout -b <new-branch>   Safe delete the branch:   git branch -d <branch>   Merge the specified branch into the current branch:   git merge <branch>   Undo any undesired changes   Generate a new commit that undoes all of the changes introduced in  <commit> , then apply it to the current branch  git revert <commit>  git revert  undoes a single commit \u2014 it does not \u201crevert\u201d back to the previous state of a project by removing all subsequent commits.   Reset (dangerous method - erases history):   git reset   List the remote connections you have to other repositories.   git remote -v   Create a new connection / delete a connection to a remote repository.   git remote add <name> <url>   # often \"origin\" \ngit remote rm <name>          # delete    Fetch the specified remote\u2019s copy of the current branch and immediately merge it into the local copy. This is the same as  git fetch <remote>  followed by  git merge origin/<current-branch> .   git pull <remote>   Put my changes on top of what everybody else has done. Ensure a linear history by preventing unnecessary merge commits.   git pull --rebase <remote>   Transfer commits from your local repository to a remote repo.   git push <remote> <branch>   Pushes the current branch to the remote server and links the local branch to the remote so next time you can do  git pull  or  git push .   git push -u origin <branch>",
            "title": "Branches"
        },
        {
            "location": "/DevOps/Git/#typical-workflows",
            "text": "",
            "title": "Typical Workflows"
        },
        {
            "location": "/DevOps/Git/#clone-a-repo",
            "text": "$ mkdir repos\n$  cd  ~/repos\n$ git clone https://<url>\n$ ls -al  <repo dir>",
            "title": "Clone a Repo"
        },
        {
            "location": "/DevOps/Git/#add-a-change-in-the-working-directory-to-the-staging-area",
            "text": "$ git status\n$ git add README  -A ,  --all  finds new files as well as staging modified content and removing files that are no longer in the working tree.  $ git add -A\n$ git commit -m  \"Add repo instructions\" \n$ git push -u origin master\n$ git pull\n$ ssh -p  2222  user@domain.com",
            "title": "Add a change in the working directory to the staging area"
        },
        {
            "location": "/DevOps/Git/#short-lived-topic-branches",
            "text": "Start a new feature:    git checkout -b new-feature master   Edit some files:   git add <file>\ngit commit -m  \"Start a feature\"    Edit some files   git add <file>\ngit commit -m  \"Finish a feature\"    Merge in the  new-feature  branch   git checkout master\ngit merge new-feature\ngit branch -d new-feature",
            "title": "Short-lived topic branches"
        },
        {
            "location": "/DevOps/Git/#push-and-pull-from-a-centralized-repo",
            "text": "To push the master branch to the central repo:   git push origin master  If local history has diverged from the central repository, Git will refuse the request.  git pull --rebase origin master",
            "title": "Push and pull from a centralized repo"
        },
        {
            "location": "/DevOps/Git/#sync-my-local-repo-with-the-remote-repo",
            "text": "git pull origin master\ngit add filename.xyz\ngit commit . -m \u201ccomment\u201d\ngit push origin master",
            "title": "Sync my local repo with the remote repo"
        },
        {
            "location": "/DevOps/Git/#create-a-central-repo",
            "text": "The  --bare  flag creates a repository that doesn\u2019t have a working directory, making it impossible to edit files and commit changes in that repository. Central repositories should always be created as bare repositories because pushing branches to a non-bare repository has the potential to overwrite changes.  $ git init --bare foobar.git  \n$ git rev-parse --show-toplevel      # print top-level directory \n$ git rev-parse --git-dir            # print .git directory name",
            "title": "Create a central Repo"
        },
        {
            "location": "/Java/Java/",
            "text": "Install \nJava\n\u00b6\n\n\nJDK download\n\n\njava -version\n\n\n\n\n\nJava Tools\n\u00b6\n\n\nList 1\n\n\nList 2\n\n\n\n\nEclipse\n IDE\n\n\nMaven\n or Graddle build tool\n\n\nNexus\n private repository\n\n\nMaven public repository\n\n\nPhabrikator\n code review\n\n\nPhabrikator blog\n\n\n\n\n\n\nJenkins\n CI / CD automation server\n\n\nJProfiler\n\n\nFindBugs\n static analysis or \nChecker Framework\n\n\nCheckstyle\n coding standard checker\n\n\nStyle guidelines\n\n\n\n\n\n\n\n\nJava Libraries\n\u00b6\n\n\nLibraries\n\n\n\n\nLog4j\n\n\nSpring\n\n\nSpring Cloud for Amazon Web Services\n\n\nSpring boot code generator\n\n\n\n\n\n\nApache Commons\n\n\nGuava\n\n\nJackson JSON\n or \nGSON\n\n\nHibernate\n\non the JVM. \n\n\nPlay framework\n\n\nSpark web microframework\n\n\nAkka\n - actor model, to build highly concurrent, distributed, and resilient message-driven applications\n\n\n\n\nRandom Snippets\n\u00b6\n\n\nUUID\n.\nrandomUUID\n().\ntoString\n();",
            "title": "Java"
        },
        {
            "location": "/Java/Java/#install-java",
            "text": "JDK download  java -version",
            "title": "Install Java"
        },
        {
            "location": "/Java/Java/#java-tools",
            "text": "List 1  List 2   Eclipse  IDE  Maven  or Graddle build tool  Nexus  private repository  Maven public repository  Phabrikator  code review  Phabrikator blog    Jenkins  CI / CD automation server  JProfiler  FindBugs  static analysis or  Checker Framework  Checkstyle  coding standard checker  Style guidelines",
            "title": "Java Tools"
        },
        {
            "location": "/Java/Java/#java-libraries",
            "text": "Libraries   Log4j  Spring  Spring Cloud for Amazon Web Services  Spring boot code generator    Apache Commons  Guava  Jackson JSON  or  GSON  Hibernate \non the JVM.   Play framework  Spark web microframework  Akka  - actor model, to build highly concurrent, distributed, and resilient message-driven applications",
            "title": "Java Libraries"
        },
        {
            "location": "/Java/Java/#random-snippets",
            "text": "UUID . randomUUID (). toString ();",
            "title": "Random Snippets"
        },
        {
            "location": "/Java/Log4j/",
            "text": "Apache Log4j 2\n\u00b6\n\n\nhttp://www.tutorialspoint.com/log4j/log4j_quick_guide.htm\nhttp://www.tutorialspoint.com/log4j/index.htm\n\n\nKey Components\n\u00b6\n\n\n\n\nloggers: Responsible for capturing logging information.\n\n\nappenders: Responsible for publishing logging information to various preferred destinations.\n\n\nlayouts: Responsible for formatting logging information in different styles.\n\n\n\n\nThere are seven levels of logging defined within the API: OFF, DEBUG, INFO, ERROR, WARN, FATAL, and ALL.\n\n\nInstall\n\u00b6\n\n\nhttps://logging.apache.org/log4j/2.x/download.html\n\n\n$ gunzip apache-log4j-1.2.15.tar.gz\n$ tar -xvf apache-log4j-1.2.15.tar\n$ \npwd\n\n/usr/local/apache-log4j-1.2.15\n$ \nexport\n \nCLASSPATH\n=\n$CLASSPATH\n:/usr/local/apache-log4j-1.2.15/log4j-1.2.15.jar\n$ \nexport\n \nPATH\n=\n$PATH\n:/usr/local/apache-log4j-1.2.15/\n\n\n\n\n\nMaven Snippet\n\u00b6\n\n\n<dependencies>\n<dependency>\n<groupId>org.apache.logging.log4j</groupId>\n<artifactId>log4j-api</artifactId>\n<version>2.6.1</version>\n</dependency>\n<dependency>\n<groupId>org.apache.logging.log4j</groupId>\n<artifactId>log4j-core</artifactId>\n<version>2.6.1</version>\n</dependency>\n</dependencies>\n\n\n\n\n\nlog4j.properties\n\u00b6\n\n\nAll the libraries should be available in CLASSPATH and yourlog4j.properties file should be available in PATH.\n\n\n# Define the root logger with appender file\n\n\nlog\n \n=\n \n/usr/home/log4j\n\n\nlog4j.rootLogger\n \n=\n \nWARN, FILE\n\n\n\n# Define the file appender\n\n\nlog4j.appender.FILE\n=\norg.apache.log4j.FileAppender\n\n\nlog4j.appender.FILE.File\n=\n${log}/log.out\n\n\n\n# Define the layout for file appender\n\n\nlog4j.appender.FILE.layout\n=\norg.apache.log4j.PatternLayout\n\n\nlog4j.appender.FILE.layout.conversionPattern\n=\n%m%n\n\n\n\n\n\n\nSnippets\n\u00b6\n\n\nimport\n \norg.apache.logging.log4j.LogManager\n;\n\n\nimport\n \norg.apache.logging.log4j.Logger\n;\n\n\n\npublic\n \nclass\n \nMyTest\n \n{\n\n\n    \nprivate\n \nstatic\n \nfinal\n \nLogger\n \nlogger\n \n=\n \nLogManager\n.\ngetLogger\n();\n \n// equiv to  LogManager.getLogger(MyTest.class);\n\n    \nprivate\n \nstatic\n \nfinal\n \nLogger\n \nlogger\n \n=\n \nLogManager\n.\ngetLogger\n(\n\"HelloWorld\"\n);\n\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \n{\n\n         \nlogger\n.\nsetLevel\n(\nLevel\n.\nWARN\n);\n\n         \nlogger\n.\ninfo\n(\n\"Hello, World!\"\n);\n\n         \n// string interpolation\n\n         \nlogger\n.\ndebug\n(\n\"Logging in user {} with birthday {}\"\n,\n \nuser\n.\ngetName\n(),\n \nuser\n.\ngetBirthdayCalendar\n());\n\n\n         \n// pre-Java 8 style optimization: explicitly check the log level\n\n         \n// to make sure the expensiveOperation() method is only called if necessary\n\n         \nif\n \n(\nlogger\n.\nisTraceEnabled\n())\n \n{\n\n        \nlogger\n.\ntrace\n(\n\"Some long-running operation returned {}\"\n,\n \nexpensiveOperation\n());\n\n         \n}\n\n\n         \n// Java-8 style optimization: no need to explicitly check the log level:\n\n         \n// the lambda expression is not evaluated if the TRACE level is not enabledlogger.trace(\"Some long-running operation returned {}\", () -> expensiveOperation());\n\n         \n}\n\n\n}\n\n\n\n// FORMATTER LOGGER\n\n\npublic\n \nstatic\n \nLogger\n \nlogger\n \n=\n \nLogManager\n.\ngetFormatterLogger\n(\n\"Foo\"\n);\n\n\n\nlogger\n.\ndebug\n(\n\"Logging in user %s with birthday %s\"\n,\n \nuser\n.\ngetName\n(),\n \nuser\n.\ngetBirthdayCalendar\n());\n\n\nlogger\n.\ndebug\n(\n\"Logging in user %1$s with birthday %2$tm %2$te,%2$tY\"\n,\n \nuser\n.\ngetName\n(),\n \nuser\n.\ngetBirthdayCalendar\n());\n\n\n//\n\n\nlogger\n.\ndebug\n(\n\"Logging in user {} with birthday {}\"\n,\n \nuser\n.\ngetName\n(),\n \nuser\n.\ngetBirthdayCalendar\n());",
            "title": "Log4j"
        },
        {
            "location": "/Java/Log4j/#apache-log4j-2",
            "text": "http://www.tutorialspoint.com/log4j/log4j_quick_guide.htm\nhttp://www.tutorialspoint.com/log4j/index.htm",
            "title": "Apache Log4j 2"
        },
        {
            "location": "/Java/Log4j/#key-components",
            "text": "loggers: Responsible for capturing logging information.  appenders: Responsible for publishing logging information to various preferred destinations.  layouts: Responsible for formatting logging information in different styles.   There are seven levels of logging defined within the API: OFF, DEBUG, INFO, ERROR, WARN, FATAL, and ALL.",
            "title": "Key Components"
        },
        {
            "location": "/Java/Log4j/#install",
            "text": "https://logging.apache.org/log4j/2.x/download.html  $ gunzip apache-log4j-1.2.15.tar.gz\n$ tar -xvf apache-log4j-1.2.15.tar\n$  pwd \n/usr/local/apache-log4j-1.2.15\n$  export   CLASSPATH = $CLASSPATH :/usr/local/apache-log4j-1.2.15/log4j-1.2.15.jar\n$  export   PATH = $PATH :/usr/local/apache-log4j-1.2.15/",
            "title": "Install"
        },
        {
            "location": "/Java/Log4j/#maven-snippet",
            "text": "<dependencies>\n<dependency>\n<groupId>org.apache.logging.log4j</groupId>\n<artifactId>log4j-api</artifactId>\n<version>2.6.1</version>\n</dependency>\n<dependency>\n<groupId>org.apache.logging.log4j</groupId>\n<artifactId>log4j-core</artifactId>\n<version>2.6.1</version>\n</dependency>\n</dependencies>",
            "title": "Maven Snippet"
        },
        {
            "location": "/Java/Log4j/#log4jproperties",
            "text": "All the libraries should be available in CLASSPATH and yourlog4j.properties file should be available in PATH.  # Define the root logger with appender file  log   =   /usr/home/log4j  log4j.rootLogger   =   WARN, FILE  # Define the file appender  log4j.appender.FILE = org.apache.log4j.FileAppender  log4j.appender.FILE.File = ${log}/log.out  # Define the layout for file appender  log4j.appender.FILE.layout = org.apache.log4j.PatternLayout  log4j.appender.FILE.layout.conversionPattern = %m%n",
            "title": "log4j.properties"
        },
        {
            "location": "/Java/Log4j/#snippets",
            "text": "import   org.apache.logging.log4j.LogManager ;  import   org.apache.logging.log4j.Logger ;  public   class   MyTest   { \n\n     private   static   final   Logger   logger   =   LogManager . getLogger ();   // equiv to  LogManager.getLogger(MyTest.class); \n     private   static   final   Logger   logger   =   LogManager . getLogger ( \"HelloWorld\" ); \n\n     public   static   void   main ( String []   args )   { \n          logger . setLevel ( Level . WARN ); \n          logger . info ( \"Hello, World!\" ); \n          // string interpolation \n          logger . debug ( \"Logging in user {} with birthday {}\" ,   user . getName (),   user . getBirthdayCalendar ()); \n\n          // pre-Java 8 style optimization: explicitly check the log level \n          // to make sure the expensiveOperation() method is only called if necessary \n          if   ( logger . isTraceEnabled ())   { \n         logger . trace ( \"Some long-running operation returned {}\" ,   expensiveOperation ()); \n          } \n\n          // Java-8 style optimization: no need to explicitly check the log level: \n          // the lambda expression is not evaluated if the TRACE level is not enabledlogger.trace(\"Some long-running operation returned {}\", () -> expensiveOperation()); \n          }  }  // FORMATTER LOGGER  public   static   Logger   logger   =   LogManager . getFormatterLogger ( \"Foo\" );  logger . debug ( \"Logging in user %s with birthday %s\" ,   user . getName (),   user . getBirthdayCalendar ());  logger . debug ( \"Logging in user %1$s with birthday %2$tm %2$te,%2$tY\" ,   user . getName (),   user . getBirthdayCalendar ());  //  logger . debug ( \"Logging in user {} with birthday {}\" ,   user . getName (),   user . getBirthdayCalendar ());",
            "title": "Snippets"
        },
        {
            "location": "/Linux/Linux/",
            "text": "Vim\n\u00b6\n\n\nVim Commands Cheat Sheet\n\n\n:q\n:q!\n:wq\n:wq {file}\n\n:e[dit] {file}\n\ni  insert\ndd  delete [count] lines\n\n\n\n\n\nBash\n\u00b6\n\n\n\n\nBASH Programming - Introduction\n\n\nBash CheatSheet for UNIX Systems\n\n\nBash Cheat Sheet\n\n\n\n\n#!/bin/bash\n\n\n\nvarname\n=\nvalue\n\necho\n \n$varname\n\n\n\n\n\n\nDon't forget chmod +x filename\n\n\nAmazon Linux\n\u00b6\n\n\nAmazon Linux Basics\n\n\nAdding Packages\n\u00b6\n\n\nsudo yum update -y                  \n# all packages\n\nsudo yum install -y package_name\nsudo yum install -y httpd24 php56 mysql55-server php56-mysqlnd\n\n\n\n\n\nStart a Service\n\u00b6\n\n\nsudo service docker start\nsudo service jenkins start\nsudo service nginx start\n\n\n\n\n\nAutostarting a service on Amazon Linux\n\u00b6\n\n\n\n\nTutorial on \"Chkconfig\" Command in Linux with Examples\n\n\nman page\n\n\n\n\n# check a service is configured for startup\n\nsudo chkconfig sshd\n\necho\n \n$?\n  \n# 0 = configured for startup\n\n\n# or\n\nsudo chkconfig --list mysqld\nsudo chkconfig --list         \n# all services\n\n\n\n# add a service\n\nsudo chkconfig --add vsftpd\nsudo chkconfig mysqld on\nsudo chkconfig --level \n3\n httpd on  \n# specific runlevel\n\n\n\n\n\n\nLinux Boot Process\n\u00b6\n\n\n\n\n Linux Boot Process\n\n\n Scripts in /etc/init.d \n\n\n\n\nYou can also use a \n/etc/rc.d/rc.local\n script.\n\n\nRunning Commands on your Linux Instance at Launch\n\u00b6\n\n\n\n\nPaste a user data script into the \nUser data\n field\n\n\n\n\n#!/bin/bash\n\nyum update -y\nyum install -y httpd24 php56 mysql55-server php56-mysqlnd\nservice httpd start\nchkconfig httpd on\ngroupadd www\nusermod -a -G www ec2-user\nchown -R root:www /var/www\nchmod \n2775\n /var/www\nfind /var/www -type d -exec chmod \n2775\n \n{}\n +\nfind /var/www -type f -exec chmod \n0664\n \n{}\n +\n\necho\n \n\"<?php phpinfo(); ?>\"\n > /var/www/html/phpinfo.php\n\n\n\n\n\n\n\nOr use \ncloud-init\n\n\ncloud-init for AWS EC2\n\n\ncloud-init docs\n\n\n\n\n\n\n\n\nFile location: \n/etc/sysconfig/cloudinit\n\n\nCloud-init output log file: \n/var/log/cloud-init-output.log\n\n\nInstall the SSM Agent on EC2 Instances at Start-Up\n\u00b6\n\n\n#!/bin/bash\n\n\ncd\n /tmp\ncurl https://amazon-ssm-region.s3.amazonaws.com/latest/linux_amd64/amazon-ssm-agent.rpm -o amazon-ssm-agent.rpm\nyum install -y amazon-ssm-agent.rpm\n\n\n\n\n\nLinux desktop\n\u00b6\n\n\nHow can I connect to an Amazon EC2 Linux instance with desktop functionality from Windows?",
            "title": "Linux"
        },
        {
            "location": "/Linux/Linux/#vim",
            "text": "Vim Commands Cheat Sheet  :q\n:q!\n:wq\n:wq {file}\n\n:e[dit] {file}\n\ni  insert\ndd  delete [count] lines",
            "title": "Vim"
        },
        {
            "location": "/Linux/Linux/#bash",
            "text": "BASH Programming - Introduction  Bash CheatSheet for UNIX Systems  Bash Cheat Sheet   #!/bin/bash  varname = value echo   $varname   Don't forget chmod +x filename",
            "title": "Bash"
        },
        {
            "location": "/Linux/Linux/#amazon-linux",
            "text": "Amazon Linux Basics",
            "title": "Amazon Linux"
        },
        {
            "location": "/Linux/Linux/#adding-packages",
            "text": "sudo yum update -y                   # all packages \nsudo yum install -y package_name\nsudo yum install -y httpd24 php56 mysql55-server php56-mysqlnd",
            "title": "Adding Packages"
        },
        {
            "location": "/Linux/Linux/#start-a-service",
            "text": "sudo service docker start\nsudo service jenkins start\nsudo service nginx start",
            "title": "Start a Service"
        },
        {
            "location": "/Linux/Linux/#autostarting-a-service-on-amazon-linux",
            "text": "Tutorial on \"Chkconfig\" Command in Linux with Examples  man page   # check a service is configured for startup \nsudo chkconfig sshd echo   $?    # 0 = configured for startup  # or \nsudo chkconfig --list mysqld\nsudo chkconfig --list          # all services  # add a service \nsudo chkconfig --add vsftpd\nsudo chkconfig mysqld on\nsudo chkconfig --level  3  httpd on   # specific runlevel",
            "title": "Autostarting a service on Amazon Linux"
        },
        {
            "location": "/Linux/Linux/#linux-boot-process",
            "text": "Linux Boot Process   Scripts in /etc/init.d    You can also use a  /etc/rc.d/rc.local  script.",
            "title": "Linux Boot Process"
        },
        {
            "location": "/Linux/Linux/#running-commands-on-your-linux-instance-at-launch",
            "text": "Paste a user data script into the  User data  field   #!/bin/bash \nyum update -y\nyum install -y httpd24 php56 mysql55-server php56-mysqlnd\nservice httpd start\nchkconfig httpd on\ngroupadd www\nusermod -a -G www ec2-user\nchown -R root:www /var/www\nchmod  2775  /var/www\nfind /var/www -type d -exec chmod  2775   {}  +\nfind /var/www -type f -exec chmod  0664   {}  + echo   \"<?php phpinfo(); ?>\"  > /var/www/html/phpinfo.php   Or use  cloud-init  cloud-init for AWS EC2  cloud-init docs     File location:  /etc/sysconfig/cloudinit  Cloud-init output log file:  /var/log/cloud-init-output.log",
            "title": "Running Commands on your Linux Instance at Launch"
        },
        {
            "location": "/Linux/Linux/#install-the-ssm-agent-on-ec2-instances-at-start-up",
            "text": "#!/bin/bash  cd  /tmp\ncurl https://amazon-ssm-region.s3.amazonaws.com/latest/linux_amd64/amazon-ssm-agent.rpm -o amazon-ssm-agent.rpm\nyum install -y amazon-ssm-agent.rpm",
            "title": "Install the SSM Agent on EC2 Instances at Start-Up"
        },
        {
            "location": "/Linux/Linux/#linux-desktop",
            "text": "How can I connect to an Amazon EC2 Linux instance with desktop functionality from Windows?",
            "title": "Linux desktop"
        },
        {
            "location": "/Markup_and_Documentation/Jekyll/",
            "text": "Jekyll Basics\n\u00b6\n\n\nJekyll Home Page\n\n\nCheck out the \nJekyll docs\n for more info on how to get the most out of Jekyll. File all bugs/feature requests at \nJekyll\u2019s GitHub repo\n. If you have questions, you can ask them on \nJekyll Talk\n.\n\n\nJekyll source code\n\n\nGuide to basic Jekyll\n\n\nJekyll Install How-To\n\u00b6\n\n\nInstall Instructions\n\n\n\n\n\n\nInstall Ruby via \nRubyInstaller\n\n\n\n\n\n\nUpdate RubyGems\n\n\n\n\n\n\n$ gem update --system\n\n\n\n\n\n\n\nInstall Jekyll\n\n\n\n\n$ gem install jekyll\n\n\n\n\n\n\n\nTest Jekyll\n\n\n\n\n$ jekyll --version\n$ gem list jekyll\n\n\n\n\n\n\n\nInstall bundler\n\n\n\n\n$ gem install bundler\n\n\n\n\n\nBundler\n is a gem that manages other Ruby gems. It makes sure your gems and gem versions are compatible, and that you have all necessary dependencies each gem requires.\n\n\n\n\nCreate a new site\n\n\n\n\n# Create a new Jekyll site at ./myblog\n\n~ $ jekyll new myblog\n\n\n# Change into your new directory\n\n~ $ \ncd\n myblog\n\n\n\n\n\nJekyll installs a site that uses a gem-based theme called Minima.\n\n\nWith gem-based themes, some of the site\u2019s directories (such as the assets, _layouts, _includes, and _sass directories) are stored in the theme\u2019s gem, hidden from your immediate view. Yet all of the necessary directories will be read and processed during Jekyll\u2019s build process.\n\n\n\n\nBuild site locally\n\n\n\n\n# Build the site on the preview server\n\n~/myblog $ bundle \nexec\n jekyll serve\n\n\n\n\n\nNow browse to \nlocalhost:4000\n\n\nJekyll Quickstart\n\n\nWhen you run bundle exec jekyll serve, Bundler uses the gems and versions as specified in Gemfile.lock to ensure your Jekyll site builds with no compatibility or dependency conflicts.\n\n\nThe Gemfile and Gemfile.lock files inform Bundler about the gem requirements in your site. If your site doesn\u2019t have these Gemfiles, you can omit bundle exec and just run jekyll serve.\n\n\n$ jekyll build\n\n# => The current folder will be generated into ./_site\n\n\n$ jekyll serve\n\n# => A development server will run at http://localhost:4000/\n\n\n# Auto-regeneration: enabled. Use `--no-watch` to disable.\n\n\n\n\n\n\nPlugins\n\u00b6\n\n\n$ gem install jekyll-sitemap\n$ gem install jekyll-feed\netc...\n\n\n\n\n\nAdd to \n_config.yml\n\n\ngems:\n  - jekyll-paginate\n  - jekyll-feed\n  - jekyll-sitemap\n``\n\n\n# Custom Search\n\n[Adding a custom Google search](http://digitaldrummerj.me/blogging-on-github-part-7-adding-a-custom-google-search/)\n\n\n# Themes\n\n[Theme documentation](https://jekyllrb.com/docs/themes/)\n\nTo change theme, search for jekyll theme on [RubyGems](https://rubygems.org/search?utf8=%E2%9C%93&query=jekyll-theme) to find other gem-based themes.\n\nAdd the theme to your site\u2019s Gemfile:\n\n\n\n\n\ngem \"jekyll-theme-tactile\"\n\n\n```bash\n$ bundle install\n\n# check proper install\n$ bundle show jekyll-theme-tactile\n\n\n\n\n\nAdd the following to your site\u2019s _config.yml to activate the theme:\n\n\ntheme: jekyll-theme-tactile\n\n\n\n\n\nBuild your site:\n\n\n$ bundle \nexec\n jekyll serve\n\n\n\n\n\nYou can find out info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at \njekyllrb.com\n\n\nYou can find the source code for the Jekyll minima theme at:\n\nminima\n\n\n\n\nYou\u2019ll find this post in your \n_posts\n directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run \njekyll serve\n, which launches a web server and auto-regenerates your site when a file is updated.\n\n\nTo add new posts, simply add a file in the \n_posts\n directory that follows the convention \nYYYY-MM-DD-name-of-post.ext\n and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.\n\n\nJekyll also offers powerful support for code snippets:\n\n\n{% highlight ruby %}\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')\n\n\n=> prints 'Hi, Tom' to STDOUT.\n\u00b6\n\n\n{% endhighlight %}",
            "title": "Jekyll"
        },
        {
            "location": "/Markup_and_Documentation/Jekyll/#jekyll-basics",
            "text": "Jekyll Home Page  Check out the  Jekyll docs  for more info on how to get the most out of Jekyll. File all bugs/feature requests at  Jekyll\u2019s GitHub repo . If you have questions, you can ask them on  Jekyll Talk .  Jekyll source code  Guide to basic Jekyll",
            "title": "Jekyll Basics"
        },
        {
            "location": "/Markup_and_Documentation/Jekyll/#jekyll-install-how-to",
            "text": "Install Instructions    Install Ruby via  RubyInstaller    Update RubyGems    $ gem update --system   Install Jekyll   $ gem install jekyll   Test Jekyll   $ jekyll --version\n$ gem list jekyll   Install bundler   $ gem install bundler  Bundler  is a gem that manages other Ruby gems. It makes sure your gems and gem versions are compatible, and that you have all necessary dependencies each gem requires.   Create a new site   # Create a new Jekyll site at ./myblog \n~ $ jekyll new myblog # Change into your new directory \n~ $  cd  myblog  Jekyll installs a site that uses a gem-based theme called Minima.  With gem-based themes, some of the site\u2019s directories (such as the assets, _layouts, _includes, and _sass directories) are stored in the theme\u2019s gem, hidden from your immediate view. Yet all of the necessary directories will be read and processed during Jekyll\u2019s build process.   Build site locally   # Build the site on the preview server \n~/myblog $ bundle  exec  jekyll serve  Now browse to  localhost:4000  Jekyll Quickstart  When you run bundle exec jekyll serve, Bundler uses the gems and versions as specified in Gemfile.lock to ensure your Jekyll site builds with no compatibility or dependency conflicts.  The Gemfile and Gemfile.lock files inform Bundler about the gem requirements in your site. If your site doesn\u2019t have these Gemfiles, you can omit bundle exec and just run jekyll serve.  $ jekyll build # => The current folder will be generated into ./_site \n\n$ jekyll serve # => A development server will run at http://localhost:4000/  # Auto-regeneration: enabled. Use `--no-watch` to disable.",
            "title": "Jekyll Install How-To"
        },
        {
            "location": "/Markup_and_Documentation/Jekyll/#plugins",
            "text": "$ gem install jekyll-sitemap\n$ gem install jekyll-feed\netc...  Add to  _config.yml  gems:\n  - jekyll-paginate\n  - jekyll-feed\n  - jekyll-sitemap\n``\n\n\n# Custom Search\n\n[Adding a custom Google search](http://digitaldrummerj.me/blogging-on-github-part-7-adding-a-custom-google-search/)\n\n\n# Themes\n\n[Theme documentation](https://jekyllrb.com/docs/themes/)\n\nTo change theme, search for jekyll theme on [RubyGems](https://rubygems.org/search?utf8=%E2%9C%93&query=jekyll-theme) to find other gem-based themes.\n\nAdd the theme to your site\u2019s Gemfile:  gem \"jekyll-theme-tactile\"  ```bash\n$ bundle install\n\n# check proper install\n$ bundle show jekyll-theme-tactile  Add the following to your site\u2019s _config.yml to activate the theme:  theme: jekyll-theme-tactile  Build your site:  $ bundle  exec  jekyll serve  You can find out info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at  jekyllrb.com  You can find the source code for the Jekyll minima theme at: minima   You\u2019ll find this post in your  _posts  directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run  jekyll serve , which launches a web server and auto-regenerates your site when a file is updated.  To add new posts, simply add a file in the  _posts  directory that follows the convention  YYYY-MM-DD-name-of-post.ext  and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.  Jekyll also offers powerful support for code snippets:  {% highlight ruby %}\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')",
            "title": "Plugins"
        },
        {
            "location": "/Markup_and_Documentation/Jekyll/#prints-hi-tom-to-stdout",
            "text": "{% endhighlight %}",
            "title": "=&gt; prints 'Hi, Tom' to STDOUT."
        },
        {
            "location": "/Markup_and_Documentation/Markdown/",
            "text": "Markdown Essentials\n\u00b6\n\n\nMarkdown main site\n\n\nGitHub Flavored Markdown Guide\n\n\nBasics\n\u00b6\n\n\nA paragraph is one or more consecutive lines of text separated by one or more blank lines. A blank line contains nothing but spaces or tabs. \n\n\nDo not indent normal paragraphs with spaces or tabs. Indent at least 4 spaces or a tab for code blocks.\n\n\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\n\n\n\n\n\nEmphasis\n\u00b6\n\n\n*single asterisks*\n_single underscores_\n**double asterisks**\n__double underscores__\n\nEmphasis can be used in the mi\\*dd\\*le of a word.\n\n\n\n\n\nHeaders\n\u00b6\n\n\n# H1\n## H2\n### H3\n#### H4\n##### H5\n###### H6\n\nAlt-H1\n======\n\nAlt-H2\n------\n\n\n\n\n\nLinks and Images\n\u00b6\n\n\n[ Text for the link ](URL)\n\nThis is [an example][id] reference-style link.\n[id]: http://example.com/  \"Optional Title Here\"\n\n![Alt text](/path/to/img.jpg \"Optional title\")\n\n\n\n\n\nCode\n\u00b6\n\n\n`span of code`\n\n\n```python\n\ndef wiki_rocks(text):\n    formatter = lambda t: \"funky\"+t\n    return formatter(text)\n```\n\n\n\n\n\nwill be displayed as\n\n\ndef\n \nwiki_rocks\n(\ntext\n):\n\n    \nformatter\n \n=\n \nlambda\n \nt\n:\n \n\"funky\"\n+\nt\n\n    \nreturn\n \nformatter\n(\ntext\n)\n\n\n\n\n\n\nBlockquotes\n\u00b6\n\n\n> This is a blockquote with two paragraphs. \n> \n> Second paragraph.\n\n\n\n\n\nGitHub Pages\n\u00b6\n\n\nGitHub Pages documentation\n \n\n\nGitHub Pages site will use the layout and styles from the Jekyll theme you have selected in your \nrepository settings\n. The name of this theme is saved in the Jekyll \n_config.yml\n configuration file.\n\n\nBitbucket\n\u00b6\n\n\nBitbucket doesn't support arbitrary HTML in Markdown, it instead uses safe mode. \n\nSafe mode\n requires that you replace, remove, or escape HTML tags appropriately.\n\n\nCode highlighting to bitbucket README.md written in Python Markdown \n\n\n    \nfriends\n \n=\n \n[\n'john'\n,\n \n'pat'\n,\n \n'gary'\n,\n \n'michael'\n]\n\n    \nfor\n \ni\n,\n \nname\n \nin\n \nenumerate\n(\nfriends\n):\n\n        \nprint\n \n\"iteration {iteration} is {name}\"\n.\nformat\n(\niteration\n=\ni\n,\n \nname\n=\nname\n)\n\n\n\n\n\n\nPython markdown main site\n \n\n\nCloning your Bitbucket Wiki\n\u00b6\n\n\n$ git clone http://bitbucket.org/MY_USER/MY_REPO/wiki",
            "title": "Markdown"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#markdown-essentials",
            "text": "Markdown main site  GitHub Flavored Markdown Guide",
            "title": "Markdown Essentials"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#basics",
            "text": "A paragraph is one or more consecutive lines of text separated by one or more blank lines. A blank line contains nothing but spaces or tabs.   Do not indent normal paragraphs with spaces or tabs. Indent at least 4 spaces or a tab for code blocks.  Syntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)",
            "title": "Basics"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#emphasis",
            "text": "*single asterisks*\n_single underscores_\n**double asterisks**\n__double underscores__\n\nEmphasis can be used in the mi\\*dd\\*le of a word.",
            "title": "Emphasis"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#headers",
            "text": "# H1\n## H2\n### H3\n#### H4\n##### H5\n###### H6\n\nAlt-H1\n======\n\nAlt-H2\n------",
            "title": "Headers"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#links-and-images",
            "text": "[ Text for the link ](URL)\n\nThis is [an example][id] reference-style link.\n[id]: http://example.com/  \"Optional Title Here\"\n\n![Alt text](/path/to/img.jpg \"Optional title\")",
            "title": "Links and Images"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#code",
            "text": "`span of code`\n\n\n```python\n\ndef wiki_rocks(text):\n    formatter = lambda t: \"funky\"+t\n    return formatter(text)\n```  will be displayed as  def   wiki_rocks ( text ): \n     formatter   =   lambda   t :   \"funky\" + t \n     return   formatter ( text )",
            "title": "Code"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#blockquotes",
            "text": "> This is a blockquote with two paragraphs. \n> \n> Second paragraph.",
            "title": "Blockquotes"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#github-pages",
            "text": "GitHub Pages documentation    GitHub Pages site will use the layout and styles from the Jekyll theme you have selected in your  repository settings . The name of this theme is saved in the Jekyll  _config.yml  configuration file.",
            "title": "GitHub Pages"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#bitbucket",
            "text": "Bitbucket doesn't support arbitrary HTML in Markdown, it instead uses safe mode.  Safe mode  requires that you replace, remove, or escape HTML tags appropriately.  Code highlighting to bitbucket README.md written in Python Markdown        friends   =   [ 'john' ,   'pat' ,   'gary' ,   'michael' ] \n     for   i ,   name   in   enumerate ( friends ): \n         print   \"iteration {iteration} is {name}\" . format ( iteration = i ,   name = name )   Python markdown main site",
            "title": "Bitbucket"
        },
        {
            "location": "/Markup_and_Documentation/Markdown/#cloning-your-bitbucket-wiki",
            "text": "$ git clone http://bitbucket.org/MY_USER/MY_REPO/wiki",
            "title": "Cloning your Bitbucket Wiki"
        },
        {
            "location": "/Markup_and_Documentation/MkDocs/",
            "text": "This website is generated by \nmkdocs.org\n and the \nMaterial Theme\n.\n\n\nBasic MkDocs Commands\n\u00b6\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nInstall and documentation generation\n\u00b6\n\n\nmkdocs.org\n.\n\n\nTo install MkDocs / create a new documentation site:\n\n\n$ pip install mkdocs\n$ mkdocs new documentation\n\n\n\n\n\nTo build the documentation site:\n\n\n$ \ncd\n documentation\n$ mkdocs build\n\n\n\n\n\nTo start the live-reloading docs server - \nhttp://localhost:8000/\n\n\n$ mkdocs serve\n\n\n\n\n\nMkDocs can use the ghp-import tool to commit to the gh-pages branch and push the gh-pages branch to GitHub Pages:\n\n\n$ mkdocs gh-deploy\n\n\n\n\n\nMkDocs project layout\n\u00b6\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "MkDocs"
        },
        {
            "location": "/Markup_and_Documentation/MkDocs/#basic-mkdocs-commands",
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.",
            "title": "Basic MkDocs Commands"
        },
        {
            "location": "/Markup_and_Documentation/MkDocs/#install-and-documentation-generation",
            "text": "mkdocs.org .  To install MkDocs / create a new documentation site:  $ pip install mkdocs\n$ mkdocs new documentation  To build the documentation site:  $  cd  documentation\n$ mkdocs build  To start the live-reloading docs server -  http://localhost:8000/  $ mkdocs serve  MkDocs can use the ghp-import tool to commit to the gh-pages branch and push the gh-pages branch to GitHub Pages:  $ mkdocs gh-deploy",
            "title": "Install and documentation generation"
        },
        {
            "location": "/Markup_and_Documentation/MkDocs/#mkdocs-project-layout",
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "MkDocs project layout"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/",
            "text": "reStructuredText\n\u00b6\n\n\nreStructuredText Quick Ref\n\n\nreStructuredText Cheat Sheet\n  (see below)\n\n\nreST Short Overview\n\u00b6\n\n\nAll reST files use an indentation of 3 spaces; no tabs are allowed. \nThe maximum line length is 80 characters for normal text, but tables, \ndeeply indented code samples and long links may extend beyond that. \nCode example bodies should use normal Python 4-space indentation.\nParagraphs are simply chunks of text separated by one or more blank lines. \nAs in Python, indentation is significant in reST, so all lines of the same\nparagraph must be left-aligned to the same level of indentation.\n\n\nSection headers are created by underlining (and optionally overlining)\nthe section title with a punctuation character, at least as long as the text:\n\n\n=================\nThis is a heading\n=================\n# with overline, for parts\n* with overline, for chapters\n= for sections\n- for subsections\n^ for subsubsections\n\" for paragraphs\n\none asterisk: *text* for emphasis (italics),\ntwo asterisks: **text** for strong emphasis (boldface), and\nbackquotes: ``text`` for code samples.\nescape with a backslash \\\n\n* This is a bulleted list.\n* It has two items, the second\n  item uses two lines.\n\n1. This is a numbered list.\n2. It has two items too.\n\n. This is a numbered list.\n. It has two items too.\n\n\n\n\n\nNested lists are possible, but be aware that they must be separated from the \nparent list items by blank lines\n\n\nSource Code Double Colon\n\u00b6\n\n\nThis is a normal text paragraph. The next paragraph is a code sample::\n\n       It is not processed in any way, except\n       that the indentation is removed.\n\n       It can span multiple lines.\n\nThis is a normal text paragraph again.\n\n\n\n\n\nLinks\n\u00b6\n\n\n`Link text <http://target>`_ for inline web links.\n\n\n\n\n\nDefinitions\n\u00b6\n\n\nterm (up to a line of text)\n   Definition of the term, which must be indented and \n   can even consist of multiple paragraphs\n\nnext term\n   Description.\n\n\n\n\n\nFootnotes\n\u00b6\n\n\nLorem ipsum [#]_ dolor sit amet ... [#]_\n\n\n\n\n\nUse of reStructuredText in Python docstrings\n\u00b6\n\n\nSee http://infinitemonkeycorps.net/docs/pph/\n\n\n    # Typical function documentation: \n\n    :param volume_id: The ID of the EBS volume to be attached.\n    :type volume_id: str\n\n    :param instance_id: The ID of the EC2 instance \n    :type instance_id: str\n\n    :return: `Reverse geocoder return value`_ dictionary giving closest\n        address(es) to `(lat, lng)`\n    :rtype: dict\n    :raises GoogleMapsError: If the coordinates could not be reverse geocoded.\n\n    Keyword arguments and return value are identical to those of :meth:`geocode()`.\n\n    .. _`Reverse geocoder return value`:\n        http://code.google.com/apis/maps/documentation/geocoding/index.html#ReverseGeocoding\n\n\n\n\n\n\n\nNormal docstring formatting conventions apply: see PEP 257.\n\n\nIdentifier references go in `backticks`.\n\n\n:param lat: some text\n  \ndocuments parameters\n\n\n:type lat: float\n \ndocuments parameter types\n\n\n:return:\n dictionary giving some info... \ndocuments return values\n\n\n:rtype: dict\n \ndocuments return type\n\n\n:raises SomeError:\n sometext...  \ndocuments exceptions raised\n\n\n>>>\n \nstarts a doctest and is automatically formatted as code\n\n\nCode can also be indicated by indenting four spaces or preceding with \n::\n and a blank line\n\n\nLink to other methods, functions, classes, modules with :meth:\nmymethod\n, \n\n\n:func:\nmyfunc\n, :class:\nmyclass\n, and :mod:\nmymodule\n.\n\n\nHyperlink names go in backticks with a trailing underscore: \nGoogle\n_\n\n\nTargets can be defined anywhere with: \n.. _Google: http://www.google.com/\n\n\n\n\nExplicit Markup\n\u00b6\n\n\nAn explicit markup block begins with a line starting with .. followed by\nwhitespace and is terminated by the next paragraph at the same level of \nindentation. (There needs to be a blank line between explicit markup\nand normal paragraphs.\n\n\n.. sectionauthor:: Guido van Rossum <guido@python.org>\n\n.. rubric:: Footnotes\n\n.. [#] Text of the first footnote.\n.. [#] Text of the second footnote.\n\n\n:mod:`parrot` -- Dead parrot access\n===================================\n\n.. module:: parrot\n   :platform: Unix, Windows\n   :synopsis: Analyze and reanimate dead parrots.\n.. moduleauthor:: Eric Cleese <eric@python.invalid>\n.. moduleauthor:: John Idle <john@python.invalid>\n\n.. function:: repeat([repeat=3[, number=1000000]])\n              repeat(y, z)\n   :bar: no\n\n   Return a line of text input from the user.\n\n\n.. class:: Spam\n\n      Description of the class.\n\n      .. data:: ham\n\n         Description of the attribute.\n\n\n\n\n\nInline markup\n\u00b6\n\n\n:rolename:`content`  or  :role:`title <target>`\n\n:meth:`~Queue.Queue.get` will refer to Queue.Queue.get but only display get as the link text.\n\n\n\n\n\nThe following roles refer to objects in modules and are possibly hyperlinked \nif a matching identifier is found:\n\n\nmod\n\n\nThe name of a module; a dotted name may be used. This should also be used for package names.\n\n\nfunc\n\n\nThe name of a Python function; dotted names may be used. The role text should not include trailing parentheses to enhance readability. The parentheses are stripped when searching for identifiers.\n\n\ndata\n\n\nThe name of a module-level variable or constant.\n\n\nconst\n\n\nThe name of a \u201cdefined\u201d constant. This may be a C-language #define or a Python variable that is not intended to be changed.\n\n\nclass\n\n\nA class name; a dotted name may be used.\n\n\nmeth\n\n\nThe name of a method of an object. The role text should include the type name and the method name. A dotted name may be used.\n\n\nattr\n\n\nThe name of a data attribute of an object.\n\n\nexc\n\n\nThe name of an exception. A dotted name may be used.\n\n\nOfficial reStructuredText Cheatsheet\n\u00b6\n\n\n=====================================================\n The reStructuredText_ Cheat Sheet: Syntax Reminders\n=====================================================\n:Info: See <http://docutils.sf.net/rst.html> for introductory docs.\n:Author: David Goodger <goodger@python.org>\n:Date: $Date: 2013-02-20 01:10:53 +0000 (Wed, 20 Feb 2013) $\n:Revision: $Revision: 7612 $\n:Description: This is a \"docinfo block\", or bibliographic field list\n\n.. NOTE:: If you are reading this as HTML, please read\n   `<cheatsheet.txt>`_ instead to see the input syntax examples!\n\nSection Structure\n=================\nSection titles are underlined or overlined & underlined.\n\nBody Elements\n=============\nGrid table:\n\n+--------------------------------+-----------------------------------+\n| Paragraphs are flush-left,     | Literal block, preceded by \"::\":: |\n| separated by blank lines.      |                                   |\n|                                |     Indented                      |\n|     Block quotes are indented. |                                   |\n+--------------------------------+ or::                              |\n| >>> print 'Doctest block'      |                                   |\n| Doctest block                  | > Quoted                          |\n+--------------------------------+-----------------------------------+\n| | Line blocks preserve line breaks & indents. [new in 0.3.6]       |\n| |     Useful for addresses, verse, and adornment-free lists; long  |\n|       lines can be wrapped with continuation lines.                |\n+--------------------------------------------------------------------+\n\nSimple tables:\n\n================  ============================================================\nList Type         Examples (syntax in the `text source <cheatsheet.txt>`_)\n================  ============================================================\nBullet list       * items begin with \"-\", \"+\", or \"*\"\nEnumerated list   1. items use any variation of \"1.\", \"A)\", and \"(i)\"\n                  #. also auto-enumerated\nDefinition list   Term is flush-left : optional classifier\n                      Definition is indented, no blank line between\nField list        :field name: field body\nOption list       -o  at least 2 spaces between option & description\n================  ============================================================\n\n================  ============================================================\nExplicit Markup   Examples (visible in the `text source`_)\n================  ============================================================\nFootnote          .. [1] Manually numbered or [#] auto-numbered\n                     (even [#labelled]) or [*] auto-symbol\nCitation          .. [CIT2002] A citation.\nHyperlink Target  .. _reStructuredText: http://docutils.sf.net/rst.html\n                  .. _indirect target: reStructuredText_\n                  .. _internal target:\nAnonymous Target  __ http://docutils.sf.net/docs/ref/rst/restructuredtext.html\nDirective (\"::\")  .. image:: images/biohazard.png\nSubstitution Def  .. |substitution| replace:: like an inline directive\nComment           .. is anything else\nEmpty Comment     (\"..\" on a line by itself, with blank lines before & after,\n                  used to separate indentation contexts)\n================  ============================================================\n\nInline Markup\n=============\n*emphasis*; **strong emphasis**; `interpreted text`; `interpreted text\nwith role`:emphasis:; ``inline literal text``; standalone hyperlink,\nhttp://docutils.sourceforge.net; named reference, reStructuredText_;\n`anonymous reference`__; footnote reference, [1]_; citation reference,\n[CIT2002]_; |substitution|; _`inline internal target`.\n\f\nDirective Quick Reference\n=========================\nSee <http://docutils.sf.net/docs/ref/rst/directives.html> for full info.\n\n================  ============================================================\nDirective Name    Description (Docutils version added to, in [brackets])\n================  ============================================================\nattention         Specific admonition; also \"caution\", \"danger\",\n                  \"error\", \"hint\", \"important\", \"note\", \"tip\", \"warning\"\nadmonition        Generic titled admonition: ``.. admonition:: By The Way``\nimage             ``.. image:: picture.png``; many options possible\nfigure            Like \"image\", but with optional caption and legend\ntopic             ``.. topic:: Title``; like a mini section\nsidebar           ``.. sidebar:: Title``; like a mini parallel document\nparsed-literal    A literal block with parsed inline markup\nrubric            ``.. rubric:: Informal Heading``\nepigraph          Block quote with class=\"epigraph\"\nhighlights        Block quote with class=\"highlights\"\npull-quote        Block quote with class=\"pull-quote\"\ncompound          Compound paragraphs [0.3.6]\ncontainer         Generic block-level container element [0.3.10]\ntable             Create a titled table [0.3.1]\nlist-table        Create a table from a uniform two-level bullet list [0.3.8]\ncsv-table         Create a table from CSV data [0.3.4]\ncontents          Generate a table of contents\nsectnum           Automatically number sections, subsections, etc.\nheader, footer    Create document decorations [0.3.8]\ntarget-notes      Create an explicit footnote for each external target\nmath              Mathematical notation (input in LaTeX format)\nmeta              HTML-specific metadata\ninclude           Read an external reST file as if it were inline\nraw               Non-reST data passed untouched to the Writer\nreplace           Replacement text for substitution definitions\nunicode           Unicode character code conversion for substitution defs\ndate              Generates today's date; for substitution defs\nclass             Set a \"class\" attribute on the next element\nrole              Create a custom interpreted text role [0.3.2]\ndefault-role      Set the default interpreted text role [0.3.10]\ntitle             Set the metadata document title [0.3.10]\n================  ============================================================\n\nInterpreted Text Role Quick Reference\n=====================================\nSee <http://docutils.sf.net/docs/ref/rst/roles.html> for full info.\n\n================  ============================================================\nRole Name         Description\n================  ============================================================\nemphasis          Equivalent to *emphasis*\nliteral           Equivalent to ``literal`` but processes backslash escapes\nmath              Mathematical notation (input in LaTeX format)\nPEP               Reference to a numbered Python Enhancement Proposal\nRFC               Reference to a numbered Internet Request For Comments\nraw               For non-reST data; cannot be used directly (see docs) [0.3.6]\nstrong            Equivalent to **strong**\nsub               Subscript\nsup               Superscript\ntitle             Title reference (book, etc.); standard default role\n================  ============================================================",
            "title": "reStructuredText"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#restructuredtext",
            "text": "reStructuredText Quick Ref  reStructuredText Cheat Sheet   (see below)",
            "title": "reStructuredText"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#rest-short-overview",
            "text": "All reST files use an indentation of 3 spaces; no tabs are allowed. \nThe maximum line length is 80 characters for normal text, but tables, \ndeeply indented code samples and long links may extend beyond that. \nCode example bodies should use normal Python 4-space indentation.\nParagraphs are simply chunks of text separated by one or more blank lines. \nAs in Python, indentation is significant in reST, so all lines of the same\nparagraph must be left-aligned to the same level of indentation.  Section headers are created by underlining (and optionally overlining)\nthe section title with a punctuation character, at least as long as the text:  =================\nThis is a heading\n=================\n# with overline, for parts\n* with overline, for chapters\n= for sections\n- for subsections\n^ for subsubsections\n\" for paragraphs\n\none asterisk: *text* for emphasis (italics),\ntwo asterisks: **text** for strong emphasis (boldface), and\nbackquotes: ``text`` for code samples.\nescape with a backslash \\\n\n* This is a bulleted list.\n* It has two items, the second\n  item uses two lines.\n\n1. This is a numbered list.\n2. It has two items too.\n\n. This is a numbered list.\n. It has two items too.  Nested lists are possible, but be aware that they must be separated from the \nparent list items by blank lines",
            "title": "reST Short Overview"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#source-code-double-colon",
            "text": "This is a normal text paragraph. The next paragraph is a code sample::\n\n       It is not processed in any way, except\n       that the indentation is removed.\n\n       It can span multiple lines.\n\nThis is a normal text paragraph again.",
            "title": "Source Code Double Colon"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#links",
            "text": "`Link text <http://target>`_ for inline web links.",
            "title": "Links"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#definitions",
            "text": "term (up to a line of text)\n   Definition of the term, which must be indented and \n   can even consist of multiple paragraphs\n\nnext term\n   Description.",
            "title": "Definitions"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#footnotes",
            "text": "Lorem ipsum [#]_ dolor sit amet ... [#]_",
            "title": "Footnotes"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#use-of-restructuredtext-in-python-docstrings",
            "text": "See http://infinitemonkeycorps.net/docs/pph/      # Typical function documentation: \n\n    :param volume_id: The ID of the EBS volume to be attached.\n    :type volume_id: str\n\n    :param instance_id: The ID of the EC2 instance \n    :type instance_id: str\n\n    :return: `Reverse geocoder return value`_ dictionary giving closest\n        address(es) to `(lat, lng)`\n    :rtype: dict\n    :raises GoogleMapsError: If the coordinates could not be reverse geocoded.\n\n    Keyword arguments and return value are identical to those of :meth:`geocode()`.\n\n    .. _`Reverse geocoder return value`:\n        http://code.google.com/apis/maps/documentation/geocoding/index.html#ReverseGeocoding   Normal docstring formatting conventions apply: see PEP 257.  Identifier references go in `backticks`.  :param lat: some text    documents parameters  :type lat: float   documents parameter types  :return:  dictionary giving some info...  documents return values  :rtype: dict   documents return type  :raises SomeError:  sometext...   documents exceptions raised  >>>   starts a doctest and is automatically formatted as code  Code can also be indicated by indenting four spaces or preceding with  ::  and a blank line  Link to other methods, functions, classes, modules with :meth: mymethod ,   :func: myfunc , :class: myclass , and :mod: mymodule .  Hyperlink names go in backticks with a trailing underscore:  Google _  Targets can be defined anywhere with:  .. _Google: http://www.google.com/",
            "title": "Use of reStructuredText in Python docstrings"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#explicit-markup",
            "text": "An explicit markup block begins with a line starting with .. followed by\nwhitespace and is terminated by the next paragraph at the same level of \nindentation. (There needs to be a blank line between explicit markup\nand normal paragraphs.  .. sectionauthor:: Guido van Rossum <guido@python.org>\n\n.. rubric:: Footnotes\n\n.. [#] Text of the first footnote.\n.. [#] Text of the second footnote.\n\n\n:mod:`parrot` -- Dead parrot access\n===================================\n\n.. module:: parrot\n   :platform: Unix, Windows\n   :synopsis: Analyze and reanimate dead parrots.\n.. moduleauthor:: Eric Cleese <eric@python.invalid>\n.. moduleauthor:: John Idle <john@python.invalid>\n\n.. function:: repeat([repeat=3[, number=1000000]])\n              repeat(y, z)\n   :bar: no\n\n   Return a line of text input from the user.\n\n\n.. class:: Spam\n\n      Description of the class.\n\n      .. data:: ham\n\n         Description of the attribute.",
            "title": "Explicit Markup"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#inline-markup",
            "text": ":rolename:`content`  or  :role:`title <target>`\n\n:meth:`~Queue.Queue.get` will refer to Queue.Queue.get but only display get as the link text.  The following roles refer to objects in modules and are possibly hyperlinked \nif a matching identifier is found:  mod  The name of a module; a dotted name may be used. This should also be used for package names.  func  The name of a Python function; dotted names may be used. The role text should not include trailing parentheses to enhance readability. The parentheses are stripped when searching for identifiers.  data  The name of a module-level variable or constant.  const  The name of a \u201cdefined\u201d constant. This may be a C-language #define or a Python variable that is not intended to be changed.  class  A class name; a dotted name may be used.  meth  The name of a method of an object. The role text should include the type name and the method name. A dotted name may be used.  attr  The name of a data attribute of an object.  exc  The name of an exception. A dotted name may be used.",
            "title": "Inline markup"
        },
        {
            "location": "/Markup_and_Documentation/reStructuredText/#official-restructuredtext-cheatsheet",
            "text": "=====================================================\n The reStructuredText_ Cheat Sheet: Syntax Reminders\n=====================================================\n:Info: See <http://docutils.sf.net/rst.html> for introductory docs.\n:Author: David Goodger <goodger@python.org>\n:Date: $Date: 2013-02-20 01:10:53 +0000 (Wed, 20 Feb 2013) $\n:Revision: $Revision: 7612 $\n:Description: This is a \"docinfo block\", or bibliographic field list\n\n.. NOTE:: If you are reading this as HTML, please read\n   `<cheatsheet.txt>`_ instead to see the input syntax examples!\n\nSection Structure\n=================\nSection titles are underlined or overlined & underlined.\n\nBody Elements\n=============\nGrid table:\n\n+--------------------------------+-----------------------------------+\n| Paragraphs are flush-left,     | Literal block, preceded by \"::\":: |\n| separated by blank lines.      |                                   |\n|                                |     Indented                      |\n|     Block quotes are indented. |                                   |\n+--------------------------------+ or::                              |\n| >>> print 'Doctest block'      |                                   |\n| Doctest block                  | > Quoted                          |\n+--------------------------------+-----------------------------------+\n| | Line blocks preserve line breaks & indents. [new in 0.3.6]       |\n| |     Useful for addresses, verse, and adornment-free lists; long  |\n|       lines can be wrapped with continuation lines.                |\n+--------------------------------------------------------------------+\n\nSimple tables:\n\n================  ============================================================\nList Type         Examples (syntax in the `text source <cheatsheet.txt>`_)\n================  ============================================================\nBullet list       * items begin with \"-\", \"+\", or \"*\"\nEnumerated list   1. items use any variation of \"1.\", \"A)\", and \"(i)\"\n                  #. also auto-enumerated\nDefinition list   Term is flush-left : optional classifier\n                      Definition is indented, no blank line between\nField list        :field name: field body\nOption list       -o  at least 2 spaces between option & description\n================  ============================================================\n\n================  ============================================================\nExplicit Markup   Examples (visible in the `text source`_)\n================  ============================================================\nFootnote          .. [1] Manually numbered or [#] auto-numbered\n                     (even [#labelled]) or [*] auto-symbol\nCitation          .. [CIT2002] A citation.\nHyperlink Target  .. _reStructuredText: http://docutils.sf.net/rst.html\n                  .. _indirect target: reStructuredText_\n                  .. _internal target:\nAnonymous Target  __ http://docutils.sf.net/docs/ref/rst/restructuredtext.html\nDirective (\"::\")  .. image:: images/biohazard.png\nSubstitution Def  .. |substitution| replace:: like an inline directive\nComment           .. is anything else\nEmpty Comment     (\"..\" on a line by itself, with blank lines before & after,\n                  used to separate indentation contexts)\n================  ============================================================\n\nInline Markup\n=============\n*emphasis*; **strong emphasis**; `interpreted text`; `interpreted text\nwith role`:emphasis:; ``inline literal text``; standalone hyperlink,\nhttp://docutils.sourceforge.net; named reference, reStructuredText_;\n`anonymous reference`__; footnote reference, [1]_; citation reference,\n[CIT2002]_; |substitution|; _`inline internal target`.\n\f\nDirective Quick Reference\n=========================\nSee <http://docutils.sf.net/docs/ref/rst/directives.html> for full info.\n\n================  ============================================================\nDirective Name    Description (Docutils version added to, in [brackets])\n================  ============================================================\nattention         Specific admonition; also \"caution\", \"danger\",\n                  \"error\", \"hint\", \"important\", \"note\", \"tip\", \"warning\"\nadmonition        Generic titled admonition: ``.. admonition:: By The Way``\nimage             ``.. image:: picture.png``; many options possible\nfigure            Like \"image\", but with optional caption and legend\ntopic             ``.. topic:: Title``; like a mini section\nsidebar           ``.. sidebar:: Title``; like a mini parallel document\nparsed-literal    A literal block with parsed inline markup\nrubric            ``.. rubric:: Informal Heading``\nepigraph          Block quote with class=\"epigraph\"\nhighlights        Block quote with class=\"highlights\"\npull-quote        Block quote with class=\"pull-quote\"\ncompound          Compound paragraphs [0.3.6]\ncontainer         Generic block-level container element [0.3.10]\ntable             Create a titled table [0.3.1]\nlist-table        Create a table from a uniform two-level bullet list [0.3.8]\ncsv-table         Create a table from CSV data [0.3.4]\ncontents          Generate a table of contents\nsectnum           Automatically number sections, subsections, etc.\nheader, footer    Create document decorations [0.3.8]\ntarget-notes      Create an explicit footnote for each external target\nmath              Mathematical notation (input in LaTeX format)\nmeta              HTML-specific metadata\ninclude           Read an external reST file as if it were inline\nraw               Non-reST data passed untouched to the Writer\nreplace           Replacement text for substitution definitions\nunicode           Unicode character code conversion for substitution defs\ndate              Generates today's date; for substitution defs\nclass             Set a \"class\" attribute on the next element\nrole              Create a custom interpreted text role [0.3.2]\ndefault-role      Set the default interpreted text role [0.3.10]\ntitle             Set the metadata document title [0.3.10]\n================  ============================================================\n\nInterpreted Text Role Quick Reference\n=====================================\nSee <http://docutils.sf.net/docs/ref/rst/roles.html> for full info.\n\n================  ============================================================\nRole Name         Description\n================  ============================================================\nemphasis          Equivalent to *emphasis*\nliteral           Equivalent to ``literal`` but processes backslash escapes\nmath              Mathematical notation (input in LaTeX format)\nPEP               Reference to a numbered Python Enhancement Proposal\nRFC               Reference to a numbered Internet Request For Comments\nraw               For non-reST data; cannot be used directly (see docs) [0.3.6]\nstrong            Equivalent to **strong**\nsub               Subscript\nsup               Superscript\ntitle             Title reference (book, etc.); standard default role\n================  ============================================================",
            "title": "Official reStructuredText Cheatsheet"
        },
        {
            "location": "/Python/Jupyter/",
            "text": "IPython\n / \nJupyter\n\u00b6\n\n\n\n\nUsing IPython makes interactive work easy.\n\n\nBetter shell\n\n\nNotebook interface\n\n\nEmbeddable kernel\n\n\nParallel python\n\n\n\n\n\n\n\n\nIPython shell shortcuts\n\u00b6\n\n\n\n\nTAB expansion to complete python names and file paths\n\n\n~ and * directory / file expansion\n\n\nmany \"magic\" methods:\n\n\n\n\n%lsmagic                # list of all magic methods\n%quickref               # cheatsheet\n%magic\n\n\n\n\n\nHelp\n\u00b6\n\n\n?                       # overall help\nhelp                    # python help system\n?someobj or someobj?    # help\n??someobj or someobj??  # detailed help\n\n\n\n\n\n%pdoc\n \n%pdef\n \n%psource\n  for docstring, function definition, source code only.\n\n\nRun\n\u00b6\n\n\nTo run a program directly from the IPython console:\n\n\n%run somescript.py      # instead of execfile(\"somescript.py\") at the python prompt\n\n\n\n\n\n%run\n has special flags for timing the execution of your scripts (\n-t\n) or for running them under the control of either Python's pdb debugger (\n-d\n) or profiler (\n-p\n):\n\n\n%run -d myscript.py\n\n\n\n\n\nOther Commands\n\u00b6\n\n\n%edit %ed               # edit then execute\n%save\n%load example.py        # load local (example) file (or url) allowing modification\n%load http://matplotlib.org/plot_directive/mpl_examples/mplot3d/contour3d_demo.py\n%macro                  # define macro with range of history lines, filenames or string objects\n%recall\n\n%whos                   # list identifiers you have defined interactively\n%reset  -f -s           # remove objects -f for force -s for soft (leaves history).\n\n\n\n\n\n\n\n%reset\n is not a kernel restart\n\n\nRestart with \nCtrl+.\n in \"qtconsole\"\n\n\nimport module ; reload(module)\n to reload a module from disk\n\n\n\n\nDebugging\n\u00b6\n\n\n%debug                  # jump into the Python debugger (pdb)\n%pdb                    # start the debugger on any uncaught exception.\n\n%cd                     # change directory\n%pwd                    # print working directory\n%env                    # OS environment variables\n\n\n\n\n\nOS Commands\n\u00b6\n\n\n!OScommand\n!ping www.bbc.co.uk\n%alias                  # system command alias\n\n\n\n\n\nHistory\n\u00b6\n\n\n_ __ ___                # etc... for previous outputs.\n_i _ii _i4              # etc.. for previous input. _ih for list of previous inputs\n\n\n\n\n\nGUI integration\n\u00b6\n\n\nStart with \nipython --gui=qt\n or at the IPython prompt:\n\n\n%gui wx\n\n\n\n\n\nArguments can be \nwx\n, \nqt\n, \ngtk\n and \ntk\n.\n\n\nMatplotlib / pylab graphics in an iPython shell\n\u00b6\n\n\nStart with: \nipython --matplotlib\n ( or \n--matplotlib=qt\n etc...)\n\n\nAt the IPython prompt:\n\n\n%matplotlib             # set matplotlib to work interactively; does not import anythig\n%matplotlib  inline\n%matplotlib qt          # request a specific GUI backend\n%pylab inline\n\n\n\n\n\n%pylab\n makes the following imports:\n\n\nimport\n \nnumpy\n\n\nimport\n \nmatplotlib\n\n\nfrom\n \nmatplotlib\n \nimport\n \npylab\n,\n \nmlab\n,\n \npyplot\n\n\nnp\n \n=\n \nnumpy\n\n\nplt\n \n=\n \npyplot\n\n\nfrom\n \nIPython.display\n \nimport\n \ndisplay\n\n\nfrom\n \nIPython.core.pylabtools\n \nimport\n \nfigsize\n,\n \ngetfigs\n\n\nfrom\n \npylab\n \nimport\n \n*\n\n\nfrom\n \nnumpy\n \nimport\n \n*\n\n\n\n\n\n\nQtconsole - an improved console\n\u00b6\n\n\nAt the command prompt:\n\n\nipython.exe qtconsole --pylab=inline --ConsoleWidget.font_size=10\n\n\n\n\n\nalternative: --matplotlib inline\nor within IPython:\n\n\n%matplotlib  inline\n%pylab inline\n\n\n\n\n\nTo embed plots, SVG or HTML in qtconsole, call display:\n\n\nfrom IPython.core.display import display, display_html\nfrom IPython.core.display import display_png, display_svg\ndisplay(plt.gcf()) # embeds the current figure in the qtconsole\ndisplay(*getfigs()) # embeds all active figures in the qtconsole\n#or:\nf = plt.figure()\nplt.plot(np.rand(100))\ndisplay(f)\n\n\n\n\n\nipython and ipython notebook for matlab users\n\n\nIPython Notebook web-based interface\n\u00b6\n\n\n\n\nStart with: ipython notebook and switch to browser\n\n\nKeyboard shortcuts:\n\n\nEnter\n to edit a cell\n\n\nShift + Enter\n to evaluate\n\n\nCtrl + m\n or \nEsc\n for the \"command mode\"\n\n\n\n\n\n\n\n\nIn command mode:\n\n\n h list of keyboard shortcuts\n 1-6 to convert to heading cell\n m to convert to markdown cell\n y to convert to code\n c copy / v paste\n d d delete cell\n s save notebook\n . to restart kernel",
            "title": "Jupyter"
        },
        {
            "location": "/Python/Jupyter/#ipython-jupyter",
            "text": "Using IPython makes interactive work easy.  Better shell  Notebook interface  Embeddable kernel  Parallel python",
            "title": "IPython / Jupyter"
        },
        {
            "location": "/Python/Jupyter/#ipython-shell-shortcuts",
            "text": "TAB expansion to complete python names and file paths  ~ and * directory / file expansion  many \"magic\" methods:   %lsmagic                # list of all magic methods\n%quickref               # cheatsheet\n%magic",
            "title": "IPython shell shortcuts"
        },
        {
            "location": "/Python/Jupyter/#help",
            "text": "?                       # overall help\nhelp                    # python help system\n?someobj or someobj?    # help\n??someobj or someobj??  # detailed help  %pdoc   %pdef   %psource   for docstring, function definition, source code only.",
            "title": "Help"
        },
        {
            "location": "/Python/Jupyter/#run",
            "text": "To run a program directly from the IPython console:  %run somescript.py      # instead of execfile(\"somescript.py\") at the python prompt  %run  has special flags for timing the execution of your scripts ( -t ) or for running them under the control of either Python's pdb debugger ( -d ) or profiler ( -p ):  %run -d myscript.py",
            "title": "Run"
        },
        {
            "location": "/Python/Jupyter/#other-commands",
            "text": "%edit %ed               # edit then execute\n%save\n%load example.py        # load local (example) file (or url) allowing modification\n%load http://matplotlib.org/plot_directive/mpl_examples/mplot3d/contour3d_demo.py\n%macro                  # define macro with range of history lines, filenames or string objects\n%recall\n\n%whos                   # list identifiers you have defined interactively\n%reset  -f -s           # remove objects -f for force -s for soft (leaves history).   %reset  is not a kernel restart  Restart with  Ctrl+.  in \"qtconsole\"  import module ; reload(module)  to reload a module from disk",
            "title": "Other Commands"
        },
        {
            "location": "/Python/Jupyter/#debugging",
            "text": "%debug                  # jump into the Python debugger (pdb)\n%pdb                    # start the debugger on any uncaught exception.\n\n%cd                     # change directory\n%pwd                    # print working directory\n%env                    # OS environment variables",
            "title": "Debugging"
        },
        {
            "location": "/Python/Jupyter/#os-commands",
            "text": "!OScommand\n!ping www.bbc.co.uk\n%alias                  # system command alias",
            "title": "OS Commands"
        },
        {
            "location": "/Python/Jupyter/#history",
            "text": "_ __ ___                # etc... for previous outputs.\n_i _ii _i4              # etc.. for previous input. _ih for list of previous inputs",
            "title": "History"
        },
        {
            "location": "/Python/Jupyter/#gui-integration",
            "text": "Start with  ipython --gui=qt  or at the IPython prompt:  %gui wx  Arguments can be  wx ,  qt ,  gtk  and  tk .",
            "title": "GUI integration"
        },
        {
            "location": "/Python/Jupyter/#matplotlib-pylab-graphics-in-an-ipython-shell",
            "text": "Start with:  ipython --matplotlib  ( or  --matplotlib=qt  etc...)  At the IPython prompt:  %matplotlib             # set matplotlib to work interactively; does not import anythig\n%matplotlib  inline\n%matplotlib qt          # request a specific GUI backend\n%pylab inline  %pylab  makes the following imports:  import   numpy  import   matplotlib  from   matplotlib   import   pylab ,   mlab ,   pyplot  np   =   numpy  plt   =   pyplot  from   IPython.display   import   display  from   IPython.core.pylabtools   import   figsize ,   getfigs  from   pylab   import   *  from   numpy   import   *",
            "title": "Matplotlib / pylab graphics in an iPython shell"
        },
        {
            "location": "/Python/Jupyter/#qtconsole-an-improved-console",
            "text": "At the command prompt:  ipython.exe qtconsole --pylab=inline --ConsoleWidget.font_size=10  alternative: --matplotlib inline\nor within IPython:  %matplotlib  inline\n%pylab inline  To embed plots, SVG or HTML in qtconsole, call display:  from IPython.core.display import display, display_html\nfrom IPython.core.display import display_png, display_svg\ndisplay(plt.gcf()) # embeds the current figure in the qtconsole\ndisplay(*getfigs()) # embeds all active figures in the qtconsole\n#or:\nf = plt.figure()\nplt.plot(np.rand(100))\ndisplay(f)  ipython and ipython notebook for matlab users",
            "title": "Qtconsole - an improved console"
        },
        {
            "location": "/Python/Jupyter/#ipython-notebook-web-based-interface",
            "text": "Start with: ipython notebook and switch to browser  Keyboard shortcuts:  Enter  to edit a cell  Shift + Enter  to evaluate  Ctrl + m  or  Esc  for the \"command mode\"     In command mode:   h list of keyboard shortcuts\n 1-6 to convert to heading cell\n m to convert to markdown cell\n y to convert to code\n c copy / v paste\n d d delete cell\n s save notebook\n . to restart kernel",
            "title": "IPython Notebook web-based interface"
        },
        {
            "location": "/Python/Matplotlib/",
            "text": "Matplotlib Cheatsheet\n\u00b6\n\n\nMatplotlib prepares 2D (and some 3D) graphics. \n\n\n\n\nMain page: http://www.matplotlib.org\n\n\nImage gallery: http://matplotlib.org/gallery.html \n\n\npyplot command summary: http://matplotlib.org/api/pyplot_summary.html \n\n\nExamples http://matplotlib.org/examples/index.html\n\n\nTutorial: http://www.loria.fr/~rougier/teaching/matplotlib/\n\n\nSee also: https://www.wakari.io/nb/url///wakari.io/static/notebooks/Lecture_4_Matplotlib.ipynb\n\n\n\n\nMatplotlib, pylab, and pyplot: how are they related?\n\u00b6\n\n\n\n\nMatplotlib is the whole package. Pylab and matplotlib.pyplot (pyplot in the following) are modules in matplotlib.\n\n\nPyplot makes matplotlib work like MATLAB. \n\n\nPyplot provides the state-machine interface to the underlying plotting library (the matplotlib API in the matplotlib module). \n\n\nEach pyplot function makes some change to a figure: eg, create a figure, create a plotting area in a figure, plot some lines in a plotting area, decorate the plot with labels, etc.... \n\n\nPyplot is \nstateful\n, in that it keeps track of the current figure and plotting area, and the plotting functions are directed to the current axes.\n\n\nPylab combines the pyplot functionality (for plotting) with the numpy functionality (mathematics / arrays) in a single namespace, making that namespace (or environment) even more MATLAB-like. \n\n\nThe pyplot interface is generally preferred for non-interactive plotting (i.e., scripting). \n\n\nThe pylab interface is convenient for interactive calculations and plotting, as it minimizes typing.\n\n\n\n\nExamples\n\u00b6\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n\n# Compute the x and y coordinates for points on sine and cosine curves\n\n\nx\n \n=\n \nnp\n.\narange\n(\n0\n,\n \n3\n \n*\n \nnp\n.\npi\n,\n \n0.1\n)\n\n\ny_sin\n \n=\n \nnp\n.\nsin\n(\nx\n)\n\n\ny_cos\n \n=\n \nnp\n.\ncos\n(\nx\n)\n\n\n\n# Set up a subplot grid that has height 2 and width 1,\n\n\n# and set the first such subplot as active.\n\n\nplt\n.\nsubplot\n(\n2\n,\n \n1\n,\n \n1\n)\n\n\n\n# Make the first plot\n\n\nplt\n.\nplot\n(\nx\n,\n \ny_sin\n)\n\n\nplt\n.\ntitle\n(\n'Sine'\n)\n\n\n\n# Set the second subplot as active, and make the second plot.\n\n\nplt\n.\nsubplot\n(\n2\n,\n \n1\n,\n \n2\n)\n\n\nplt\n.\nplot\n(\nx\n,\n \ny_cos\n)\n\n\nplt\n.\ntitle\n(\n'Cosine'\n)\n\n\n\n# Show the figure.\n\n\nplt\n.\nshow\n()",
            "title": "Matplotlib"
        },
        {
            "location": "/Python/Matplotlib/#matplotlib-cheatsheet",
            "text": "Matplotlib prepares 2D (and some 3D) graphics.    Main page: http://www.matplotlib.org  Image gallery: http://matplotlib.org/gallery.html   pyplot command summary: http://matplotlib.org/api/pyplot_summary.html   Examples http://matplotlib.org/examples/index.html  Tutorial: http://www.loria.fr/~rougier/teaching/matplotlib/  See also: https://www.wakari.io/nb/url///wakari.io/static/notebooks/Lecture_4_Matplotlib.ipynb",
            "title": "Matplotlib Cheatsheet"
        },
        {
            "location": "/Python/Matplotlib/#matplotlib-pylab-and-pyplot-how-are-they-related",
            "text": "Matplotlib is the whole package. Pylab and matplotlib.pyplot (pyplot in the following) are modules in matplotlib.  Pyplot makes matplotlib work like MATLAB.   Pyplot provides the state-machine interface to the underlying plotting library (the matplotlib API in the matplotlib module).   Each pyplot function makes some change to a figure: eg, create a figure, create a plotting area in a figure, plot some lines in a plotting area, decorate the plot with labels, etc....   Pyplot is  stateful , in that it keeps track of the current figure and plotting area, and the plotting functions are directed to the current axes.  Pylab combines the pyplot functionality (for plotting) with the numpy functionality (mathematics / arrays) in a single namespace, making that namespace (or environment) even more MATLAB-like.   The pyplot interface is generally preferred for non-interactive plotting (i.e., scripting).   The pylab interface is convenient for interactive calculations and plotting, as it minimizes typing.",
            "title": "Matplotlib, pylab, and pyplot: how are they related?"
        },
        {
            "location": "/Python/Matplotlib/#examples",
            "text": "import   numpy   as   np  import   matplotlib.pyplot   as   plt  # Compute the x and y coordinates for points on sine and cosine curves  x   =   np . arange ( 0 ,   3   *   np . pi ,   0.1 )  y_sin   =   np . sin ( x )  y_cos   =   np . cos ( x )  # Set up a subplot grid that has height 2 and width 1,  # and set the first such subplot as active.  plt . subplot ( 2 ,   1 ,   1 )  # Make the first plot  plt . plot ( x ,   y_sin )  plt . title ( 'Sine' )  # Set the second subplot as active, and make the second plot.  plt . subplot ( 2 ,   1 ,   2 )  plt . plot ( x ,   y_cos )  plt . title ( 'Cosine' )  # Show the figure.  plt . show ()",
            "title": "Examples"
        },
        {
            "location": "/Python/Python3/",
            "text": "What's new in Python 3.x\n\u00b6\n\n\nWhat's really new in Python 3\n\n\nnonlocal / global\n\u00b6\n\n\nx\n \n=\n \n0\n\n\ndef\n \nouter\n():\n\n  \nx\n \n=\n \n1\n\n  \ndef\n \ninner\n():\n\n  \nnonlocal\n \nx\n\n  \nx\n \n=\n \n2\n\n  \nprint\n(\n\"inner:\"\n,\n \nx\n)\n\n\n  \ninner\n()\n\n  \nprint\n(\n\"outer:\"\n,\n \nx\n)\n\n\n\nouter\n()\n\n\nprint\n(\n\"global:\"\n,\n \nx\n)\n\n\n\n# inner: 2\n\n\n# outer: 2\n\n\n# global: 0\n\n\n\n## with global\n\n\nx\n \n=\n \n0\n\n\ndef\n \nouter\n():\n\n     \nx\n \n=\n \n1\n\n     \ndef\n \ninner\n():\n\n           \nglobal\n \nx\n\n           \nx\n \n=\n \n2\n\n           \nprint\n(\n\"inner:\"\n,\n \nx\n)\n\n  \ninner\n()\n\n  \nprint\n(\n\"outer:\"\n,\n \nx\n)\n\n\nouter\n()\n\n\nprint\n(\n\"global:\"\n,\n \nx\n)\n\n\n\n# inner: 2\n\n\n# outer: 1\n\n\n# global: 2\n\n\n\n\n\n\nString interpolation - new in 3.6\n\u00b6\n\n\nname\n=\n\"David\"\n\n\nf\n\"My name is {name}\"\n\n\nvalue\n \n=\n \ndecimal\n.\nDecimal\n(\n\"10.4507\"\n)\n\n\nprint\n(\nf\n\"result: {value:10.5}\"\n \n)\n  \n# width precision\n\n\n\n\n\n\nPEP 492 - Coroutines with async and await syntax\n\u00b6\n\n\nasync and await\n\n\nyield from iterator\n\n\nis equivalent\n\n\nfor\n \nx\n \nin\n \niterator\n:\n\n     \nyield\n \nx\n\n\n\n\n\n\nExample:\n\n\ndef\n \nlazy_range\n(\nup_to\n):\n\n     \n\"\"\"Generator to return the sequence of integers from 0 to up_to, exclusive.\"\"\"\n\n     \nindex\n \n=\n \n0\n\n     \ndef\n \ngratuitous_refactor\n():\n\n           \nnonlocal\n \nindex\n\n           \nwhile\n \nindex\n \n<\n \nup_to\n:\n\n               \nyield\n \nindex\n\n               \nindex\n \n+=\n \n1\n\n     \nyield from\n \ngratuitous_refactor\n()\n\n\n\n\n\n\nNew 3.6 syntax:\n\n\nasync\n \ndef\n \nfunc\n(\nparam1\n,\n \nparam2\n):\n\n    \ndo_stuff\n()\n\n    \nawait\n \nsome_coroutine\n()\n\n\n\nasync\n \ndef\n \nread_data\n(\ndb\n):\n\n  \ndata\n \n=\n \nawait\n \ndb\n.\nfetch\n(\n'SELECT ...'\n)\n\n\n\nasync\n \ndef\n \ndisplay_date\n(\nloop\n):\n\n  \nend_time\n \n=\n \nloop\n.\ntime\n()\n \n+\n \n5.0\n\n  \nwhile\n \nTrue\n:\n\n  \nprint\n(\ndatetime\n.\ndatetime\n.\nnow\n())\n\n  \nif\n \n(\nloop\n.\ntime\n()\n \n+\n \n1.0\n)\n \n>=\n \nend_time\n:\n\n  \nbreak\n\n  \nawait\n \nasyncio\n.\nsleep\n(\n1\n)\n\n\n\n\nloop\n \n=\n \nasyncio\n.\nget_event_loop\n()\n# Blocking call which returns when the display_date() coroutine is done\n\n\nloop\n.\nrun_until_complete\n(\ndisplay_date\n(\nloop\n))\n\n\nloop\n.\nclose\n()\n\n\n\n\n\n\nAsync for\n\u00b6\n\n\nasync\n \nfor\n \nTARGET\n \nin\n \nITER\n:\n\n  \nBLOCK\n\n\nelse\n:\n\n  \nBLOCK2\n\n\n\n\n\n\nAsync improvements - 3.6\n\u00b6\n\n\n\n\nset comprehension: \n{i async for i in agen()}\n\n\nlist comprehension: \n[i async for i in agen()]\n\n\ndict comprehension: \n{i: i ** 2 async for i in agen()}\n\n\ngenerator expression: \n(i ** 2 async for i in agen())\n\n\n\n\nType hinting\n\u00b6\n\n\nPEP 484\n\n\nmypy-lang.org\n\n\ndef\n \ngreet\n(\nname\n:\n \nstr\n)\n \n->\n \nstr\n\n    \nreturn\n \n'Hello there, {}'\n.\nformat\n(\nname\n)\n\n\n\n\n\n\nType aliases\n\u00b6\n\n\nUrl\n \n=\n \nstr\n\n\ndef\n \nretry\n(\nurl\n:\n \nUrl\n,\n \nretry_count\n:\n \nint\n)\n \n->\n \nNone\n:\n \n...\n\n\n\n\n\n\nfrom\n \ntyping\n \nimport\n \nTypeVar\n,\n \nIterable\n,\n \nTuple\n   \n\n\n\n\n\nOther common typings include: Any; Generic, Dict, List, Optional, Mapping, Set, Sequence - expressed as Sequence[int] \n\n\nT\n \n=\n \nTypeVar\n(\n'T'\n,\n \nint\n,\n \nfloat\n,\n \ncomplex\n)\n  \n# T is either or an int, a float or a complex \n\n\nVector\n \n=\n \nIterable\n[\nTuple\n[\nT\n,\n \nT\n]]\n          \n# \n\n\n\ndef\n \ninproduct\n(\nv\n:\n \nVector\n[\nT\n])\n \n->\n \nT\n:\n\n       \nreturn\n \nsum\n(\nx\n*\ny\n \nfor\n \nx\n,\n \ny\n \nin\n \nv\n)\n\n\n\ndef\n \ndilate\n(\nv\n:\n \nVector\n[\nT\n],\n \nscale\n:\n \nT\n)\n \n->\n \nVector\n[\nT\n]:\n\n       \nreturn\n \n((\nx\n \n*\n \nscale\n,\n \ny\n \n*\n \nscale\n)\n \nfor\n \nx\n,\n \ny\n \nin\n \nv\n)\n\n\nvec\n \n=\n \n[]\n \n# type: Vector[float]\n\n\n\n\n\n\nFor functions\n\u00b6\n\n\nCallable\n[[\nArg1Type\n,\n \nArg2Type\n],\n \nReturnType\n]\n\n\n\n\n\n\nType comments\n\u00b6\n\n\nx\n \n=\n \n[]\n   \n# type: List[Employee]\n\n\nx\n,\n \ny\n,\n \nz\n \n=\n \n[],\n \n[],\n \n[]\n  \n# type: List[int], List[int], List[str]",
            "title": "Python3"
        },
        {
            "location": "/Python/Python3/#whats-new-in-python-3x",
            "text": "What's really new in Python 3",
            "title": "What's new in Python 3.x"
        },
        {
            "location": "/Python/Python3/#nonlocal-global",
            "text": "x   =   0  def   outer (): \n   x   =   1 \n   def   inner (): \n   nonlocal   x \n   x   =   2 \n   print ( \"inner:\" ,   x ) \n\n   inner () \n   print ( \"outer:\" ,   x )  outer ()  print ( \"global:\" ,   x )  # inner: 2  # outer: 2  # global: 0  ## with global  x   =   0  def   outer (): \n      x   =   1 \n      def   inner (): \n            global   x \n            x   =   2 \n            print ( \"inner:\" ,   x ) \n   inner () \n   print ( \"outer:\" ,   x )  outer ()  print ( \"global:\" ,   x )  # inner: 2  # outer: 1  # global: 2",
            "title": "nonlocal / global"
        },
        {
            "location": "/Python/Python3/#string-interpolation-new-in-36",
            "text": "name = \"David\"  f \"My name is {name}\"  value   =   decimal . Decimal ( \"10.4507\" )  print ( f \"result: {value:10.5}\"   )    # width precision",
            "title": "String interpolation - new in 3.6"
        },
        {
            "location": "/Python/Python3/#pep-492-coroutines-with-async-and-await-syntax",
            "text": "async and await  yield from iterator  is equivalent  for   x   in   iterator : \n      yield   x   Example:  def   lazy_range ( up_to ): \n      \"\"\"Generator to return the sequence of integers from 0 to up_to, exclusive.\"\"\" \n      index   =   0 \n      def   gratuitous_refactor (): \n            nonlocal   index \n            while   index   <   up_to : \n                yield   index \n                index   +=   1 \n      yield from   gratuitous_refactor ()   New 3.6 syntax:  async   def   func ( param1 ,   param2 ): \n     do_stuff () \n     await   some_coroutine ()  async   def   read_data ( db ): \n   data   =   await   db . fetch ( 'SELECT ...' )  async   def   display_date ( loop ): \n   end_time   =   loop . time ()   +   5.0 \n   while   True : \n   print ( datetime . datetime . now ()) \n   if   ( loop . time ()   +   1.0 )   >=   end_time : \n   break \n   await   asyncio . sleep ( 1 )  loop   =   asyncio . get_event_loop () # Blocking call which returns when the display_date() coroutine is done  loop . run_until_complete ( display_date ( loop ))  loop . close ()",
            "title": "PEP 492 - Coroutines with async and await syntax"
        },
        {
            "location": "/Python/Python3/#async-for",
            "text": "async   for   TARGET   in   ITER : \n   BLOCK  else : \n   BLOCK2",
            "title": "Async for"
        },
        {
            "location": "/Python/Python3/#async-improvements-36",
            "text": "set comprehension:  {i async for i in agen()}  list comprehension:  [i async for i in agen()]  dict comprehension:  {i: i ** 2 async for i in agen()}  generator expression:  (i ** 2 async for i in agen())",
            "title": "Async improvements - 3.6"
        },
        {
            "location": "/Python/Python3/#type-hinting",
            "text": "PEP 484  mypy-lang.org  def   greet ( name :   str )   ->   str \n     return   'Hello there, {}' . format ( name )",
            "title": "Type hinting"
        },
        {
            "location": "/Python/Python3/#type-aliases",
            "text": "Url   =   str  def   retry ( url :   Url ,   retry_count :   int )   ->   None :   ...   from   typing   import   TypeVar ,   Iterable ,   Tuple      Other common typings include: Any; Generic, Dict, List, Optional, Mapping, Set, Sequence - expressed as Sequence[int]   T   =   TypeVar ( 'T' ,   int ,   float ,   complex )    # T is either or an int, a float or a complex   Vector   =   Iterable [ Tuple [ T ,   T ]]            #   def   inproduct ( v :   Vector [ T ])   ->   T : \n        return   sum ( x * y   for   x ,   y   in   v )  def   dilate ( v :   Vector [ T ],   scale :   T )   ->   Vector [ T ]: \n        return   (( x   *   scale ,   y   *   scale )   for   x ,   y   in   v )  vec   =   []   # type: Vector[float]",
            "title": "Type aliases"
        },
        {
            "location": "/Python/Python3/#for-functions",
            "text": "Callable [[ Arg1Type ,   Arg2Type ],   ReturnType ]",
            "title": "For functions"
        },
        {
            "location": "/Python/Python3/#type-comments",
            "text": "x   =   []     # type: List[Employee]  x ,   y ,   z   =   [],   [],   []    # type: List[int], List[int], List[str]",
            "title": "Type comments"
        },
        {
            "location": "/Scala/Scala/",
            "text": "Links\n\u00b6\n\n\n\n\nScala Cheatsheet\n\n\nScala @ TutorialPoint\n\n\nScala Tutorial (PDF)\n\n\n\n\nMain Features of Scala\n\u00b6\n\n\n\n\nAll types are objects\n\n\nType inference\n\n\nNested Functions\n\n\nFunctions are objects\n\n\nDomain specific language (DSL) support\n\n\nTraits\n\n\nClosures\n\n\nConcurrency support inspired by Erlang\n\n\n\n\nSyntax\n\u00b6\n\n\nClass Names - For all class names, the first letter should be in Upper Case. If several words are used to form a name of the class, each inner word's first letter should be in Upper Case.\n\n\nclass MyFirstScalaClass\n\n\nMethod Names - All method names should start with a Lower Case letter. If multiple words are used to form the name of the method, then each inner word's first letter should be in Upper Case.\n\n\ndef myMethodName()\n\n\nProgram File Name - Name of the program file should exactly match the object name. When saving the file you should save it using the object name (Remember Scala is case-sensitive) and append \".scala\" to the end of the name. If the file name and the object name do not match your program will not compile.\n\n\nAssume 'HelloWorld' is the object name: the file should be saved as 'HelloWorld.scala'.\n\n\nKeywords\n\u00b6\n\n\nabstract\ncase\ncatch\nclass\ndef\ndo\nelse\nextends\nfalse\nfinal\nfinally\nfor\nforSome\nif\nimplicit\nimport\nlazy\nmatch\nnew\nNull\nobject\noverride\npackage\nprivate\nprotected\nreturn\nsealed\nsuper\nthis\nthrow\ntrait\nTry\ntrue\ntype\nval\nvar\nwhile\nwith\nyield\n -\n :\n =\n =>\n<-\n<:\n<%\n>:\n#\n@\n\n\n\n\n\nCompilation\n\u00b6\n\n\nscalac HelloWorld.scala  // produces HelloWorld.class\nscala -classpath . HelloWorld\n\n\n\n\n\nVariables and Values\n\u00b6\n\n\nvar\n \nx\n \n=\n \n5\n \n// variable\n\n\nval\n \nx\n \n=\n \n5\n \n// \"const\"\n\n\nvar\n \nx\n:\n \nDouble\n \n=\n \n5\n \n// explicit type\n\n\n\n\n\n\nEntry Point\n\u00b6\n\n\nobject\n \nHelloWorld\n \n{\n\n    \ndef\n \nmain\n(\nargs\n:\n \nArray\n[\nString\n])\n \n{\n\n        \nprintln\n(\n\"Hello, world!\"\n)\n\n    \n}\n\n\n}\n\n\n\n\n\n\nStatic members (methods or fields) do not exist in Scala. Rather than defining static members, the Scala programmer declares these members in singleton objects, that is a class with a single instance.\n\n\nImports\n\u00b6\n\n\nAll classes from the java.lang package are imported by default, while others need to be imported explicitly.\n\n\nimport scala.collection.mutable.HashMap\n\nimport java.util.{Date, Locale}  // Multiple classes can be imported from the same package by enclosing them in curly braces\nimport java.text.DateFormat\nimport java.text.DateFormat._    // When importing all the names of a package or class, one uses the underscore character (_) instead of the asterisk (*). \n\nobject FrenchDate {\n    def main(args: Array[String]) {\n        val now = new Date\n        val df = getDateInstance(LONG, Locale.FRANCE)\n    println(df format now)       // Methods taking one argument can be used with an infix syntax. Equivalent to df.format(now)   \n    }\n}\n\n\n\n\n\nScala is Object-Oriented\n\u00b6\n\n\nScala is a pure object-oriented language in the sense that everything is an object, including numbers or functions. \n\n\n1 + 2 * 3 / x\n consists exclusively of method calls, because it is equivalent to the following expression: \n(1).+(((2).*(3))./(x))\n\nThis also means that +, *, etc. are valid identifiers in Scala.\n\n\nTypes and behavior of objects are described by classes and traits.\n\n\nClasses are extended by subclassing and a flexible mixin-based composition mechanism as a clean replacement for multiple inheritance.\n\n\nclass\n \nComplex\n(\nreal\n:\n \nDouble\n,\n \nimaginary\n:\n \nDouble\n)\n \n{\n\n    \ndef\n \nre\n \n=\n \nreal\n       \n// return type inferred automatically by the compiler  \n\n    \ndef\n \nim\n \n=\n \nimaginary\n  \n// methods withot arguments\n\n    \noverride\n \ndef\n \ntoString\n()\n \n=\n \n\"\"\n \n+\n \nre\n \n+\n \n(\nif\n \n(\nim\n \n<\n \n0\n)\n \n\"\"\n \nelse\n \n\"+\"\n)\n \n+\n \nim\n \n+\n \n\"i\"\n  \n// override methods inherited from a super-class\n\n\n}\n\n\n\n\n\n\nScala is Functional\n\u00b6\n\n\nScala is also a functional language in the sense that every function is a value and every value is an object so ultimately every function is an object.\nScala provides a lightweight syntax for defining anonymous functions, it supports higher-order functions, it allows functions to be nested, and supports currying. \n\n\nobject\n \nTimer\n \n{\n\n    \ndef\n \noncePerSecond\n(\ncallback\n:\n \n()\n \n=>\n \nUnit\n)\n \n{\n\n        \nwhile\n \n(\ntrue\n)\n \n{\n \ncallback\n();\n \nThread\n \nsleep\n \n1000\n \n}\n\n    \n}\n\n\n    \ndef\n \ntimeFlies\n()\n \n{\n\n        \nprintln\n(\n\"time flies like an arrow...\"\n)\n\n    \n}\n\n\n    \ndef\n \nmain\n(\nargs\n:\n \nArray\n[\nString\n])\n \n{\n\n        \noncePerSecond\n(\ntimeFlies\n)\n\n    \n}\n\n\n}\n\n\n\n\n\n\nAnonymous Functions\n\u00b6\n\n\nobject TimerAnonymous {\n    def oncePerSecond(callback: () => Unit) {\n        while (true) { callback(); Thread sleep 1000 }\n    }\n    def main(args: Array[String]) {\n        oncePerSecond(() => println(\"time flies like an arrow...\"))\n    }\n}\n\n\n\n\n\nCase Classes\n\u00b6\n\n\nabstract\n \nclass\n \nTree\n\n\ncase\n \nclass\n \nSum\n(\nl\n:\n \nTree\n,\n \nr\n:\n \nTree\n)\n \nextends\n \nTree\n\n\ncase\n \nclass\n \nVar\n(\nn\n:\n \nString\n)\n \nextends\n \nTree\n\n\ncase\n \nclass\n \nConst\n(\nv\n:\n \nInt\n)\n \nextends\n \nTree\n\n\n\n\n\n\n\n\nThe \nnew\n keyword is not mandatory to create instances of these classes (i.e. one can write Const(5) instead of new Const(5)),\n\n\nGetter functions are automatically defined for the constructor parameters (i.e. it is possible to get the value of the v constructor parameter of some instance c of class Const just by writing c.v),\n\n\nDefault definitions for methods equals and hashCode are provided, which work on the structure of the instances and not on their identity,\n\n\nA default definition for method toString is provided, and prints the value in a source form (e.g. the tree for expression x+1 prints as Sum(Var(x),Const(1))),\n\n\nInstances of these classes can be decomposed through pattern matching\n\n\n\n\nPattern Matching\n\u00b6\n\n\n{ case \"x\" => 5 }\n defines a function which, when given the string \"x\" as argument, returns the integer 5, and fails with an exception otherwise.\n\n\ntype\n \nEnvironment\n \n=\n \nString\n \n=>\n \nInt\n  \n// the type Environment can be used as an alias of the type of functions from String to Int\n\n\n\n\ndef\n \neval\n(\nt\n:\n \nTree\n,\n \nenv\n:\n \nEnvironment\n)\n:\n \nInt\n \n=\n \nt\n \nmatch\n \n{\n\n    \ncase\n \nSum\n(\nl\n,\n \nr\n)\n \n=>\n \neval\n(\nl\n,\n \nenv\n)\n \n+\n \neval\n(\nr\n,\n \nenv\n)\n\n    \ncase\n \nVar\n(\nn\n)\n \n=>\n \nenv\n(\nn\n)\n\n    \ncase\n \nConst\n(\nv\n)\n \n=>\n \nv\n\n\n}\n\n\n\ndef\n \nderive\n(\nt\n:\n \nTree\n,\n \nv\n:\n \nString\n)\n:\n \nTree\n \n=\n \nt\n \nmatch\n \n{\n\n    \ncase\n \nSum\n(\nl\n,\n \nr\n)\n \n=>\n \nSum\n(\nderive\n(\nl\n,\n \nv\n),\n \nderive\n(\nr\n,\n \nv\n))\n\n    \ncase\n \nVar\n(\nn\n)\n \nif\n \n(\nv\n \n==\n \nn\n)\n \n=>\n \nConst\n(\n1\n)\n                  \n// guard, an expression following the if keyword.\n\n    \ncase\n \n_\n \n=>\n \nConst\n(\n0\n)\n                                   \n// wild-card, written _, which is a pattern matching any value, without giving it a name. \n\n\n}\n\n\n\n\n\n\nTraits\n\u00b6\n\n\nApart from inheriting code from a super-class, a Scala class can also import code from one or several traits i.e. interfaces which can also contain code. \nIn Scala, when a class inherits from a trait, it implements that traits's interface, and inherits all the code contained in the trait.\n\n\ntrait\n \nOrd\n \n{\n\n\ndef\n \n<\n \n(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n                                   \n// The type Any which is used above is the type which is a super-type of all other types in Scala\n\n\ndef\n \n<=(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n \n=\n \n(\nthis\n \n<\n \nthat\n)\n \n||\n \n(\nthis\n \n==\n \nthat\n)\n\n\ndef\n \n>\n \n(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n \n=\n \n!(\nthis\n \n<=\n \nthat\n)\n\n\ndef\n \n>=(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n \n=\n \n!(\nthis\n \n<\n \nthat\n)\n\n\n}\n\n\n\nclass\n \nDate\n(\ny\n:\n \nInt\n,\n \nm\n:\n \nInt\n,\n \nd\n:\n \nInt\n)\n \nextends\n \nOrd\n \n{\n\n    \ndef\n \nyear\n \n=\n \ny\n\n    \ndef\n \nmonth\n \n=\n \nm\n\n    \ndef\n \nday\n \n=\n \nd\n\n    \noverride\n \ndef\n \ntoString\n()\n:\n \nString\n \n=\n \nyear\n \n+\n \n\"-\"\n \n+\n \nmonth\n \n+\n \n\"-\"\n \n+\n \nday\n\n\n    \noverride\n \ndef\n \nequals\n(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n \n=\n\n        \nthat\n.\nisInstanceOf\n[\nDate\n]\n \n&&\n \n{\n\n        \nval\n \no\n \n=\n \nthat\n.\nasInstanceOf\n[\nDate\n]\n\n        \no\n.\nday\n \n==\n \nday\n \n&&\n \no\n.\nmonth\n \n==\n \nmonth\n \n&&\n \no\n.\nyear\n \n==\n \nyear\n\n    \n}\n\n\n    \ndef\n \n<(\nthat\n:\n \nAny\n)\n:\n \nBoolean\n \n=\n \n{\n\n        \nif\n \n(!\nthat\n.\nisInstanceOf\n[\nDate\n])\n\n            \nerror\n(\n\"cannot compare \"\n \n+\n \nthat\n \n+\n \n\" and a Date\"\n)\n\n        \nval\n \no\n \n=\n \nthat\n.\nasInstanceOf\n[\nDate\n](\nyear\n \n<\n \no\n.\nyear\n)\n \n||\n\n            \n(\nyear\n \n==\n \no\n.\nyear\n \n&&\n \n(\nmonth\n \n<\n \no\n.\nmonth\n \n||\n\n            \n(\nmonth\n \n==\n \no\n.\nmonth\n \n&&\n \nday\n \n<\n \no\n.\nday\n    \n)))\n\n    \n}\n\n\n\n}\n\n\n\n\n\n\nGenerics\n\u00b6\n\n\nclass\n \nReference\n[\nT\n]\n \n{\n\n    \nprivate\n \nvar\n \ncontents\n:\n \nT\n \n=\n \n_\n             \n// _ represents a default value. This default value is 0 for numeric types, false for the Boolean type, () for the Unit type and null for all object types.\n\n    \ndef\n \nset\n(\nvalue\n:\n \nT\n)\n \n{\n \ncontents\n \n=\n \nvalue\n \n}\n\n    \ndef\n \nget\n:\n \nT\n \n=\n \ncontents\n\n\n}\n\n\n\n\n\n\nFrameworks\n\u00b6\n\n\n\n\nThe Lift Framework\n\n\nThe Play framework\n\n\nThe Bowler framework\n\n\nAkka",
            "title": "Scala"
        },
        {
            "location": "/Scala/Scala/#links",
            "text": "Scala Cheatsheet  Scala @ TutorialPoint  Scala Tutorial (PDF)",
            "title": "Links"
        },
        {
            "location": "/Scala/Scala/#main-features-of-scala",
            "text": "All types are objects  Type inference  Nested Functions  Functions are objects  Domain specific language (DSL) support  Traits  Closures  Concurrency support inspired by Erlang",
            "title": "Main Features of Scala"
        },
        {
            "location": "/Scala/Scala/#syntax",
            "text": "Class Names - For all class names, the first letter should be in Upper Case. If several words are used to form a name of the class, each inner word's first letter should be in Upper Case.  class MyFirstScalaClass  Method Names - All method names should start with a Lower Case letter. If multiple words are used to form the name of the method, then each inner word's first letter should be in Upper Case.  def myMethodName()  Program File Name - Name of the program file should exactly match the object name. When saving the file you should save it using the object name (Remember Scala is case-sensitive) and append \".scala\" to the end of the name. If the file name and the object name do not match your program will not compile.  Assume 'HelloWorld' is the object name: the file should be saved as 'HelloWorld.scala'.",
            "title": "Syntax"
        },
        {
            "location": "/Scala/Scala/#keywords",
            "text": "abstract\ncase\ncatch\nclass\ndef\ndo\nelse\nextends\nfalse\nfinal\nfinally\nfor\nforSome\nif\nimplicit\nimport\nlazy\nmatch\nnew\nNull\nobject\noverride\npackage\nprivate\nprotected\nreturn\nsealed\nsuper\nthis\nthrow\ntrait\nTry\ntrue\ntype\nval\nvar\nwhile\nwith\nyield\n -\n :\n =\n =>\n<-\n<:\n<%\n>:\n#\n@",
            "title": "Keywords"
        },
        {
            "location": "/Scala/Scala/#compilation",
            "text": "scalac HelloWorld.scala  // produces HelloWorld.class\nscala -classpath . HelloWorld",
            "title": "Compilation"
        },
        {
            "location": "/Scala/Scala/#variables-and-values",
            "text": "var   x   =   5   // variable  val   x   =   5   // \"const\"  var   x :   Double   =   5   // explicit type",
            "title": "Variables and Values"
        },
        {
            "location": "/Scala/Scala/#entry-point",
            "text": "object   HelloWorld   { \n     def   main ( args :   Array [ String ])   { \n         println ( \"Hello, world!\" ) \n     }  }   Static members (methods or fields) do not exist in Scala. Rather than defining static members, the Scala programmer declares these members in singleton objects, that is a class with a single instance.",
            "title": "Entry Point"
        },
        {
            "location": "/Scala/Scala/#imports",
            "text": "All classes from the java.lang package are imported by default, while others need to be imported explicitly.  import scala.collection.mutable.HashMap\n\nimport java.util.{Date, Locale}  // Multiple classes can be imported from the same package by enclosing them in curly braces\nimport java.text.DateFormat\nimport java.text.DateFormat._    // When importing all the names of a package or class, one uses the underscore character (_) instead of the asterisk (*). \n\nobject FrenchDate {\n    def main(args: Array[String]) {\n        val now = new Date\n        val df = getDateInstance(LONG, Locale.FRANCE)\n    println(df format now)       // Methods taking one argument can be used with an infix syntax. Equivalent to df.format(now)   \n    }\n}",
            "title": "Imports"
        },
        {
            "location": "/Scala/Scala/#scala-is-object-oriented",
            "text": "Scala is a pure object-oriented language in the sense that everything is an object, including numbers or functions.   1 + 2 * 3 / x  consists exclusively of method calls, because it is equivalent to the following expression:  (1).+(((2).*(3))./(x)) \nThis also means that +, *, etc. are valid identifiers in Scala.  Types and behavior of objects are described by classes and traits.  Classes are extended by subclassing and a flexible mixin-based composition mechanism as a clean replacement for multiple inheritance.  class   Complex ( real :   Double ,   imaginary :   Double )   { \n     def   re   =   real         // return type inferred automatically by the compiler   \n     def   im   =   imaginary    // methods withot arguments \n     override   def   toString ()   =   \"\"   +   re   +   ( if   ( im   <   0 )   \"\"   else   \"+\" )   +   im   +   \"i\"    // override methods inherited from a super-class  }",
            "title": "Scala is Object-Oriented"
        },
        {
            "location": "/Scala/Scala/#scala-is-functional",
            "text": "Scala is also a functional language in the sense that every function is a value and every value is an object so ultimately every function is an object.\nScala provides a lightweight syntax for defining anonymous functions, it supports higher-order functions, it allows functions to be nested, and supports currying.   object   Timer   { \n     def   oncePerSecond ( callback :   ()   =>   Unit )   { \n         while   ( true )   {   callback ();   Thread   sleep   1000   } \n     } \n\n     def   timeFlies ()   { \n         println ( \"time flies like an arrow...\" ) \n     } \n\n     def   main ( args :   Array [ String ])   { \n         oncePerSecond ( timeFlies ) \n     }  }",
            "title": "Scala is Functional"
        },
        {
            "location": "/Scala/Scala/#anonymous-functions",
            "text": "object TimerAnonymous {\n    def oncePerSecond(callback: () => Unit) {\n        while (true) { callback(); Thread sleep 1000 }\n    }\n    def main(args: Array[String]) {\n        oncePerSecond(() => println(\"time flies like an arrow...\"))\n    }\n}",
            "title": "Anonymous Functions"
        },
        {
            "location": "/Scala/Scala/#case-classes",
            "text": "abstract   class   Tree  case   class   Sum ( l :   Tree ,   r :   Tree )   extends   Tree  case   class   Var ( n :   String )   extends   Tree  case   class   Const ( v :   Int )   extends   Tree    The  new  keyword is not mandatory to create instances of these classes (i.e. one can write Const(5) instead of new Const(5)),  Getter functions are automatically defined for the constructor parameters (i.e. it is possible to get the value of the v constructor parameter of some instance c of class Const just by writing c.v),  Default definitions for methods equals and hashCode are provided, which work on the structure of the instances and not on their identity,  A default definition for method toString is provided, and prints the value in a source form (e.g. the tree for expression x+1 prints as Sum(Var(x),Const(1))),  Instances of these classes can be decomposed through pattern matching",
            "title": "Case Classes"
        },
        {
            "location": "/Scala/Scala/#pattern-matching",
            "text": "{ case \"x\" => 5 }  defines a function which, when given the string \"x\" as argument, returns the integer 5, and fails with an exception otherwise.  type   Environment   =   String   =>   Int    // the type Environment can be used as an alias of the type of functions from String to Int  def   eval ( t :   Tree ,   env :   Environment ) :   Int   =   t   match   { \n     case   Sum ( l ,   r )   =>   eval ( l ,   env )   +   eval ( r ,   env ) \n     case   Var ( n )   =>   env ( n ) \n     case   Const ( v )   =>   v  }  def   derive ( t :   Tree ,   v :   String ) :   Tree   =   t   match   { \n     case   Sum ( l ,   r )   =>   Sum ( derive ( l ,   v ),   derive ( r ,   v )) \n     case   Var ( n )   if   ( v   ==   n )   =>   Const ( 1 )                    // guard, an expression following the if keyword. \n     case   _   =>   Const ( 0 )                                     // wild-card, written _, which is a pattern matching any value, without giving it a name.   }",
            "title": "Pattern Matching"
        },
        {
            "location": "/Scala/Scala/#traits",
            "text": "Apart from inheriting code from a super-class, a Scala class can also import code from one or several traits i.e. interfaces which can also contain code. \nIn Scala, when a class inherits from a trait, it implements that traits's interface, and inherits all the code contained in the trait.  trait   Ord   {  def   <   ( that :   Any ) :   Boolean                                     // The type Any which is used above is the type which is a super-type of all other types in Scala  def   <=( that :   Any ) :   Boolean   =   ( this   <   that )   ||   ( this   ==   that )  def   >   ( that :   Any ) :   Boolean   =   !( this   <=   that )  def   >=( that :   Any ) :   Boolean   =   !( this   <   that )  }  class   Date ( y :   Int ,   m :   Int ,   d :   Int )   extends   Ord   { \n     def   year   =   y \n     def   month   =   m \n     def   day   =   d \n     override   def   toString () :   String   =   year   +   \"-\"   +   month   +   \"-\"   +   day \n\n     override   def   equals ( that :   Any ) :   Boolean   = \n         that . isInstanceOf [ Date ]   &&   { \n         val   o   =   that . asInstanceOf [ Date ] \n         o . day   ==   day   &&   o . month   ==   month   &&   o . year   ==   year \n     } \n\n     def   <( that :   Any ) :   Boolean   =   { \n         if   (! that . isInstanceOf [ Date ]) \n             error ( \"cannot compare \"   +   that   +   \" and a Date\" ) \n         val   o   =   that . asInstanceOf [ Date ]( year   <   o . year )   || \n             ( year   ==   o . year   &&   ( month   <   o . month   || \n             ( month   ==   o . month   &&   day   <   o . day      ))) \n     }  }",
            "title": "Traits"
        },
        {
            "location": "/Scala/Scala/#generics",
            "text": "class   Reference [ T ]   { \n     private   var   contents :   T   =   _               // _ represents a default value. This default value is 0 for numeric types, false for the Boolean type, () for the Unit type and null for all object types. \n     def   set ( value :   T )   {   contents   =   value   } \n     def   get :   T   =   contents  }",
            "title": "Generics"
        },
        {
            "location": "/Scala/Scala/#frameworks",
            "text": "The Lift Framework  The Play framework  The Bowler framework  Akka",
            "title": "Frameworks"
        },
        {
            "location": "/Search/ElasticSearch/",
            "text": "Cheatsheets\n\u00b6\n\n\nJolicode\n\n\nDevelopment URLs\n\u00b6\n\n\nKibana (port 5601)\n\n\nSense\n\n\nElasticSearch (port 9200)\n\n\nINSTALL\n\u00b6\n\n\n\n\nInstall \ncurl\n\n\nInstall \nJava\n\n\nDownload \nElasticSearch\n\n\nOptionally change the \ncluster.name\n in the \nelasticsearch.yml\n configuration\n\n\n\n\ncd\n elasticsearch-<version>\n./bin/elasticsearch -d\n\n# or on Windows \n\n\n# bin\\elasticsearch.bat\n\ncurl \n'http://localhost:9200/?pretty'\n\n\n\n\n\n\n\n\n\n\nInstall \nKibana\n\n\n\n\nOpen \nconfig/kibana.yml\n in an editor\n\n\nSet the elasticsearch.url to point at your Elasticsearch instance\n\n\nRun \n./bin/kibana\n (orbin\\kibana.bat on Windows)\n\n\nPoint your browser at \nhttp://localhost:5601\n\n\n\n\n\n\n\n\nInstall \nSense\n\n\n\n\n\n\n./bin/kibana plugin --install elastic/sense\n\n\n\n\n\nOn Windows:\n \n\n\nbin\n\\k\nibana.bat plugin --install elastic/sense\n\n\n\n\n\nThen go to\n\n\nhttp://localhost:5601/app/sense\n\n\nCURL\n\u00b6\n\n\ncurl -X<VERB> \n'<PROTOCOL>://<HOST>:<PORT>/<PATH>?<QUERY_STRING>'\n -d \n'<BODY>'\n\n\n\n\n\n\nverb is GET, POST, PUT, HEAD, or DELETE\n\n\nExamples\n\u00b6\n\n\ncurl -XGET \n'http://localhost:9200/_count?pretty'\n -d \n'{ \"query\": { \"match_all\": {} }}'\n\n\n\n\n\n\ncurl -XGET <id>.us-west-2.es.amazonaws.com\n\ncurl -XGET \n'https://<id>.us-west-2.es.amazonaws.com/_count?pretty'\n -d \n'{ \"query\": { \"match_all\": {} } }'\n\n\ncurl -XPUT https://<id>.us-west-2.es.amazonaws.com/movies/movie/tt0116996 -d \n'{\"directors\" : [\"Tim Burton\"],\"genres\" : [\"Comedy\",\"Sci-Fi\"], \"plot\": \"The Earth is invaded by Martians with irresistible weapons and a cruel sense of humor.\", \"title\" : \"Mars Attacks!\", \"actors\" :[\"Jack Nicholson\",\"Pierce Brosnan\",\"Sarah Jessica Parker\"], \"year\" : 1996}'\n\n\n\n\n\n\nSense\n\u00b6\n\n\nSense syntax is similar to curl:\n\n\nIndex a document\n\n\nPUT index/type/1\n{\n \"body\": \"here\"\n}\n\n\n\n\n\nand retrieve it\n\n\nGET index/type/1\n\n\n\n\n\nPLUGINS\n\u00b6\n\n\nURL pattern\n\n\nhttp://yournode:9200/_plugin/<plugin name>\n\n\nOn Debian, the script is in: \n/usr/share/elasticsearch/bin/plugin\n.\n\n\nInstall various plugins\n\n\n./bin/plugin --install mobz/elasticsearch-head\n./bin/plugin --install lmenezes/elasticsearch-kopf/1.2\n./bin/plugin --install elasticsearch/marvel/latest\n\n\n\n\n\nRemove a plugin\n\n\n./bin/plugin --remove\n\n\n\n\n\nList installed plugins\n\n\n./bin/plugin --list\n\n\n\n\n\nGET /_nodes?plugin=true\n\n\n\n\n\nElasticsearch monitoring and management plugins\n\n\nHead\n\n\nHead\n\n\n\n\nelasticsearch/bin/plugin -install mobz/elasticsearch-head\n\n\nopen \nhttp://localhost:9200/_plugin/head\n\n\n\n\nelastichq.org\n\n\nBigDesk\n\n\nLive charts and statistics for elasticsearch cluster: \n\nBigDesk\n\n\nKopf\n\n\nKopf\n\n\n./bin/plugin --install lmenezes/elasticsearch-kopf/1.2`\n\n\n\n\n\nMarvel\n\n\n./bin/plugin --install elasticsearch/marvel/latest\n\n\n\n\n\nIntegrations (CMS, import/export, hadoop...)\n\u00b6\n\n\nIntegrations\n\n\nAspire\n\n\nAspire\n\n\nAspire is a framework and libraries of extensible components designed to enable creation of solutions to acquire data from one or more content repositories (such as file systems, relational databases, cloud storage, or content management systems), extract metadata and text from the documents, analyze, modify and enhance the content and metadata if needed, and then publish each document, together with its metadata, to a search engine or other target application\n\n\nDocs\n\n\nIntegration with Hadoop\n\n\nIntegration with Hadoop\n\n\nBulk loading for elastic search http://infochimps.com\n\n\nIntegration with Spring\n\n\nSpring Data\n\n\nWordPress\n\n\nWordpress\n\n\nTOOLS\n\u00b6\n\n\nBI platforms that can use ES as an analytics engine:\n\n\n\n\nKibana\n\n\nGrafana\n\n\n\n\nBIRT\n\n\n\n\nBirt\n\n\nBirt\n\n\n\n\n\n\n\n\nAdminer\n\n\n\n\nAdminer.org\n\n\nDatabase management in a single PHP file. Works with MySQL, PostgreSQL, SQLite, MS SQL, Oracle, SimpleDB, Elasticsearch, MongoDB. Needs a webserver + PHP: \nWAMP\n\n\n\n\n\n\n\n\nMongolastic\n\n\n\n\nA tool that migrates data from MongoDB to Elasticsearch and vice versa\n\n\nMongolastic\n\n\n\n\n\n\n\n\nElasticsearch-exporter\n\n\n\n\nElasticsearch-exporter\n\n\n\n\n\n\n\n\nCode Examples - developing a Web UI for ES\n\u00b6\n\n\n\n\nSitepoint\n\n\nCottageLabs\n\n\nscrutmydocs.org\n\n\nqbox.io\n\n\n\n\nJava API\n\u00b6\n\n\n\n\nJava clients\n\n\nelasticsearch tutorial\n\n\nelasticsearchfr/\n\n\nIBM\n\n\ndzone\n\n\n\n\nBASICS\n\u00b6\n\n\nAn Elasticsearch cluster can contain multiple indices, which in turn contain multiple types. These types hold multiple documents, and each document has multiple fields.\n\n\nExplore (using Sense)\n\u00b6\n\n\nGET _stats/\n\n\n# List indices\n\n\nGET /_cat/indices/\nGET /_cat/indices/my_ind*\n\n\n\n\n\n# Get info about one index\n\n\nGET /twitter\nGET /my_index_nr_1*/_settings?pretty   or ?v\nGET /twitter/_settings,_mappings\n\n\n\n\n\nThe available features are _settings, _mappings, _warmers and _aliases\n\n\n# cluster\n\n\nGET /_nodes\n\n\n\n\n\n# insert data\n\n\nPUT my_index/user/1\n{\n\"first_name\":    \"John\",\n\"last_name\":     \"Smith\",\n\"date_of_birth\": \"1970-10-24\"\n}\n\n\n\n\n\n#search\n\n\nGET my_index/_search\n\nGET _count?pretty\n\n\n\n\n\n# Data schema\n\n\nGET my_index/_mapping\n\n\n\n\n\nINSERT DOCUMENTS\n\u00b6\n\n\nPUT /index/type/ID\nPUT /megacorp/employee/1\n{ \"first_name\" : \"John\", \"last_name\" : \"Smith\", \"age\" : 25, \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ]}\n\nPUT /megacorp/employee/2\n{ \"first_name\" : \"Jane\", \"last_name\" : \"Smith\", \"age\" : 32, \"about\" : \"I like to collect rock albums\", \"interests\": [ \"music\" ]}\n\nGET /megacorp/employee/1\n\n\n\n\n\nField names can be any valid string, but may not include periods.\nEvery document in Elasticsearch has a version number. Every time a change is made to a document (including deleting it), the _version number is incremented.\n\n\nOptimistic concurrency control\n\n\nPUT /website/blog/1?version=1  { \"title\": \"My first blog entry\", \"text\": \"Starting to get the hang of this...\"}\n\nWe want this update to succeed only if the current _version of this document in our index is version 1\n\nExternal version:\n\nPUT /website/blog/2?version=5&version_type=external { \"title\": \"My first external blog entry\", \"text\": \"Starting to get the hang of this...\"}\n\n\n\n\n\nINSERT DOCUMENTS - AUTOGENERATED IDS\n\u00b6\n\n\nPOST /website/blog/\n{\n\"title\": \"My second blog entry\",\n\"text\":  \"Still trying this out...\",\n\"date\":  \"2014/01/01\"\n}\n\n\n\n\n\nResponse:\n\n\n{\n\"_index\":    \"website\",\n\"_type\":     \"blog\",\n\"_id\":       \"AVFgSgVHUP18jI2wRx0w\",\n\"_version\":  1,\n\"created\":   true\n}\n\n\n\n\n\n#  creating an entirely new document and not overwriting an existing one\n\n\nPUT /website/blog/123?op_type=create { ... }\nPUT /website/blog/123/_create { ... }\n\n\n\n\n\nRETRIEVE DOCUMENTS\n\u00b6\n\n\nGET /website/blog/123  # optional ?pretty\n\n\n\n\n\n{ \"_index\" : \"website\", \"_type\" : \"blog\", \"_id\" : \"123\", \"_version\" : 1, \"found\" : true, \"_source\" : { \"title\": \"My first blog entry\", \"text\": \"Just trying this out...\", \"date\": \"2014/01/01\" }}\n\n\n# Contains just the fields that we requested\n\n\nGET /website/blog/123?_source=title,text\n\n\n\n\n\n# Just get the original doc\n\n\nGET /website/blog/123/_source\n\n\n\n\n\n# check if doc exists -- HTTP 200 or 404\n\n\ncurl -i -XHEAD http://localhost:9200/website/blog/123\n\n\n\n\n\n# Note: HEAD/exists requests do not work in Sense\n# because they only return HTTP headers, not\n# a JSON body\n\n\n# multiple docs at once\n\n\nGET /website/blog/_mget { \"ids\" : [ \"2\", \"1\" ]}\n\n\n\n\n\nUPDATE\n\u00b6\n\n\nDocuments in Elasticsearch are immutable; we cannot change them. Instead, if we need to update an existing document, we reindex or replace it\n\n\n# Accepts a partial document as the doc parameter, which just gets merged with the existing document.\n\n\nPOST /website/blog/1/_update\n{ \"doc\" : { \"tags\" : [ \"testing\" ], \"views\": 0 }}\n\n\n\n\n\n# Script\n\n\nPOST /website/blog/1/_update\n{ \"script\" : \"ctx._source.views+=1\"}\n\n\n\n\n\n# script with parameters\n\n\nPOST /website/blog/1/_update\n{ \"script\" : \"ctx._source.tags+=new_tag\", \"params\" : { \"new_tag\" : \"search\" }}\n\n\n\n\n\n# upsert\n\n\nPOST/website/pageviews/1/_update\n{\"script\":\"ctx._source.views+=1\",\"upsert\":{\"views\":1}}\n\n\n\n\n\nDELETE\n\u00b6\n\n\nDELETE /website/blog/123\n\n\n\n\n\n# delete doc based on its contents\n\n\nPOST /website/blog/1/_update { \"script\" : \"ctx.op = ctx._source.views == count ? 'delete' : 'none'\", \"params\" : { \"count\": 1 }}\n\n\n\n\n\nBULK\n\u00b6\n\n\nPOST /_bulk\n{\"delete\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}}\n{\"create\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}} #  Create a document only if the document does not already exist\n{\"title\":\"My first blog post\"}\n{\"index\":{\"_index\":\"website\",\"_type\":\"blog\"}}\n{\"title\":\"My second blog post\"}\n{\"update\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\",\"_retry_on_conflict\":3}}\n{\"doc\":{\"title\":\"My updated blog post\"}}\n\n\n\n\n\nBulk in the same index or index/type\n\n\nPOST /website/_bulk\n{\"index\":{\"_type\":\"log\"}}\n{\"event\":\"User logged in\"}\n{\"index\":{\"_type\":\"blog\"}}\n{\"title\":\"My second blog post\"}\n\n\n\n\n\nTry  around 5-15MB in size.\n\n\nSEARCH\n\u00b6\n\n\nEvery field in a document is indexed and can be queried.\n\n\n# Search for all employees in the megacorp index:\n\n\nGET /megacorp/employee/_search\n\n\n\n\n\n# Search for all employees in the megacorp index\n# who have \"Smith\" in the last_name field\n\n\nGET /megacorp/employee/_search?q=last_name:Smith\n\n\n\n\n\n# Same query as above, but using the Query DSL\n\n\nGET /megacorp/employee/_search\n{\n    \"query\": {\n      \"match\": {\n        \"last_name\": \"smith\"\n      }\n    }\n}\n\n\n\n\n\n# SEARCH QUERY STRING\n\n\nGET /_all/tweet/_search?q=tweet:elasticsearch\n\n\n\n\n\nDon't forget to URL encode special characters e.g. +name:john +tweet:mary\n\n\nGET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary\n\n\n\n\n\nThe + prefix indicates conditions that must be satisfied for our query to match. Similarly a - prefix would indicate conditions that must not match. All conditions without a + or - are optional\n\n\n+name:(mary john) +date:>2014-09-10 +(aggregations geo) # last part searches _all\n\n\n\n\n\nQUERY DSL\n\u00b6\n\n\nWhen used in filtering context, the query is said to be a \"non-scoring\" or \"filtering\" query. That is, the query simply asks the question: \"Does this document match?\". The answer is always a simple, binary yes|no.\nWhen used in a querying context, the query becomes a \"scoring\" query.\n\n\n# Find all employees whose `last_name` is Smith\n# and who are older than 30\nGET /megacorp/employee/_search\n{\n\"query\" : {\n  \"filtered\" : {\n      \"filter\" : {\n      \"range\" : {\n          \"age\" : { \"gt\" : 30 }\n      }\n      },\n      \"query\" : {\n      \"match\" : {\n          \"last_name\" : \"smith\"\n      }\n      }\n  }\n}\n}\n\n\n\n\n\nMATCH\n\u00b6\n\n\n# Find all employees who enjoy \"rock\" or \"climbing\"\n\n\nGET /megacorp/employee/_search\n{\n\"query\" : {\n  \"match\" : {\n      \"about\" : \"rock climbing\"\n  }\n}\n}\n\n\n\n\n\nThe match query should be the standard query that you reach for whenever you want to query for a full-text or exact value in almost any field.\nIf you run a match query against a full-text field, it will analyze the query string by using the correct analyzer for that field before executing the search\nIf you use it on a field containing an exact value, such as a number, a date, a Boolean, or a not_analyzedstring field, then it will search for that exact value\n\n\nMATCH ON MULTIPLE FIELDS\n\u00b6\n\n\n{\n\"multi_match\": {\n  \"query\":    \"full text search\",\n  \"fields\":  [ \"title\", \"body\" ]\n}}\n\n\n\n\n\nEXACT SEARCH\n\u00b6\n\n\n# Find all employees who enjoy \"rock climbing\"\n\n\nGET /megacorp/employee/_search\n{\n\"query\" : {\n  \"match_phrase\" : {\n      \"about\" : \"rock climbing\"\n  }\n}\n}\n\n\n\n\n\n# EXACT VALUES\n\n\nThe term query is used to search by exact values, be they numbers, dates, Booleans, or not_analyzed exact-value string fields\n\n\nThe terms query is the same as the term query, but allows you to specify multiple values to match. If the field contains any of the specified values, the document matches\n\n\n{ \"terms\": { \"tag\": [ \"search\", \"full_text\", \"nosql\" ] }}\n\n\n\n\n\n# Compound Queries\n\n\n{\n     \"bool\": {\n       \"must\": { \"match\": { \"tweet\": \"elasticsearch\" }},\n        \"must_not\": { \"match\": { \"name\": \"mary\" }},\n        \"should\": { \"match\": { \"tweet\": \"full text\" }},\n        \"filter\": { \"range\": { \"age\" : { \"gt\" : 30 }} }\n     }\n  }\n\n\n# VALIDATE A QUERY\n\n\nGET /gb/tweet/_validate/query?explain { \"query\": { \"tweet\" : { \"match\" : \"really powerful\" } }}\n\n\n# understand why one particular document matched or, more important, why it didn\u2019t match\n\n\nGET /us/tweet/12/_explain { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 2 }}, \"must\" : { \"match\" : { \"tweet\" : \"honeymoon\" }} } }}\n\n\nMULTIPLE INDICES OR TYPES\n\u00b6\n\n\n# all documents all indices\n\n\n\n\n\n/_search\n\n\n/gb,us/_search\n  Search all types in the gb and us indices\n\n\n/g\n,u\n/_search\n  Search all types in any indices beginning with g or beginning with u\n\n\n/gb/user/_search\n  Search type user in the gb index\n\n\n/gb,us/user,tweet/_search\n  Search types user and tweet in the gb and us indices\n\n\n/_all/user,tweet/_search\n  Search types user and tweet in all indices\n\n\nPAGINATION\n\u00b6\n\n\n GET /_search?size=5GET /_search?size=5&from=5\n\n\n\n\n\nSORTING\n\u00b6\n\n\n GET /_search { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 1 }} } }, \"sort\": { \"date\": { \"order\": \"desc\" }}}\n\n\n\n\n\nFor string sorting, use multi-field mapping:\n\n\n \"tweet\": { \"type\": \"string\", \"analyzer\": \"english\", \"fields\": { \"raw\": {\"type\": \"string\", \"index\": \"not_analyzed\" } }}\n\n\n\n\n\nThe main tweet field is just the same as before: an analyzed full-text field.\nThe new tweet.raw subfield is not_analyzed.\n\n\nthen sort on the new field\n\n\n GET /_search { \"query\": { \"match\": { \"tweet\": \"elasticsearch\" } }, \"sort\": \"tweet.raw\"}\n\n\n\n\n\nHIGHLIGHTS\n\u00b6\n\n\n# Find all employees who enjoy \"rock climbing\" - highlights\n# and highlight the matches\n\n\nGET /megacorp/employee/_search\n  {\n      \"query\" : {\n          \"match_phrase\" : {\n              \"about\" : \"rock climbing\"\n          }\n      },\n      \"highlight\": {\n          \"fields\" : {\n              \"about\" : {}\n          }\n      }\n  }\n\n\nANALYSIS\n\u00b6\n\n\nAn analyzer is really just a wrapper that combines three functions into a single package:\n\n\n* Character filters\n* Tokenizer\n* Token filters\n\n\n\n\n\n#  See how text is analyzed\n\n\nGET /_analyze { \"analyzer\": \"standard\", \"text\": \"Text to analyze\"}\n\n\n# test analyzer\n\n\nGET /gb/_analyze { \"field\": \"tweet\", \"text\": \"Black-cats\"}\n\n\nMAPPINGS (schemas)\n\u00b6\n\n\nEvery type has its own mapping, or schema definition. A mapping defines the fields within a type, the datatype for each field, and how the field should be handled by Elasticsearch. A mapping is also used to configure metadata associated with the type.\n\n\nYou can control dynamic nature of mappings\n\n\nMapping (or schema definition) for the tweet type in the gb index\n\n\n  GET /gb/_mapping/tweet\n\n\n\n\n\nElasticsearch supports the following simple field types:\n\n String: string\n\n Whole number: byte, short, integer, long\n\n Floating-point: float, double\n\n Boolean: boolean\n* Date: date\n\n\nFields of type string are, by default, considered to contain full text. That is, their value will be passed through an analyzer before being indexed, and a full-text query on the field will pass the query string through an analyzer before searching.\nThe two most important mapping attributes for string fields are index and analyzer.\n\n\nThe index attribute controls how the string will be indexed. It can contain one of three values:\n\n analyzed  First analyze the string and then index it. In other words, index this field as full text.\n\n not_analyzed  Index this field, so it is searchable, but index the value exactly as specified. Do not analyze it.\n* no  Don\u2019t index this field at all. This field will not be searchable.\n\n\nIf we want to map the field as an exact value, we need to set it to not_analyzed:\n\n\n  {\n    \"tag\": {\n    \"type\": \"string\",\n    \"index\": \"not_analyzed\"\n    }\n  }\n\n\n\n\n\nFor analyzed string fields, use the analyzer attribute to specify which analyzer to apply both at search time and at index time. By default, Elasticsearch uses the standard analyzer, but you can change this by specifying one of the built-in analyzers, such as whitespace, simple, or english:\n\n\n  {\n    \"tweet\": {\n    \"type\": \"string\",\n    \"analyzer\": \"english\"\n    }\n  }\n\n\n\n\n\n#  create a new index, specifying that the tweet field should use the english analyzer\n\n\nPUT /gb\n  { \"mappings\":\n       { \"tweet\" :\n                 { \"properties\" : {\n                      \"tweet\" : { \"type\" : \"string\", \"analyzer\": \"english\" },\n                      \"date\" : { \"type\" : \"date\" },\n                      \"name\" : { \"type\" : \"string\" },\n                      \"user_id\" : { \"type\" : \"long\" }\n                    }}}}\n\n\nnull, arrays, objects: see \ncomplex core fields\n\n\nParent Child Relationships\n\u00b6\n\n\nDELETE\n \n/test_index\n\n\n\nPUT\n \n/test_index\n\n\n{\n\n   \n\"mappings\"\n:\n \n{\n\n      \n\"parent_type\"\n:\n \n{\n\n         \n\"properties\"\n:\n \n{\n\n            \n\"num_prop\"\n:\n \n{\n\n               \n\"type\"\n:\n \n\"integer\"\n\n            \n},\n\n            \n\"str_prop\"\n:\n \n{\n\n               \n\"type\"\n:\n \n\"string\"\n\n            \n}\n\n         \n}\n\n      \n},\n\n      \n\"child_type\"\n:\n \n{\n\n         \n\"_parent\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"parent_type\"\n\n         \n},\n\n         \n\"properties\"\n:\n \n{\n\n            \n\"child_num\"\n:\n \n{\n\n               \n\"type\"\n:\n \n\"integer\"\n\n            \n},\n\n            \n\"child_str\"\n:\n \n{\n\n               \n\"type\"\n:\n \n\"string\"\n\n            \n}\n\n         \n}\n\n      \n}\n\n   \n}\n\n\n}\n\n\n\nPOST\n \n/test_index/_bulk\n\n\n{\n\"index\"\n:{\n\"_type\"\n:\n\"parent_type\"\n,\n\"_id\"\n:\n1\n}}\n\n\n{\n\"num_prop\"\n:\n1\n,\n\"str_prop\"\n:\n\"hello\"\n}\n\n\n{\n\"index\"\n:{\n\"_type\"\n:\n\"child_type\"\n,\n\"_id\"\n:\n1\n,\n\"_parent\"\n:\n1\n}}\n\n\n{\n\"child_num\"\n:\n11\n,\n\"child_str\"\n:\n\"foo\"\n}\n\n\n{\n\"index\"\n:{\n\"_type\"\n:\n\"child_type\"\n,\n\"_id\"\n:\n2\n,\n\"_parent\"\n:\n1\n}}\n\n\n{\n\"child_num\"\n:\n12\n,\n\"child_str\"\n:\n\"bar\"\n}\n\n\n{\n\"index\"\n:{\n\"_type\"\n:\n\"parent_type\"\n,\n\"_id\"\n:\n2\n}}\n\n\n{\n\"num_prop\"\n:\n2\n,\n\"str_prop\"\n:\n\"goodbye\"\n}\n\n\n{\n\"index\"\n:{\n\"_type\"\n:\n\"child_type\"\n,\n\"_id\"\n:\n3\n,\n\"_parent\"\n:\n2\n}}\n\n\n{\n\"child_num\"\n:\n21\n,\n\"child_str\"\n:\n\"baz\"\n}\n\n\n\nPOST\n \n/test_index/child_type/_search\n\n\n\nPOST\n \n/test_index/child_type/\n2\n?parent=\n1\n\n\n{\n\n   \n\"child_num\"\n:\n \n13\n,\n\n   \n\"child_str\"\n:\n \n\"bars\"\n\n\n}\n\n\n\nPOST\n \n/test_index/child_type/_search\n\n\n\nPOST\n \n/test_index/child_type/\n3\n/_update?parent=\n2\n\n\n{\n\n   \n\"script\"\n:\n \n\"ctx._source.child_num+=1\"\n\n\n}\n\n\n\nPOST\n \n/test_index/child_type/_search\n\n\n\nPOST\n \n/test_index/child_type/_search\n\n\n{\n\n    \n\"query\"\n:\n \n{\n\n        \n\"term\"\n:\n \n{\n\n           \n\"child_str\"\n:\n \n{\n\n              \n\"value\"\n:\n \n\"foo\"\n\n           \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\nPOST\n \n/test_index/parent_type/_search\n\n\n{\n\n   \n\"query\"\n:\n \n{\n\n      \n\"filtered\"\n:\n \n{\n\n         \n\"query\"\n:\n \n{\n\n            \n\"match_all\"\n:\n \n{}\n\n         \n},\n\n         \n\"filter\"\n:\n \n{\n\n            \n\"has_child\"\n:\n \n{\n\n               \n\"type\"\n:\n \n\"child_type\"\n,\n\n               \n\"filter\"\n:\n \n{\n\n                  \n\"term\"\n:\n \n{\n\n                     \n\"child_str\"\n:\n \n\"foo\"\n\n                  \n}\n\n               \n}\n\n            \n}\n\n         \n}\n\n      \n}\n\n   \n}\n\n\n}\n\n\n\n\n\n\nAGGREGATES\n\u00b6\n\n\nAggregations and searches can span multiple indices\n\n\n# Calculate the most popular interests for all employees\n\n\nGET /megacorp/employee/_search\n  {\n    \"aggs\": {\n      \"all_interests\": {\n        \"terms\": {\n          \"field\": \"interests\"\n        }\n      }\n    }\n  }\n\n\n# Calculate the most popular interests for\n# employees named \"Smith\"\n\n\nGET /megacorp/employee/_search\n  {\n    \"query\": {\n      \"match\": {\n        \"last_name\": \"smith\"\n      }\n    },\n    \"aggs\": {\n      \"all_interests\": {\n        \"terms\": {\n          \"field\": \"interests\"\n        }\n      }\n    }\n  }\n\n\n# Calculate the average age of employee per interest - hierarchical aggregates\n\n\nGET /megacorp/employee/_search\n  {\n      \"aggs\" : {\n          \"all_interests\" : {\n              \"terms\" : { \"field\" : \"interests\" },\n              \"aggs\" : {\n                  \"avg_age\" : {\n                      \"avg\" : { \"field\" : \"age\" }\n                  }\n              }\n          }\n      }\n  }\n\n\n# requires in config/elasticsearch.yml\n# script.inline: true\n# script.indexed: true\n\n\nGET /tlo/contacts/_search\n  {\n    \"size\" : 0,\n    \"query\": {\n      \"constant_score\": {\n        \"filter\": {\n          \"terms\": {\n            \"version\": [\n              \"20160301\",\n              \"20160401\"\n            ]\n          }\n        }\n      }\n    },\n    \"aggs\": {\n      \"counts\": {\n        \"cardinality\": {\n          \"script\": \"doc['first_name'].value + ' ' + doc['last_name'].value + ' ' + doc['company'].value\",\n          \"missing\": \"N/A\"\n        }\n      }\n    }\n  }\n\n\nINDEX MANAGEMENT\n\u00b6\n\n\nBy default, indices are assigned five primary shards. The number of primary shards can be set only when an index is created and never changed\n\n\n# Add an index\n\n\n  PUT /blogs { \"settings\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 1 }}\n  PUT /blogs/_settings { \"number_of_replicas\" : 2}\n\n\n\n\n\n\n\nElasticSearch Shards should be 50 GB or less in size.\n\n\nUse aliases to shelter the underlying index (or indices) and allow index swapping\n\n\n\n\nCLUSTER MANAGEMENT\n\u00b6\n\n\n GET /_cluster/health\n\n\n\n\n\nCONFIGURATION\n\u00b6\n\n\n\n\nconfig directory\n\n\n\n\nyaml file\n\n\n\n\n\n\nSets the JVM heap size to 0.5 memory size. The OS will use it for file system cache\n\n\n\n\nPrefer not to allocate 30GB !! --> uncompressed pointers\n\n\nNever let the JVM swap    bootstrap.mlockall = true\n\n\nKeep the JVM defaults\n\n\nDo not use G1GC alternative garbage collector\n\n\n\n\ncluster.name: <my cluster>\n\n\n\n\n\n\n\nAll nodes in the cluster must have the same cluster name\n\n\n\n\nnode.name: <my_node_name>\n\n\n\n\n\n./bin/elasticsearch --node.name=`hostname`\n\n\n\n\n\nto override the configuration file\n\n\n\n\nHTTP port: 9200 and successors\n\n\nTransport : 9300 (internal communications)\n\n\n\n\nDiscovery\n\u00b6\n\n\n\n\nAWS plugin available   --> also include integration with S3 (snapshot to S3)\n\n\nAWS: multi-AZ is OK but replication across far data centers is not recommended\n\n\nSee: resiliency\n\n\n\n\nSites plugins -- kopf / head / paramedic / bigdesk / kibana\n- contain static web content (JS, HTML....)\n\n\nInstall plugins on ALL machines of the cluster\n\n\nTo install,\n\n\n  ./bin/plugin install marvel-agent\n  ./bin/plugin remove marvel-agent\n\n\n\n\n\nOne type per index is recommended, except for parent child / nested indexes.\n\n\nindex size optimization:\n- can disable \n_source\n and \n_all\n (the index that captures every field - not needed unless the top search bar changes)\n- by default, Kibana will search \n_all\n\n\ndata types:\nstring, number, bool, datetime, binary, array, object, geo_point, geo_shape, ip, multifield\nbinary should be base64 encoded before storage\n\n\nMAINTENANCE\n\u00b6\n\n\nSteps to restore elastic search data:\n\n\n\n\nStop elastic search\n\n\nExtract the zip file (dump file)\n\n\nStart elastic search\n\n\nReload elastic search\n\n\n\n\nThe commands to do the above are as below:\n\n\n\n\nsystemctl stop elasticsearch\n\n\nextract gz file to destination path\n\n\nsystemctl start elasticsearch\n\n\nsystemctl daemon-reload elasticsearch",
            "title": "ElasticSearch"
        },
        {
            "location": "/Search/ElasticSearch/#cheatsheets",
            "text": "Jolicode",
            "title": "Cheatsheets"
        },
        {
            "location": "/Search/ElasticSearch/#development-urls",
            "text": "Kibana (port 5601)  Sense  ElasticSearch (port 9200)",
            "title": "Development URLs"
        },
        {
            "location": "/Search/ElasticSearch/#install",
            "text": "Install  curl  Install  Java  Download  ElasticSearch  Optionally change the  cluster.name  in the  elasticsearch.yml  configuration   cd  elasticsearch-<version>\n./bin/elasticsearch -d # or on Windows   # bin\\elasticsearch.bat \ncurl  'http://localhost:9200/?pretty'     Install  Kibana   Open  config/kibana.yml  in an editor  Set the elasticsearch.url to point at your Elasticsearch instance  Run  ./bin/kibana  (orbin\\kibana.bat on Windows)  Point your browser at  http://localhost:5601     Install  Sense    ./bin/kibana plugin --install elastic/sense  On Windows:    bin \\k ibana.bat plugin --install elastic/sense  Then go to  http://localhost:5601/app/sense",
            "title": "INSTALL"
        },
        {
            "location": "/Search/ElasticSearch/#curl",
            "text": "curl -X<VERB>  '<PROTOCOL>://<HOST>:<PORT>/<PATH>?<QUERY_STRING>'  -d  '<BODY>'   verb is GET, POST, PUT, HEAD, or DELETE",
            "title": "CURL"
        },
        {
            "location": "/Search/ElasticSearch/#examples",
            "text": "curl -XGET  'http://localhost:9200/_count?pretty'  -d  '{ \"query\": { \"match_all\": {} }}'   curl -XGET <id>.us-west-2.es.amazonaws.com\n\ncurl -XGET  'https://<id>.us-west-2.es.amazonaws.com/_count?pretty'  -d  '{ \"query\": { \"match_all\": {} } }' \n\ncurl -XPUT https://<id>.us-west-2.es.amazonaws.com/movies/movie/tt0116996 -d  '{\"directors\" : [\"Tim Burton\"],\"genres\" : [\"Comedy\",\"Sci-Fi\"], \"plot\": \"The Earth is invaded by Martians with irresistible weapons and a cruel sense of humor.\", \"title\" : \"Mars Attacks!\", \"actors\" :[\"Jack Nicholson\",\"Pierce Brosnan\",\"Sarah Jessica Parker\"], \"year\" : 1996}'",
            "title": "Examples"
        },
        {
            "location": "/Search/ElasticSearch/#sense",
            "text": "Sense syntax is similar to curl:  Index a document  PUT index/type/1\n{\n \"body\": \"here\"\n}  and retrieve it  GET index/type/1",
            "title": "Sense"
        },
        {
            "location": "/Search/ElasticSearch/#plugins",
            "text": "URL pattern  http://yournode:9200/_plugin/<plugin name>  On Debian, the script is in:  /usr/share/elasticsearch/bin/plugin .  Install various plugins  ./bin/plugin --install mobz/elasticsearch-head\n./bin/plugin --install lmenezes/elasticsearch-kopf/1.2\n./bin/plugin --install elasticsearch/marvel/latest  Remove a plugin  ./bin/plugin --remove  List installed plugins  ./bin/plugin --list  GET /_nodes?plugin=true  Elasticsearch monitoring and management plugins  Head  Head   elasticsearch/bin/plugin -install mobz/elasticsearch-head  open  http://localhost:9200/_plugin/head   elastichq.org  BigDesk  Live charts and statistics for elasticsearch cluster:  BigDesk  Kopf  Kopf  ./bin/plugin --install lmenezes/elasticsearch-kopf/1.2`  Marvel  ./bin/plugin --install elasticsearch/marvel/latest",
            "title": "PLUGINS"
        },
        {
            "location": "/Search/ElasticSearch/#integrations-cms-importexport-hadoop",
            "text": "Integrations  Aspire  Aspire  Aspire is a framework and libraries of extensible components designed to enable creation of solutions to acquire data from one or more content repositories (such as file systems, relational databases, cloud storage, or content management systems), extract metadata and text from the documents, analyze, modify and enhance the content and metadata if needed, and then publish each document, together with its metadata, to a search engine or other target application  Docs  Integration with Hadoop  Integration with Hadoop  Bulk loading for elastic search http://infochimps.com  Integration with Spring  Spring Data  WordPress  Wordpress",
            "title": "Integrations (CMS, import/export, hadoop...)"
        },
        {
            "location": "/Search/ElasticSearch/#tools",
            "text": "BI platforms that can use ES as an analytics engine:   Kibana  Grafana   BIRT   Birt  Birt     Adminer   Adminer.org  Database management in a single PHP file. Works with MySQL, PostgreSQL, SQLite, MS SQL, Oracle, SimpleDB, Elasticsearch, MongoDB. Needs a webserver + PHP:  WAMP     Mongolastic   A tool that migrates data from MongoDB to Elasticsearch and vice versa  Mongolastic     Elasticsearch-exporter   Elasticsearch-exporter",
            "title": "TOOLS"
        },
        {
            "location": "/Search/ElasticSearch/#code-examples-developing-a-web-ui-for-es",
            "text": "Sitepoint  CottageLabs  scrutmydocs.org  qbox.io",
            "title": "Code Examples - developing a Web UI for ES"
        },
        {
            "location": "/Search/ElasticSearch/#java-api",
            "text": "Java clients  elasticsearch tutorial  elasticsearchfr/  IBM  dzone",
            "title": "Java API"
        },
        {
            "location": "/Search/ElasticSearch/#basics",
            "text": "An Elasticsearch cluster can contain multiple indices, which in turn contain multiple types. These types hold multiple documents, and each document has multiple fields.",
            "title": "BASICS"
        },
        {
            "location": "/Search/ElasticSearch/#explore-using-sense",
            "text": "GET _stats/  # List indices  GET /_cat/indices/\nGET /_cat/indices/my_ind*  # Get info about one index  GET /twitter\nGET /my_index_nr_1*/_settings?pretty   or ?v\nGET /twitter/_settings,_mappings  The available features are _settings, _mappings, _warmers and _aliases  # cluster  GET /_nodes  # insert data  PUT my_index/user/1\n{\n\"first_name\":    \"John\",\n\"last_name\":     \"Smith\",\n\"date_of_birth\": \"1970-10-24\"\n}  #search  GET my_index/_search\n\nGET _count?pretty  # Data schema  GET my_index/_mapping",
            "title": "Explore (using Sense)"
        },
        {
            "location": "/Search/ElasticSearch/#insert-documents",
            "text": "PUT /index/type/ID\nPUT /megacorp/employee/1\n{ \"first_name\" : \"John\", \"last_name\" : \"Smith\", \"age\" : 25, \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ]}\n\nPUT /megacorp/employee/2\n{ \"first_name\" : \"Jane\", \"last_name\" : \"Smith\", \"age\" : 32, \"about\" : \"I like to collect rock albums\", \"interests\": [ \"music\" ]}\n\nGET /megacorp/employee/1  Field names can be any valid string, but may not include periods.\nEvery document in Elasticsearch has a version number. Every time a change is made to a document (including deleting it), the _version number is incremented.  Optimistic concurrency control  PUT /website/blog/1?version=1  { \"title\": \"My first blog entry\", \"text\": \"Starting to get the hang of this...\"}\n\nWe want this update to succeed only if the current _version of this document in our index is version 1\n\nExternal version:\n\nPUT /website/blog/2?version=5&version_type=external { \"title\": \"My first external blog entry\", \"text\": \"Starting to get the hang of this...\"}",
            "title": "INSERT DOCUMENTS"
        },
        {
            "location": "/Search/ElasticSearch/#insert-documents-autogenerated-ids",
            "text": "POST /website/blog/\n{\n\"title\": \"My second blog entry\",\n\"text\":  \"Still trying this out...\",\n\"date\":  \"2014/01/01\"\n}  Response:  {\n\"_index\":    \"website\",\n\"_type\":     \"blog\",\n\"_id\":       \"AVFgSgVHUP18jI2wRx0w\",\n\"_version\":  1,\n\"created\":   true\n}  #  creating an entirely new document and not overwriting an existing one  PUT /website/blog/123?op_type=create { ... }\nPUT /website/blog/123/_create { ... }",
            "title": "INSERT DOCUMENTS - AUTOGENERATED IDS"
        },
        {
            "location": "/Search/ElasticSearch/#retrieve-documents",
            "text": "GET /website/blog/123  # optional ?pretty  { \"_index\" : \"website\", \"_type\" : \"blog\", \"_id\" : \"123\", \"_version\" : 1, \"found\" : true, \"_source\" : { \"title\": \"My first blog entry\", \"text\": \"Just trying this out...\", \"date\": \"2014/01/01\" }}  # Contains just the fields that we requested  GET /website/blog/123?_source=title,text  # Just get the original doc  GET /website/blog/123/_source  # check if doc exists -- HTTP 200 or 404  curl -i -XHEAD http://localhost:9200/website/blog/123  # Note: HEAD/exists requests do not work in Sense\n# because they only return HTTP headers, not\n# a JSON body  # multiple docs at once  GET /website/blog/_mget { \"ids\" : [ \"2\", \"1\" ]}",
            "title": "RETRIEVE DOCUMENTS"
        },
        {
            "location": "/Search/ElasticSearch/#update",
            "text": "Documents in Elasticsearch are immutable; we cannot change them. Instead, if we need to update an existing document, we reindex or replace it  # Accepts a partial document as the doc parameter, which just gets merged with the existing document.  POST /website/blog/1/_update\n{ \"doc\" : { \"tags\" : [ \"testing\" ], \"views\": 0 }}  # Script  POST /website/blog/1/_update\n{ \"script\" : \"ctx._source.views+=1\"}  # script with parameters  POST /website/blog/1/_update\n{ \"script\" : \"ctx._source.tags+=new_tag\", \"params\" : { \"new_tag\" : \"search\" }}  # upsert  POST/website/pageviews/1/_update\n{\"script\":\"ctx._source.views+=1\",\"upsert\":{\"views\":1}}",
            "title": "UPDATE"
        },
        {
            "location": "/Search/ElasticSearch/#delete",
            "text": "DELETE /website/blog/123  # delete doc based on its contents  POST /website/blog/1/_update { \"script\" : \"ctx.op = ctx._source.views == count ? 'delete' : 'none'\", \"params\" : { \"count\": 1 }}",
            "title": "DELETE"
        },
        {
            "location": "/Search/ElasticSearch/#bulk",
            "text": "POST /_bulk\n{\"delete\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}}\n{\"create\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}} #  Create a document only if the document does not already exist\n{\"title\":\"My first blog post\"}\n{\"index\":{\"_index\":\"website\",\"_type\":\"blog\"}}\n{\"title\":\"My second blog post\"}\n{\"update\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\",\"_retry_on_conflict\":3}}\n{\"doc\":{\"title\":\"My updated blog post\"}}  Bulk in the same index or index/type  POST /website/_bulk\n{\"index\":{\"_type\":\"log\"}}\n{\"event\":\"User logged in\"}\n{\"index\":{\"_type\":\"blog\"}}\n{\"title\":\"My second blog post\"}  Try  around 5-15MB in size.",
            "title": "BULK"
        },
        {
            "location": "/Search/ElasticSearch/#search",
            "text": "Every field in a document is indexed and can be queried.  # Search for all employees in the megacorp index:  GET /megacorp/employee/_search  # Search for all employees in the megacorp index\n# who have \"Smith\" in the last_name field  GET /megacorp/employee/_search?q=last_name:Smith  # Same query as above, but using the Query DSL  GET /megacorp/employee/_search\n{\n    \"query\": {\n      \"match\": {\n        \"last_name\": \"smith\"\n      }\n    }\n}  # SEARCH QUERY STRING  GET /_all/tweet/_search?q=tweet:elasticsearch  Don't forget to URL encode special characters e.g. +name:john +tweet:mary  GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary  The + prefix indicates conditions that must be satisfied for our query to match. Similarly a - prefix would indicate conditions that must not match. All conditions without a + or - are optional  +name:(mary john) +date:>2014-09-10 +(aggregations geo) # last part searches _all",
            "title": "SEARCH"
        },
        {
            "location": "/Search/ElasticSearch/#query-dsl",
            "text": "When used in filtering context, the query is said to be a \"non-scoring\" or \"filtering\" query. That is, the query simply asks the question: \"Does this document match?\". The answer is always a simple, binary yes|no.\nWhen used in a querying context, the query becomes a \"scoring\" query.  # Find all employees whose `last_name` is Smith\n# and who are older than 30\nGET /megacorp/employee/_search\n{\n\"query\" : {\n  \"filtered\" : {\n      \"filter\" : {\n      \"range\" : {\n          \"age\" : { \"gt\" : 30 }\n      }\n      },\n      \"query\" : {\n      \"match\" : {\n          \"last_name\" : \"smith\"\n      }\n      }\n  }\n}\n}",
            "title": "QUERY DSL"
        },
        {
            "location": "/Search/ElasticSearch/#match",
            "text": "# Find all employees who enjoy \"rock\" or \"climbing\"  GET /megacorp/employee/_search\n{\n\"query\" : {\n  \"match\" : {\n      \"about\" : \"rock climbing\"\n  }\n}\n}  The match query should be the standard query that you reach for whenever you want to query for a full-text or exact value in almost any field.\nIf you run a match query against a full-text field, it will analyze the query string by using the correct analyzer for that field before executing the search\nIf you use it on a field containing an exact value, such as a number, a date, a Boolean, or a not_analyzedstring field, then it will search for that exact value",
            "title": "MATCH"
        },
        {
            "location": "/Search/ElasticSearch/#match-on-multiple-fields",
            "text": "{\n\"multi_match\": {\n  \"query\":    \"full text search\",\n  \"fields\":  [ \"title\", \"body\" ]\n}}",
            "title": "MATCH ON MULTIPLE FIELDS"
        },
        {
            "location": "/Search/ElasticSearch/#exact-search",
            "text": "# Find all employees who enjoy \"rock climbing\"  GET /megacorp/employee/_search\n{\n\"query\" : {\n  \"match_phrase\" : {\n      \"about\" : \"rock climbing\"\n  }\n}\n}  # EXACT VALUES  The term query is used to search by exact values, be they numbers, dates, Booleans, or not_analyzed exact-value string fields  The terms query is the same as the term query, but allows you to specify multiple values to match. If the field contains any of the specified values, the document matches  { \"terms\": { \"tag\": [ \"search\", \"full_text\", \"nosql\" ] }}  # Compound Queries  {\n     \"bool\": {\n       \"must\": { \"match\": { \"tweet\": \"elasticsearch\" }},\n        \"must_not\": { \"match\": { \"name\": \"mary\" }},\n        \"should\": { \"match\": { \"tweet\": \"full text\" }},\n        \"filter\": { \"range\": { \"age\" : { \"gt\" : 30 }} }\n     }\n  }  # VALIDATE A QUERY  GET /gb/tweet/_validate/query?explain { \"query\": { \"tweet\" : { \"match\" : \"really powerful\" } }}  # understand why one particular document matched or, more important, why it didn\u2019t match  GET /us/tweet/12/_explain { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 2 }}, \"must\" : { \"match\" : { \"tweet\" : \"honeymoon\" }} } }}",
            "title": "EXACT SEARCH"
        },
        {
            "location": "/Search/ElasticSearch/#multiple-indices-or-types",
            "text": "# all documents all indices  /_search  /gb,us/_search\n  Search all types in the gb and us indices  /g ,u /_search\n  Search all types in any indices beginning with g or beginning with u  /gb/user/_search\n  Search type user in the gb index  /gb,us/user,tweet/_search\n  Search types user and tweet in the gb and us indices  /_all/user,tweet/_search\n  Search types user and tweet in all indices",
            "title": "MULTIPLE INDICES OR TYPES"
        },
        {
            "location": "/Search/ElasticSearch/#pagination",
            "text": "GET /_search?size=5GET /_search?size=5&from=5",
            "title": "PAGINATION"
        },
        {
            "location": "/Search/ElasticSearch/#sorting",
            "text": "GET /_search { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 1 }} } }, \"sort\": { \"date\": { \"order\": \"desc\" }}}  For string sorting, use multi-field mapping:   \"tweet\": { \"type\": \"string\", \"analyzer\": \"english\", \"fields\": { \"raw\": {\"type\": \"string\", \"index\": \"not_analyzed\" } }}  The main tweet field is just the same as before: an analyzed full-text field.\nThe new tweet.raw subfield is not_analyzed.  then sort on the new field   GET /_search { \"query\": { \"match\": { \"tweet\": \"elasticsearch\" } }, \"sort\": \"tweet.raw\"}",
            "title": "SORTING"
        },
        {
            "location": "/Search/ElasticSearch/#highlights",
            "text": "# Find all employees who enjoy \"rock climbing\" - highlights\n# and highlight the matches  GET /megacorp/employee/_search\n  {\n      \"query\" : {\n          \"match_phrase\" : {\n              \"about\" : \"rock climbing\"\n          }\n      },\n      \"highlight\": {\n          \"fields\" : {\n              \"about\" : {}\n          }\n      }\n  }",
            "title": "HIGHLIGHTS"
        },
        {
            "location": "/Search/ElasticSearch/#analysis",
            "text": "An analyzer is really just a wrapper that combines three functions into a single package:  * Character filters\n* Tokenizer\n* Token filters  #  See how text is analyzed  GET /_analyze { \"analyzer\": \"standard\", \"text\": \"Text to analyze\"}  # test analyzer  GET /gb/_analyze { \"field\": \"tweet\", \"text\": \"Black-cats\"}",
            "title": "ANALYSIS"
        },
        {
            "location": "/Search/ElasticSearch/#mappings-schemas",
            "text": "Every type has its own mapping, or schema definition. A mapping defines the fields within a type, the datatype for each field, and how the field should be handled by Elasticsearch. A mapping is also used to configure metadata associated with the type.  You can control dynamic nature of mappings  Mapping (or schema definition) for the tweet type in the gb index    GET /gb/_mapping/tweet  Elasticsearch supports the following simple field types:  String: string  Whole number: byte, short, integer, long  Floating-point: float, double  Boolean: boolean\n* Date: date  Fields of type string are, by default, considered to contain full text. That is, their value will be passed through an analyzer before being indexed, and a full-text query on the field will pass the query string through an analyzer before searching.\nThe two most important mapping attributes for string fields are index and analyzer.  The index attribute controls how the string will be indexed. It can contain one of three values:  analyzed  First analyze the string and then index it. In other words, index this field as full text.  not_analyzed  Index this field, so it is searchable, but index the value exactly as specified. Do not analyze it.\n* no  Don\u2019t index this field at all. This field will not be searchable.  If we want to map the field as an exact value, we need to set it to not_analyzed:    {\n    \"tag\": {\n    \"type\": \"string\",\n    \"index\": \"not_analyzed\"\n    }\n  }  For analyzed string fields, use the analyzer attribute to specify which analyzer to apply both at search time and at index time. By default, Elasticsearch uses the standard analyzer, but you can change this by specifying one of the built-in analyzers, such as whitespace, simple, or english:    {\n    \"tweet\": {\n    \"type\": \"string\",\n    \"analyzer\": \"english\"\n    }\n  }  #  create a new index, specifying that the tweet field should use the english analyzer  PUT /gb\n  { \"mappings\":\n       { \"tweet\" :\n                 { \"properties\" : {\n                      \"tweet\" : { \"type\" : \"string\", \"analyzer\": \"english\" },\n                      \"date\" : { \"type\" : \"date\" },\n                      \"name\" : { \"type\" : \"string\" },\n                      \"user_id\" : { \"type\" : \"long\" }\n                    }}}}  null, arrays, objects: see  complex core fields",
            "title": "MAPPINGS (schemas)"
        },
        {
            "location": "/Search/ElasticSearch/#parent-child-relationships",
            "text": "DELETE   /test_index  PUT   /test_index  { \n    \"mappings\" :   { \n       \"parent_type\" :   { \n          \"properties\" :   { \n             \"num_prop\" :   { \n                \"type\" :   \"integer\" \n             }, \n             \"str_prop\" :   { \n                \"type\" :   \"string\" \n             } \n          } \n       }, \n       \"child_type\" :   { \n          \"_parent\" :   { \n             \"type\" :   \"parent_type\" \n          }, \n          \"properties\" :   { \n             \"child_num\" :   { \n                \"type\" :   \"integer\" \n             }, \n             \"child_str\" :   { \n                \"type\" :   \"string\" \n             } \n          } \n       } \n    }  }  POST   /test_index/_bulk  { \"index\" :{ \"_type\" : \"parent_type\" , \"_id\" : 1 }}  { \"num_prop\" : 1 , \"str_prop\" : \"hello\" }  { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 1 , \"_parent\" : 1 }}  { \"child_num\" : 11 , \"child_str\" : \"foo\" }  { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 2 , \"_parent\" : 1 }}  { \"child_num\" : 12 , \"child_str\" : \"bar\" }  { \"index\" :{ \"_type\" : \"parent_type\" , \"_id\" : 2 }}  { \"num_prop\" : 2 , \"str_prop\" : \"goodbye\" }  { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 3 , \"_parent\" : 2 }}  { \"child_num\" : 21 , \"child_str\" : \"baz\" }  POST   /test_index/child_type/_search  POST   /test_index/child_type/ 2 ?parent= 1  { \n    \"child_num\" :   13 , \n    \"child_str\" :   \"bars\"  }  POST   /test_index/child_type/_search  POST   /test_index/child_type/ 3 /_update?parent= 2  { \n    \"script\" :   \"ctx._source.child_num+=1\"  }  POST   /test_index/child_type/_search  POST   /test_index/child_type/_search  { \n     \"query\" :   { \n         \"term\" :   { \n            \"child_str\" :   { \n               \"value\" :   \"foo\" \n            } \n         } \n     }  }  POST   /test_index/parent_type/_search  { \n    \"query\" :   { \n       \"filtered\" :   { \n          \"query\" :   { \n             \"match_all\" :   {} \n          }, \n          \"filter\" :   { \n             \"has_child\" :   { \n                \"type\" :   \"child_type\" , \n                \"filter\" :   { \n                   \"term\" :   { \n                      \"child_str\" :   \"foo\" \n                   } \n                } \n             } \n          } \n       } \n    }  }",
            "title": "Parent Child Relationships"
        },
        {
            "location": "/Search/ElasticSearch/#aggregates",
            "text": "Aggregations and searches can span multiple indices  # Calculate the most popular interests for all employees  GET /megacorp/employee/_search\n  {\n    \"aggs\": {\n      \"all_interests\": {\n        \"terms\": {\n          \"field\": \"interests\"\n        }\n      }\n    }\n  }  # Calculate the most popular interests for\n# employees named \"Smith\"  GET /megacorp/employee/_search\n  {\n    \"query\": {\n      \"match\": {\n        \"last_name\": \"smith\"\n      }\n    },\n    \"aggs\": {\n      \"all_interests\": {\n        \"terms\": {\n          \"field\": \"interests\"\n        }\n      }\n    }\n  }  # Calculate the average age of employee per interest - hierarchical aggregates  GET /megacorp/employee/_search\n  {\n      \"aggs\" : {\n          \"all_interests\" : {\n              \"terms\" : { \"field\" : \"interests\" },\n              \"aggs\" : {\n                  \"avg_age\" : {\n                      \"avg\" : { \"field\" : \"age\" }\n                  }\n              }\n          }\n      }\n  }  # requires in config/elasticsearch.yml\n# script.inline: true\n# script.indexed: true  GET /tlo/contacts/_search\n  {\n    \"size\" : 0,\n    \"query\": {\n      \"constant_score\": {\n        \"filter\": {\n          \"terms\": {\n            \"version\": [\n              \"20160301\",\n              \"20160401\"\n            ]\n          }\n        }\n      }\n    },\n    \"aggs\": {\n      \"counts\": {\n        \"cardinality\": {\n          \"script\": \"doc['first_name'].value + ' ' + doc['last_name'].value + ' ' + doc['company'].value\",\n          \"missing\": \"N/A\"\n        }\n      }\n    }\n  }",
            "title": "AGGREGATES"
        },
        {
            "location": "/Search/ElasticSearch/#index-management",
            "text": "By default, indices are assigned five primary shards. The number of primary shards can be set only when an index is created and never changed  # Add an index    PUT /blogs { \"settings\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 1 }}\n  PUT /blogs/_settings { \"number_of_replicas\" : 2}   ElasticSearch Shards should be 50 GB or less in size.  Use aliases to shelter the underlying index (or indices) and allow index swapping",
            "title": "INDEX MANAGEMENT"
        },
        {
            "location": "/Search/ElasticSearch/#cluster-management",
            "text": "GET /_cluster/health",
            "title": "CLUSTER MANAGEMENT"
        },
        {
            "location": "/Search/ElasticSearch/#configuration",
            "text": "config directory   yaml file    Sets the JVM heap size to 0.5 memory size. The OS will use it for file system cache   Prefer not to allocate 30GB !! --> uncompressed pointers  Never let the JVM swap    bootstrap.mlockall = true  Keep the JVM defaults  Do not use G1GC alternative garbage collector   cluster.name: <my cluster>   All nodes in the cluster must have the same cluster name   node.name: <my_node_name>  ./bin/elasticsearch --node.name=`hostname`  to override the configuration file   HTTP port: 9200 and successors  Transport : 9300 (internal communications)",
            "title": "CONFIGURATION"
        },
        {
            "location": "/Search/ElasticSearch/#discovery",
            "text": "AWS plugin available   --> also include integration with S3 (snapshot to S3)  AWS: multi-AZ is OK but replication across far data centers is not recommended  See: resiliency   Sites plugins -- kopf / head / paramedic / bigdesk / kibana\n- contain static web content (JS, HTML....)  Install plugins on ALL machines of the cluster  To install,    ./bin/plugin install marvel-agent\n  ./bin/plugin remove marvel-agent  One type per index is recommended, except for parent child / nested indexes.  index size optimization:\n- can disable  _source  and  _all  (the index that captures every field - not needed unless the top search bar changes)\n- by default, Kibana will search  _all  data types:\nstring, number, bool, datetime, binary, array, object, geo_point, geo_shape, ip, multifield\nbinary should be base64 encoded before storage",
            "title": "Discovery"
        },
        {
            "location": "/Search/ElasticSearch/#maintenance",
            "text": "Steps to restore elastic search data:   Stop elastic search  Extract the zip file (dump file)  Start elastic search  Reload elastic search   The commands to do the above are as below:   systemctl stop elasticsearch  extract gz file to destination path  systemctl start elasticsearch  systemctl daemon-reload elasticsearch",
            "title": "MAINTENANCE"
        },
        {
            "location": "/Software_Development/Development_Tools/",
            "text": "Communication / IM\n\u00b6\n\n\n\n\nSlack\n\n\nTrillian / Pandion\n\n\nSkype, WeChat, Viber, Hangouts\n\n\n\n\nWiki / Knowledge Base\n\u00b6\n\n\n\n\nConfluence\n\n\nEvernote\n\n\n\n\nProject / Bug Tracking\n\u00b6\n\n\n\n\nJIRA\n\n\nBugzilla\n\n\nMantis\n\n\nRedMine\n\n\nTFS\n\n\n\n\nEnterprise Architecture / UML\n\u00b6\n\n\n\n\nViolet UML Editor\n\n\nVisio\n\n\nRational Rose \n\n\n\n\nTerminals / SSH\n\u00b6\n\n\n\n\nPutty\n\n\nMobaXterm\n\n\nmRemoteNG\n\n\nRemote Desktop Connection Manager\n\n\n\n\nEditors/ IDEs\n\u00b6\n\n\n\n\n\n\nComparison of integrated development environments\n\n\n\n\n\n\nNotepad++\n\n\n\n\nPlugins\n\n\n\n\n\n\nGedit\n\n\n\n\nSublime Text\n\n\n\n\n\n\nEclipse\n\n\n\n\nList of Eclipse-based software\n\n\n\n\n\n\nVisual Studio\n\n\nVisual Studio Code\n\n\n\n\nfor Python\n\n\n\n\nPython Tools for Visual Studio\n\n\nPyDev\n\n\nPyCharm\n\n\nAnaconda\n\n\n\n\nSource Control\n\u00b6\n\n\n\n\nSourceTree (Atlassian)\n\n\nGitHub\n\n\nTortoiseGit\n\n\n\n\nCode Quality\n\u00b6\n\n\n\n\nSonarQube\n\n\nPhabricator\n \n\n\nCode Coverage\n\n\n\n\nVirtual Machines and Containers\n\u00b6\n\n\n\n\n\n\nOracle Virtualbox\n\n\n\n\n\n\nDocker\n\n\n\n\nKubernetes\n\n\n\n\nSQL Tools\n\u00b6\n\n\n\n\nMySQL Workbench\n\n\nHeidiSQL\n\n\nAginity Workbench for Redshift (AWS)\n\n\n\n\nMongoDB tools\n\u00b6\n\n\n\n\nRoboMongo\n\n\nMongoChef\n\n\nMongoDB Compass\n\n\n\n\nData Quality\n\u00b6\n\n\n\n\nDQ Analyzer (Attacama)\n\n\n\n\nData Science\n\u00b6\n\n\n\n\nJupyter / IPython\n\n\nRodeo\n\n\nGephi\n\n\n\n\nAWS\n\u00b6\n\n\n\n\nS3 Browser\n\n\nFastGlacier\n\n\n\n\nFile Handling\n\u00b6\n\n\n\n\n7zip\n\n\nFileZilla\n\n\nFolderSize\n\n\n\n\nOther\n\u00b6\n\n\n\n\nCygwin\n\n\nLog Tail\n\n\nDiff - WinMerge",
            "title": "Development Tools"
        },
        {
            "location": "/Software_Development/Development_Tools/#communication-im",
            "text": "Slack  Trillian / Pandion  Skype, WeChat, Viber, Hangouts",
            "title": "Communication / IM"
        },
        {
            "location": "/Software_Development/Development_Tools/#wiki-knowledge-base",
            "text": "Confluence  Evernote",
            "title": "Wiki / Knowledge Base"
        },
        {
            "location": "/Software_Development/Development_Tools/#project-bug-tracking",
            "text": "JIRA  Bugzilla  Mantis  RedMine  TFS",
            "title": "Project / Bug Tracking"
        },
        {
            "location": "/Software_Development/Development_Tools/#enterprise-architecture-uml",
            "text": "Violet UML Editor  Visio  Rational Rose",
            "title": "Enterprise Architecture / UML"
        },
        {
            "location": "/Software_Development/Development_Tools/#terminals-ssh",
            "text": "Putty  MobaXterm  mRemoteNG  Remote Desktop Connection Manager",
            "title": "Terminals / SSH"
        },
        {
            "location": "/Software_Development/Development_Tools/#editors-ides",
            "text": "Comparison of integrated development environments    Notepad++   Plugins    Gedit   Sublime Text    Eclipse   List of Eclipse-based software    Visual Studio  Visual Studio Code   for Python   Python Tools for Visual Studio  PyDev  PyCharm  Anaconda",
            "title": "Editors/ IDEs"
        },
        {
            "location": "/Software_Development/Development_Tools/#source-control",
            "text": "SourceTree (Atlassian)  GitHub  TortoiseGit",
            "title": "Source Control"
        },
        {
            "location": "/Software_Development/Development_Tools/#code-quality",
            "text": "SonarQube  Phabricator    Code Coverage",
            "title": "Code Quality"
        },
        {
            "location": "/Software_Development/Development_Tools/#virtual-machines-and-containers",
            "text": "Oracle Virtualbox    Docker   Kubernetes",
            "title": "Virtual Machines and Containers"
        },
        {
            "location": "/Software_Development/Development_Tools/#sql-tools",
            "text": "MySQL Workbench  HeidiSQL  Aginity Workbench for Redshift (AWS)",
            "title": "SQL Tools"
        },
        {
            "location": "/Software_Development/Development_Tools/#mongodb-tools",
            "text": "RoboMongo  MongoChef  MongoDB Compass",
            "title": "MongoDB tools"
        },
        {
            "location": "/Software_Development/Development_Tools/#data-quality",
            "text": "DQ Analyzer (Attacama)",
            "title": "Data Quality"
        },
        {
            "location": "/Software_Development/Development_Tools/#data-science",
            "text": "Jupyter / IPython  Rodeo  Gephi",
            "title": "Data Science"
        },
        {
            "location": "/Software_Development/Development_Tools/#aws",
            "text": "S3 Browser  FastGlacier",
            "title": "AWS"
        },
        {
            "location": "/Software_Development/Development_Tools/#file-handling",
            "text": "7zip  FileZilla  FolderSize",
            "title": "File Handling"
        },
        {
            "location": "/Software_Development/Development_Tools/#other",
            "text": "Cygwin  Log Tail  Diff - WinMerge",
            "title": "Other"
        },
        {
            "location": "/dotNET/ASPdotNET/",
            "text": "Using Yeoman to generate a ASP.NET Core app from a template\n\u00b6\n\n\nInstall Node.js and npm\n\u00b6\n\n\nTo get started with Yeoman, install \nNode.js\n. The installer includes Node.js and \nnpm\n.\n\n\n\n\nfor Mac OS X\n\n\n\n\nbrew install node\n\n\n\n\n\n\n\nfor Windows OS\n\n\n\n\nchoco install nodejs\n\n\n\n\n\nInstall \nYeoman\n and \nBower\n\u00b6\n\n\nnpm install -g yo\n\nnpm install -g bower\n\n\n\n\n\nInstall \ngenerator-aspnet\n\u00b6\n\n\nnpm install -g generator-aspnet\n\n\n\n\n\nRun with \n\n\nyo aspnet\n\n\n\n\n\nSee also: \nBuilding Projects with Yeoman on docs.asp.net\n\n\nOptionaly install the \nyeoman extension\n in Visual Studio Code\n\u00b6\n\n\nArchitecture\n\u00b6\n\n\nOnion Architecture In ASP.NET Core MVC\n\n\nExample of a Web API built on ASP.NET Core\n\n\nRouting Examples\n\u00b6\n\n\npublic\n \nclass\n \nTestController\n \n:\n \nController\n\n\n{\n\n\n\n// /hello\n\n\n[Route(\"/hello\")]\n\n\npublic\n \nIActionResult\n \nHello\n()\n \n=>\n \nOk\n(\n\"Hello\"\n);\n\n\n\n// /hi only GET method\n\n\n[Route(\"/hi\")]\n \n[\nHttpGet\n]\n\n\npublic\n \nIActionResult\n \nHi\n()\n \n=>\n \nOk\n(\n\"Hi\"\n);\n\n\n\n//Alternative for previous\n\n\n[HttpGet(\"/hi\")]\n\n\npublic\n \nIActionResult\n \nHi\n()\n \n=>\n \nOk\n(\n\"Hi\"\n);\n\n\n}\n\n\n\n//Route prefix\n\n\n[Route(\"test\")]\n\n\npublic\n \nclass\n \nTestController2\n \n:\n \nController\n\n\n{\n\n\n//You can have multiple routes on an action\n\n\n[Route(\"\")]\n \n// /test\n\n\n[Route(\"hello\")]\n  \n// /test/hello\n\n\npublic\n \nIActionResult\n \nHello\n()\n \n=>\n \nOk\n(\n\"Hello\"\n);\n\n\n\n// Maps to both: // /test/hi, and: // /hi\n\n\n[Route(\"/hi\")]\n \n// Overrides the prefix with /, you can also use ~/\n\n\n[Route(\"hi\")]\n\n\npublic\n \nIActionResult\n \nHi\n()\n \n=>\n \nOk\n(\n\"Hi\"\n);\n\n\n\n// /test/greet/Joon -> maps Joon to the name parameter\n\n\n[Route(\"greet/{name}\")]\n\n\npublic\n \nIActionResult\n \nGreet\n(\nstring\n \nname\n)\n \n=>\n \nOk\n(\n$\n\"Hello {name}!\"\n);\n\n\n\n//Parameters can be optional\n\n\n// /test/greetopt -> name == null\n\n\n// /test/greetopt/Joon -> name == Joon\n\n\n[Route(\"greetopt/{name?}\")]\n\n\npublic\n \nIActionResult\n \nGreetOptional\n(\nstring\n \nname\n)\n \n=>\n \nOk\n(\nname\n \n==\n \nnull\n \n?\n \n\"No name\"\n \n:\n \n\"Hi!\"\n);\n\n\n}\n\n\n\n// You can use [controller], [action], and [area] to create generic templates\n\n\n[Route(\"[controller]\n/[\naction\n]\n\")]\n\n\npublic\n \nclass\n \nMyController\n \n:\n \nController\n\n\n{\n\n\n// /my/info\n\n\npublic\n \nIActionResult\n \nInfo\n()\n \n=>\n \nOk\n(\n\"Info\"\n);\n\n\n// /my/i\n\n\n[Route(\"/[controller]\n/\ni\n\")]\n\n\npublic\n \nIActionResult\n \nInfo2\n()\n \n=>\n \nOk\n(\n\"Info2\"\n);\n \n}\n\n\n\n[Route(\"users\")]\n\n\npublic\n \nclass\n \nSelectionController\n \n:\n \nController\n\n\n{\n\n\n//You can use constraints to influence route selection\n\n\n//Do not use for validation!\n\n\n// /users/123\n\n\n[Route(\"{id:int}\")]\n\n\npublic\n \nIActionResult\n \nInt\n(\nint\n \nid\n)\n \n=>\n \nOk\n(\n$\n\"Looked up user id {id}\"\n);\n\n\n\n// /users/joonas\n\n\n[Route(\"{name:alpha}\")]\n\n\npublic\n \nIActionResult\n \nString\n(\nstring\n \nname\n)\n \n=>\n \nOk\n(\n$\n\"User name {name}\"\n);\n \n\n}",
            "title": "ASPdotNET"
        },
        {
            "location": "/dotNET/ASPdotNET/#using-yeoman-to-generate-a-aspnet-core-app-from-a-template",
            "text": "",
            "title": "Using Yeoman to generate a ASP.NET Core app from a template"
        },
        {
            "location": "/dotNET/ASPdotNET/#install-nodejs-and-npm",
            "text": "To get started with Yeoman, install  Node.js . The installer includes Node.js and  npm .   for Mac OS X   brew install node   for Windows OS   choco install nodejs",
            "title": "Install Node.js and npm"
        },
        {
            "location": "/dotNET/ASPdotNET/#install-yeoman-and-bower",
            "text": "npm install -g yo\n\nnpm install -g bower",
            "title": "Install Yeoman and Bower"
        },
        {
            "location": "/dotNET/ASPdotNET/#install-generator-aspnet",
            "text": "npm install -g generator-aspnet  Run with   yo aspnet  See also:  Building Projects with Yeoman on docs.asp.net",
            "title": "Install generator-aspnet"
        },
        {
            "location": "/dotNET/ASPdotNET/#optionaly-install-the-yeoman-extension-in-visual-studio-code",
            "text": "",
            "title": "Optionaly install the yeoman extension in Visual Studio Code"
        },
        {
            "location": "/dotNET/ASPdotNET/#architecture",
            "text": "Onion Architecture In ASP.NET Core MVC  Example of a Web API built on ASP.NET Core",
            "title": "Architecture"
        },
        {
            "location": "/dotNET/ASPdotNET/#routing-examples",
            "text": "public   class   TestController   :   Controller  {  // /hello  [Route(\"/hello\")]  public   IActionResult   Hello ()   =>   Ok ( \"Hello\" );  // /hi only GET method  [Route(\"/hi\")]   [ HttpGet ]  public   IActionResult   Hi ()   =>   Ok ( \"Hi\" );  //Alternative for previous  [HttpGet(\"/hi\")]  public   IActionResult   Hi ()   =>   Ok ( \"Hi\" );  }  //Route prefix  [Route(\"test\")]  public   class   TestController2   :   Controller  {  //You can have multiple routes on an action  [Route(\"\")]   // /test  [Route(\"hello\")]    // /test/hello  public   IActionResult   Hello ()   =>   Ok ( \"Hello\" );  // Maps to both: // /test/hi, and: // /hi  [Route(\"/hi\")]   // Overrides the prefix with /, you can also use ~/  [Route(\"hi\")]  public   IActionResult   Hi ()   =>   Ok ( \"Hi\" );  // /test/greet/Joon -> maps Joon to the name parameter  [Route(\"greet/{name}\")]  public   IActionResult   Greet ( string   name )   =>   Ok ( $ \"Hello {name}!\" );  //Parameters can be optional  // /test/greetopt -> name == null  // /test/greetopt/Joon -> name == Joon  [Route(\"greetopt/{name?}\")]  public   IActionResult   GreetOptional ( string   name )   =>   Ok ( name   ==   null   ?   \"No name\"   :   \"Hi!\" );  }  // You can use [controller], [action], and [area] to create generic templates  [Route(\"[controller] /[ action ] \")]  public   class   MyController   :   Controller  {  // /my/info  public   IActionResult   Info ()   =>   Ok ( \"Info\" );  // /my/i  [Route(\"/[controller] / i \")]  public   IActionResult   Info2 ()   =>   Ok ( \"Info2\" );   }  [Route(\"users\")]  public   class   SelectionController   :   Controller  {  //You can use constraints to influence route selection  //Do not use for validation!  // /users/123  [Route(\"{id:int}\")]  public   IActionResult   Int ( int   id )   =>   Ok ( $ \"Looked up user id {id}\" );  // /users/joonas  [Route(\"{name:alpha}\")]  public   IActionResult   String ( string   name )   =>   Ok ( $ \"User name {name}\" );   }",
            "title": "Routing Examples"
        },
        {
            "location": "/dotNET/AkkadotNET/",
            "text": "Akka.NET Examples\n\u00b6\n\n\n// To install Akka.NET Distributed Actor Framework, run the following command in the Package Manager Console\n\n\n// PM> Install-Package Akka\n\n\n// PM> Install-Package Akka.Remote\n\n\n// Installing with Topshelf is as easy as calling \n.exe install on the command line.\n\n\nusing\n \nSystem\n;\n\n\nusing\n \nSystem.Collections.Generic\n;\n\n\nusing\n \nSystem.Linq\n;\n\n\nusing\n \nSystem.Text\n;\n\n\nusing\n \nSystem.Threading.Tasks\n;\n\n\n\n// Add these two lines\n\n\nusing\n \nAkka\n;\n\n\nusing\n \nAkka.Actor\n;\n\n\nusing\n \nTopshelf\n;\n   \n// http://topshelf.readthedocs.io/en/latest/configuration/quickstart.html\n\n\n\ninternal\n \nclass\n \nProgram\n\n\n{\n\n    \nprivate\n \nstatic\n \nvoid\n \nMain\n(\nstring\n[]\n \nargs\n)\n\n    \n{\n\n        \n// \u2018x\u2019 exposes all of the host level configuration\n\n        \nHostFactory\n.\nRun\n(\nx\n \n=>\n\n        \n{\n\n            \nx\n.\nService\n<\nActorService\n>(\ns\n \n=>\n     \n//  telling Topshelf that there is a service of type ActorService. service configuration options exposed through the \u2018s\u2019 parameter.\n\n            \n{\n\n                \ns\n.\nConstructUsing\n(\nname\n \n=>\n \nnew\n \nActorService\n());\n \n// build an instance of the service; new or pull from IoC container\n\n                \ns\n.\nWhenStarted\n(\nservice\n \n=>\n \nservice\n.\nStart\n());\n\n                \ns\n.\nWhenStopped\n(\nservice\n \n=>\n \nservice\n.\nStop\n());\n\n                \n////continue and restart directives are also available\n\n                \n//s.WhenPaused(service => service.Pause());\n\n                \n//s.WhenContinued(service => service.Continue());\n\n                \n//s.WhenShutdown(service => service.Shutdown());\n\n\n            \n});\n\n\n            \nx\n.\nRunAsLocalSystem\n();\n \n// service \u2018run as\u2019 the \u2018local system\u2019. Alternatively x.RunAsLocalSystem(); x.RunAs(\"username\", \"password\");  x.RunAsPrompt();\n\n            \nx\n.\nUseAssemblyInfoForServiceInfo\n();\n\n\n            \n//x.SetDescription(\"Orchestrator Host\"); \n\n            \n//x.SetDisplayName(\"Orchestrator\"); // display name for the winservice in the windows service monitor\n\n            \n//x.SetServiceName(\"Orchestrator\"); // service name for the winservice in the windows service monitor\n\n            \n//x.SetInstanceName(\"MyService\"); // instance name of the service, which is combined with the base service name and separated by a $.\n\n        \n});\n\n    \n}\n\n\n}\n\n\n\n// <summary>\n\n\n/// This class acts as an interface between the application and TopShelf\n\n\n/// </summary>\n\n\npublic\n \nclass\n \nActorService\n\n\n{\n\n    \nprivate\n \nActorSystem\n \nsystem\n;\n\n\n    \npublic\n \nvoid\n \nStart\n()\n\n    \n{\n\n        \n// Create a new actor system (a container for actors)\n\n        \nthis\n.\nsystem\n \n=\n \nActorSystem\n.\nCreate\n(\n\"MainSystem\"\n);\n\n    \n}\n\n\n    \npublic\n \nasync\n \nvoid\n \nStop\n()\n\n    \n{\n\n        \n//this is where you stop your actor system\n\n        \nawait\n \nthis\n.\nsystem\n.\nTerminate\n();\n\n    \n}\n\n\n    \nprivate\n \nvoid\n \nCreate\n()\n\n    \n{\n\n        \n// Create your actor and get a reference to it.\n\n        \n// This will be an \"ActorRef\", which is not a\n\n        \n// reference to the actual actor instance\n\n        \n// but rather a client or proxy to it.\n\n        \nvar\n \njob\n \n=\n \nsystem\n.\nActorOf\n<\nJobActor\n>(\n\"Job\"\n);\n\n        \n//or: var myActor = system.ActorOf(Props.Create<JobActor>());\n\n\n        \n// Send a message to the actor\n\n        \njob\n.\nTell\n(\nnew\n \nMessage\n<\nstring\n>(\n\"Hello World\"\n));\n\n    \n}\n\n\n}\n\n\n\n// Example immutable message class - C# 7.0\n\n\npublic\n \nclass\n \nMessage\n<\nT\n>\n\n\n{\n\n    \npublic\n \nMessage\n(\nT\n \ndata\n)\n\n    \n{\n\n        \nthis\n.\nData\n \n=\n \ndata\n;\n\n    \n}\n\n\n    \npublic\n \nT\n \nData\n \n{\n \nget\n;\n \n}\n\n\n    \n// Allow convesion to a tuple \n\n    \npublic\n \nvoid\n \nDeconstruct\n(\nout\n \nT\n \ndata\n)\n \n{\n \ndata\n \n=\n \nthis\n.\nData\n;\n \n}\n\n\n}\n\n\n\n// Example of ReceiveActor\n\n\npublic\n \nclass\n \nJobActor\n:\n \nReceiveActor\n\n\n{\n\n  \nprivate\n \nreadonly\n \nILoggingAdapter\n \nlog\n \n=\n \nContext\n.\nGetLogger\n();\n\n\n  \npublic\n \nJobActor\n()\n\n  \n{\n\n    \nReceive\n<\nMessage\n<\nstring\n>>(\nmessage\n \n=>\n \n{\n\n      \nlog\n.\nInfo\n(\n\"Received String message: {0}\"\n,\n \nmessage\n.\nData\n);\n\n      \n// Console.WriteLine(message.Data);\n\n\n      \n// reply back\n\n      \nSender\n.\nTell\n(\nmessage\n);\n\n    \n});\n\n  \n}\n\n\n}\n\n\n\n\n\n// Example of untyped actor\n\n\n\npublic\n \nclass\n \nMyActor\n \n:\n \nUntypedActor\n\n\n{\n\n    \nprivate\n \nActorRef\n \nlogger\n \n=\n \nContext\n.\nActorOf\n<\nLogActor\n>();\n\n\n    \n// if any child, e.g. the logger above. throws an exception\n\n    \n// apply the rules below\n\n    \n// e.g. Restart the child, if 10 exceptions occur in 30 seconds or\n\n    \n// less, then stop the actor\n\n    \nprotected\n \noverride\n \nSupervisorStrategy\n \nSupervisorStrategy\n()\n\n    \n{\n\n        \nreturn\n \nnew\n \nOneForOneStrategy\n(\n \n//or AllForOneStrategy\n\n            \nmaxNumberOfRetries\n:\n \n10\n,\n\n            \nduration\n:\n \nTimeSpan\n.\nFromSeconds\n(\n30\n),\n\n            \ndecider\n:\n \nDecider\n.\nFrom\n(\nx\n \n=>\n\n            \n{\n\n                \n//Maybe we consider ArithmeticException to not be application critical\n\n                \n//so we just ignore the error and keep going.\n\n                \nif\n \n(\nx\n \nis\n \nArithmeticException\n)\n \nreturn\n \nDirective\n.\nResume\n;\n\n                \n//Error that we cannot recover from, stop the failing actor\n\n                \nelse\n \nif\n \n(\nx\n \nis\n \nNotSupportedException\n)\n \nreturn\n \nDirective\n.\nStop\n;\n\n                \n//In all other cases, just restart the failing actor\n\n                \nelse\n \nreturn\n \nDirective\n.\nRestart\n;\n\n             \n}));\n\n    \n}\n\n\n}\n\n\n\n// Example of long-running operation in an Actor - PipeTo / Become / Stash\n\n\n\n/*\n\n\nIf you stick a long-running operation inside your Receive method then your actors will be unable to process any messages, including system messages, until that operation finishes. And if it\u2019s possible that the operation will never finish, it\u2019s possible to deadlock your actor.\n\n\nThe solution to this is simple: you need to encapsulate any long-running I/O-bound or CPU-bound operations inside a Task and make it possible to cancel that task from within the actor.\n\n\nHere\u2019s an example of how you can use behavior switching, stashing, and control messages to do this.\n\n\n\nhttps://petabridge.com/blog/akka-actors-finite-state-machines-switchable-behavior/\n\n\n\n*/\n\n\npublic\n \nclass\n \nFooActor\n \n:\n \nReceiveActor\n,\n \nIWithUnboundedStash\n\n\n{\n\n\n    \nprivate\n \nTask\n \n_runningTask\n;\n\n    \nprivate\n \nCancellationTokenSource\n \n_cancel\n;\n\n\n    \npublic\n \nIStash\n \nStash\n \n{\nget\n;\n \nset\n;}\n\n\n    \npublic\n \nFooActor\n(){\n\n        \n_cancel\n \n=\n \nnew\n \nCancellationTokenSource\n();\n\n        \nReady\n();\n\n    \n}\n\n\n    \nprivate\n \nvoid\n \nReady\n(){\n\n        \nReceive\n<\nStart\n>(\ns\n \n=>\n \n{\n\n            \nvar\n \nself\n \n=\n \nSelf\n;\n \n// closure\n\n            \n_runningTask\n \n=\n \nTask\n.\nRun\n(()\n \n=>\n \n{\n\n                \n// ... work\n\n            \n},\n \n_cancel\n.\nToken\n).\nContinueWith\n(\nx\n \n=>\n\n            \n{\n\n                \nif\n(\nx\n.\nIsCancelled\n \n||\n \nx\n.\nIsFaulted\n)\n\n                    \nreturn\n \nnew\n \nFailed\n();\n\n                \nreturn\n \nnew\n \nFinished\n();\n\n            \n},\n \nTaskContinuationOptions\n.\nExecuteSynchronously\n)\n\n            \n.\nPipeTo\n(\nself\n);\n\n\n            \n// switch behavior\n\n            \nBecome\n(\nWorking\n);\n\n        \n})\n\n    \n}\n\n\n    \nprivate\n \nvoid\n \nWorking\n(){\n\n        \nReceive\n<\nCancel\n>(\ncancel\n \n=>\n \n{\n\n            \n_cancel\n.\nCancel\n();\n \n// cancel work\n\n            \nBecomeReady\n();\n\n        \n});\n\n        \nReceive\n<\nFailed\n>(\nf\n \n=>\n \nBecomeReady\n());\n\n        \nReceive\n<\nFinished\n>(\nf\n \n=>\n \nBecomeReady\n());\n\n        \nReceiveAny\n(\no\n \n=>\n \nStash\n.\nStash\n());\n\n    \n}\n\n\n    \nprivate\n \nvoid\n \nBecomeReady\n(){\n\n        \n_cancel\n \n=\n \nnew\n \nCancellationTokenSource\n();\n\n        \nStash\n.\nUnstashAll\n();\n\n        \nBecome\n(\nReady\n);\n\n    \n}\n\n\n}",
            "title": "AkkadotNET"
        },
        {
            "location": "/dotNET/AkkadotNET/#akkanet-examples",
            "text": "// To install Akka.NET Distributed Actor Framework, run the following command in the Package Manager Console  // PM> Install-Package Akka  // PM> Install-Package Akka.Remote  // Installing with Topshelf is as easy as calling  .exe install on the command line.  using   System ;  using   System.Collections.Generic ;  using   System.Linq ;  using   System.Text ;  using   System.Threading.Tasks ;  // Add these two lines  using   Akka ;  using   Akka.Actor ;  using   Topshelf ;     // http://topshelf.readthedocs.io/en/latest/configuration/quickstart.html  internal   class   Program  { \n     private   static   void   Main ( string []   args ) \n     { \n         // \u2018x\u2019 exposes all of the host level configuration \n         HostFactory . Run ( x   => \n         { \n             x . Service < ActorService >( s   =>       //  telling Topshelf that there is a service of type ActorService. service configuration options exposed through the \u2018s\u2019 parameter. \n             { \n                 s . ConstructUsing ( name   =>   new   ActorService ());   // build an instance of the service; new or pull from IoC container \n                 s . WhenStarted ( service   =>   service . Start ()); \n                 s . WhenStopped ( service   =>   service . Stop ()); \n                 ////continue and restart directives are also available \n                 //s.WhenPaused(service => service.Pause()); \n                 //s.WhenContinued(service => service.Continue()); \n                 //s.WhenShutdown(service => service.Shutdown()); \n\n             }); \n\n             x . RunAsLocalSystem ();   // service \u2018run as\u2019 the \u2018local system\u2019. Alternatively x.RunAsLocalSystem(); x.RunAs(\"username\", \"password\");  x.RunAsPrompt(); \n             x . UseAssemblyInfoForServiceInfo (); \n\n             //x.SetDescription(\"Orchestrator Host\");  \n             //x.SetDisplayName(\"Orchestrator\"); // display name for the winservice in the windows service monitor \n             //x.SetServiceName(\"Orchestrator\"); // service name for the winservice in the windows service monitor \n             //x.SetInstanceName(\"MyService\"); // instance name of the service, which is combined with the base service name and separated by a $. \n         }); \n     }  }  // <summary>  /// This class acts as an interface between the application and TopShelf  /// </summary>  public   class   ActorService  { \n     private   ActorSystem   system ; \n\n     public   void   Start () \n     { \n         // Create a new actor system (a container for actors) \n         this . system   =   ActorSystem . Create ( \"MainSystem\" ); \n     } \n\n     public   async   void   Stop () \n     { \n         //this is where you stop your actor system \n         await   this . system . Terminate (); \n     } \n\n     private   void   Create () \n     { \n         // Create your actor and get a reference to it. \n         // This will be an \"ActorRef\", which is not a \n         // reference to the actual actor instance \n         // but rather a client or proxy to it. \n         var   job   =   system . ActorOf < JobActor >( \"Job\" ); \n         //or: var myActor = system.ActorOf(Props.Create<JobActor>()); \n\n         // Send a message to the actor \n         job . Tell ( new   Message < string >( \"Hello World\" )); \n     }  }  // Example immutable message class - C# 7.0  public   class   Message < T >  { \n     public   Message ( T   data ) \n     { \n         this . Data   =   data ; \n     } \n\n     public   T   Data   {   get ;   } \n\n     // Allow convesion to a tuple  \n     public   void   Deconstruct ( out   T   data )   {   data   =   this . Data ;   }  }  // Example of ReceiveActor  public   class   JobActor :   ReceiveActor  { \n   private   readonly   ILoggingAdapter   log   =   Context . GetLogger (); \n\n   public   JobActor () \n   { \n     Receive < Message < string >>( message   =>   { \n       log . Info ( \"Received String message: {0}\" ,   message . Data ); \n       // Console.WriteLine(message.Data); \n\n       // reply back \n       Sender . Tell ( message ); \n     }); \n   }  }  // Example of untyped actor  public   class   MyActor   :   UntypedActor  { \n     private   ActorRef   logger   =   Context . ActorOf < LogActor >(); \n\n     // if any child, e.g. the logger above. throws an exception \n     // apply the rules below \n     // e.g. Restart the child, if 10 exceptions occur in 30 seconds or \n     // less, then stop the actor \n     protected   override   SupervisorStrategy   SupervisorStrategy () \n     { \n         return   new   OneForOneStrategy (   //or AllForOneStrategy \n             maxNumberOfRetries :   10 , \n             duration :   TimeSpan . FromSeconds ( 30 ), \n             decider :   Decider . From ( x   => \n             { \n                 //Maybe we consider ArithmeticException to not be application critical \n                 //so we just ignore the error and keep going. \n                 if   ( x   is   ArithmeticException )   return   Directive . Resume ; \n                 //Error that we cannot recover from, stop the failing actor \n                 else   if   ( x   is   NotSupportedException )   return   Directive . Stop ; \n                 //In all other cases, just restart the failing actor \n                 else   return   Directive . Restart ; \n              })); \n     }  }  // Example of long-running operation in an Actor - PipeTo / Become / Stash  /*  If you stick a long-running operation inside your Receive method then your actors will be unable to process any messages, including system messages, until that operation finishes. And if it\u2019s possible that the operation will never finish, it\u2019s possible to deadlock your actor.  The solution to this is simple: you need to encapsulate any long-running I/O-bound or CPU-bound operations inside a Task and make it possible to cancel that task from within the actor.  Here\u2019s an example of how you can use behavior switching, stashing, and control messages to do this.  https://petabridge.com/blog/akka-actors-finite-state-machines-switchable-behavior/  */  public   class   FooActor   :   ReceiveActor ,   IWithUnboundedStash  { \n\n     private   Task   _runningTask ; \n     private   CancellationTokenSource   _cancel ; \n\n     public   IStash   Stash   { get ;   set ;} \n\n     public   FooActor (){ \n         _cancel   =   new   CancellationTokenSource (); \n         Ready (); \n     } \n\n     private   void   Ready (){ \n         Receive < Start >( s   =>   { \n             var   self   =   Self ;   // closure \n             _runningTask   =   Task . Run (()   =>   { \n                 // ... work \n             },   _cancel . Token ). ContinueWith ( x   => \n             { \n                 if ( x . IsCancelled   ||   x . IsFaulted ) \n                     return   new   Failed (); \n                 return   new   Finished (); \n             },   TaskContinuationOptions . ExecuteSynchronously ) \n             . PipeTo ( self ); \n\n             // switch behavior \n             Become ( Working ); \n         }) \n     } \n\n     private   void   Working (){ \n         Receive < Cancel >( cancel   =>   { \n             _cancel . Cancel ();   // cancel work \n             BecomeReady (); \n         }); \n         Receive < Failed >( f   =>   BecomeReady ()); \n         Receive < Finished >( f   =>   BecomeReady ()); \n         ReceiveAny ( o   =>   Stash . Stash ()); \n     } \n\n     private   void   BecomeReady (){ \n         _cancel   =   new   CancellationTokenSource (); \n         Stash . UnstashAll (); \n         Become ( Ready ); \n     }  }",
            "title": "Akka.NET Examples"
        },
        {
            "location": "/dotNET/C#/",
            "text": "C# Cheatsheets\n\u00b6\n\n\nQuick Reference\n\n\nCheatsheet\n\n\nC# 6.0 / 7.0 - what is new\n\u00b6\n\n\nReadonly properties\n\u00b6\n\n\npublic\n \nstring\n \nFirstName\n \n{\n \nget\n;\n \nprivate\n \nset\n;\n \n}\n  \n// private set is accessible from the entire class\n\n\n\npublic\n \nstring\n \nLastName\n \n{\n \nget\n;\n \n}\n \n// accessible only in constructor\n\n\n\npublic\n \nICollection\n<\ndouble\n>\n \nGrades\n \n{\n \nget\n;\n \n}\n \n=\n \nnew\n \nList\n<\ndouble\n>();\n \n// property initializer\n\n\n\n\n\n\nExpression-bodied function members\n\u00b6\n\n\npublic\n \noverride\n \nstring\n \nToString\n()\n \n=>\n \n\"Hi!\"\n;\n\n\n\n\n\n\nUsing static\n\u00b6\n\n\nusing\n \nstatic\n \nSystem\n.\nString\n;\n\n\n// also common: \n\n\n// using static System.Math;\n\n\n// using static System.Linq.Enumerable;\n\n\n\nif\n \n(\nIsNullOrWhiteSpace\n(\nlastName\n))\n\n  \nthrow\n \nnew\n \nArgumentException\n(\nmessage\n:\n \n\"Cannot be blank\"\n,\n \nparamName\n:\n \nnameof\n(\nlastName\n));\n\n\n\n\n\n\nNull checking\n\u00b6\n\n\nvar\n \nfirst\n \n=\n \nperson\n?.\nFirstName\n;\n\n\nfirst\n \n=\n \nperson\n?.\nFirstName\n \n??\n \n\"Unspecified\"\n;\n\n\n\n// preferred event handing in C# 6:\n\n\nthis\n.\nSomethingHappened\n?.\nInvoke\n(\nthis\n,\n \neventArgs\n);\n\n\n\n\n\n\nString interpolation\n\u00b6\n\n\npublic\n \nstring\n \nGetFormattedGradePoint\n()\n \n=>\n \n$\n\"Name: {LastName}, {FirstName}. G.P.A: {Grades.Average():F2}\"\n;\n\n\n\n\n\n\nException Filters\n\u00b6\n\n\npublic\n \nstatic\n \nasync\n \nTask\n<\nstring\n>\n \nMakeRequest\n()\n\n\n{\n\n  \nvar\n \nclient\n \n=\n \nnew\n \nSystem\n.\nNet\n.\nHttp\n.\nHttpClient\n();\n\n  \nvar\n \nstreamTask\n \n=\n \nclient\n.\nGetStringAsync\n(\n\"https://localHost:10000\"\n);\n\n  \ntry\n \n  \n{\n\n    \nvar\n \nresponseText\n \n=\n \nawait\n \nstreamTask\n;\n\n    \nreturn\n \nresponseText\n;\n\n  \n}\n \n  \ncatch\n \n(\nSystem\n.\nNet\n.\nHttp\n.\nHttpRequestException\n \ne\n)\n \nwhen\n \n(\ne\n.\nMessage\n.\nContains\n(\n\"301\"\n))\n\n  \n{\n\n  \nreturn\n \n\"Site Moved\"\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\nList and dict initializers\n\u00b6\n\n\nprivate\n \nList\n<\nstring\n>\n \nmessages\n \n=\n \nnew\n \nList\n<\nstring\n>\n\n\n{\n\n  \n\"Page not Found\"\n,\n\n  \n\"Page moved, but left a forwarding address.\"\n,\n\n  \n\"The web server can't come out to play today.\"\n\n\n};\n\n\n\nprivate\n \nDictionary\n<\nint\n,\n \nstring\n>\n \nwebErrors\n \n=\n \nnew\n \nDictionary\n<\nint\n,\n \nstring\n>\n\n\n{\n\n\n  [404]\n \n=\n \n\"Page not Found\"\n,\n\n\n  [302]\n \n=\n \n\"Page moved, but left a forwarding address.\"\n,\n\n\n  [500]\n \n=\n \n\"The web server can't come out to play today.\"\n\n\n};\n\n\n\n\n\n\nOut variables\n\u00b6\n\n\nif\n \n(\nint\n.\nTryParse\n(\ninput\n,\n \nout\n \nint\n \nresult\n))\n\n    \nWriteLine\n(\nresult\n);\n\n\nelse\n\n    \nWriteLine\n(\n\"Could not parse input\"\n);\n\n\n\n\n\n\nTuples\n\u00b6\n\n\nvar\n \nletters\n \n=\n \n(\n\"a\"\n,\n \n\"b\"\n);\n\n\n\n(\nstring\n \nAlpha\n,\n \nstring\n \nBeta\n)\n \nnamedLetters\n \n=\n \n(\n\"a\"\n,\n \n\"b\"\n);\n\n\nvar\n \nalphabetStart\n \n=\n \n(\nAlpha\n:\n \n\"a\"\n,\n \nBeta\n:\n \n\"b\"\n);\n\n\n\npublic\n \nclass\n \nPoint\n\n\n{\n\n  \npublic\n \nPoint\n(\ndouble\n \nx\n,\n \ndouble\n \ny\n)\n\n  \n{\n\n  \nthis\n.\nX\n \n=\n \nx\n;\n\n  \nthis\n.\nY\n \n=\n \ny\n;\n\n  \n}\n\n\n  \npublic\n \ndouble\n \nX\n \n{\n \nget\n;\n \n}\n\n  \npublic\n \ndouble\n \nY\n \n{\n \nget\n;\n \n}\n\n\n  \n// Deconstruct method\n\n  \npublic\n \nvoid\n \nDeconstruct\n(\nout\n \ndouble\n \nx\n,\n \nout\n \ndouble\n \ny\n)\n\n  \n{\n\n  \nx\n \n=\n \nthis\n.\nX\n;\n\n  \ny\n \n=\n \nthis\n.\nY\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\nRef return values\n\u00b6\n\n\npublic\n \nstatic\n \nref\n \nint\n \nFind3\n(\nint\n[,]\n \nmatrix\n,\n \nFunc\n<\nint\n,\n \nbool\n>\n \npredicate\n)\n\n\n{\n\n  \nfor\n \n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n<\n \nmatrix\n.\nGetLength\n(\n0\n);\n \ni\n++)\n\n             \nfor\n \n(\nint\n \nj\n \n=\n \n0\n;\n \nj\n \n<\n \nmatrix\n.\nGetLength\n(\n1\n);\n \nj\n++)\n\n                      \nif\n \n(\npredicate\n(\nmatrix\n[\ni\n,\n \nj\n]))\n\n                          \nreturn\n \nref\n \nmatrix\n[\ni\n,\n \nj\n];\n\n  \nthrow\n \nnew\n \nInvalidOperationException\n(\n\"Not found\"\n);\n\n\n}\n\n\nref\n \nvar\n \nitem\n \n=\n \nref\n \nMatrixSearch\n.\nFind3\n(\nmatrix\n,\n \n(\nval\n)\n \n=>\n \nval\n \n==\n \n42\n);\n\n\nConsole\n.\nWriteLine\n(\nitem\n);\n\n\nitem\n \n=\n \n24\n;\n\n\nConsole\n.\nWriteLine\n(\nmatrix\n[\n4\n,\n \n2\n]);\n\n\n\n\n\n\nLocal functions\n\u00b6\n\n\npublic\n \nstatic\n \nIEnumerable\n<\nchar\n>\n \nAlphabetSubset3\n(\nchar\n \nstart\n,\n \nchar\n \nend\n)\n\n\n{\n\n    \nif\n \n((\nstart\n \n<\n \n'a'\n)\n \n||\n \n(\nstart\n \n>\n \n'z'\n))\n\n        \nthrow\n \nnew\n \nArgumentOutOfRangeException\n(\nparamName\n:\n \nnameof\n(\nstart\n),\n \nmessage\n:\n \n\"start must be a letter\"\n);\n\n    \nif\n \n((\nend\n \n<\n \n'a'\n)\n \n||\n \n(\nend\n \n>\n \n'z'\n))\n\n        \nthrow\n \nnew\n \nArgumentOutOfRangeException\n(\nparamName\n:\n \nnameof\n(\nend\n),\n \nmessage\n:\n \n\"end must be a letter\"\n);\n\n    \nif\n \n(\nend\n \n<=\n \nstart\n)\n\n        \nthrow\n \nnew\n \nArgumentException\n(\n$\n\"{nameof(end)} must be greater than {nameof(start)}\"\n);\n\n    \nreturn\n \nalphabetSubsetImplementation\n();\n\n\n    \nIEnumerable\n<\nchar\n>\n \nalphabetSubsetImplementation\n()\n\n    \n{\n\n        \nfor\n \n(\nvar\n \nc\n \n=\n \nstart\n;\n \nc\n \n<\n \nend\n;\n \nc\n++)\n\n            \nyield\n \nreturn\n \nc\n;\n\n    \n}\n\n\n}",
            "title": "C#"
        },
        {
            "location": "/dotNET/C#/#c-cheatsheets",
            "text": "Quick Reference  Cheatsheet",
            "title": "C# Cheatsheets"
        },
        {
            "location": "/dotNET/C#/#c-60-70-what-is-new",
            "text": "",
            "title": "C# 6.0 / 7.0 - what is new"
        },
        {
            "location": "/dotNET/C#/#readonly-properties",
            "text": "public   string   FirstName   {   get ;   private   set ;   }    // private set is accessible from the entire class  public   string   LastName   {   get ;   }   // accessible only in constructor  public   ICollection < double >   Grades   {   get ;   }   =   new   List < double >();   // property initializer",
            "title": "Readonly properties"
        },
        {
            "location": "/dotNET/C#/#expression-bodied-function-members",
            "text": "public   override   string   ToString ()   =>   \"Hi!\" ;",
            "title": "Expression-bodied function members"
        },
        {
            "location": "/dotNET/C#/#using-static",
            "text": "using   static   System . String ;  // also common:   // using static System.Math;  // using static System.Linq.Enumerable;  if   ( IsNullOrWhiteSpace ( lastName )) \n   throw   new   ArgumentException ( message :   \"Cannot be blank\" ,   paramName :   nameof ( lastName ));",
            "title": "Using static"
        },
        {
            "location": "/dotNET/C#/#null-checking",
            "text": "var   first   =   person ?. FirstName ;  first   =   person ?. FirstName   ??   \"Unspecified\" ;  // preferred event handing in C# 6:  this . SomethingHappened ?. Invoke ( this ,   eventArgs );",
            "title": "Null checking"
        },
        {
            "location": "/dotNET/C#/#string-interpolation",
            "text": "public   string   GetFormattedGradePoint ()   =>   $ \"Name: {LastName}, {FirstName}. G.P.A: {Grades.Average():F2}\" ;",
            "title": "String interpolation"
        },
        {
            "location": "/dotNET/C#/#exception-filters",
            "text": "public   static   async   Task < string >   MakeRequest ()  { \n   var   client   =   new   System . Net . Http . HttpClient (); \n   var   streamTask   =   client . GetStringAsync ( \"https://localHost:10000\" ); \n   try  \n   { \n     var   responseText   =   await   streamTask ; \n     return   responseText ; \n   }  \n   catch   ( System . Net . Http . HttpRequestException   e )   when   ( e . Message . Contains ( \"301\" )) \n   { \n   return   \"Site Moved\" ; \n   }  }",
            "title": "Exception Filters"
        },
        {
            "location": "/dotNET/C#/#list-and-dict-initializers",
            "text": "private   List < string >   messages   =   new   List < string >  { \n   \"Page not Found\" , \n   \"Page moved, but left a forwarding address.\" , \n   \"The web server can't come out to play today.\"  };  private   Dictionary < int ,   string >   webErrors   =   new   Dictionary < int ,   string >  {    [404]   =   \"Page not Found\" ,    [302]   =   \"Page moved, but left a forwarding address.\" ,    [500]   =   \"The web server can't come out to play today.\"  };",
            "title": "List and dict initializers"
        },
        {
            "location": "/dotNET/C#/#out-variables",
            "text": "if   ( int . TryParse ( input ,   out   int   result )) \n     WriteLine ( result );  else \n     WriteLine ( \"Could not parse input\" );",
            "title": "Out variables"
        },
        {
            "location": "/dotNET/C#/#tuples",
            "text": "var   letters   =   ( \"a\" ,   \"b\" );  ( string   Alpha ,   string   Beta )   namedLetters   =   ( \"a\" ,   \"b\" );  var   alphabetStart   =   ( Alpha :   \"a\" ,   Beta :   \"b\" );  public   class   Point  { \n   public   Point ( double   x ,   double   y ) \n   { \n   this . X   =   x ; \n   this . Y   =   y ; \n   } \n\n   public   double   X   {   get ;   } \n   public   double   Y   {   get ;   } \n\n   // Deconstruct method \n   public   void   Deconstruct ( out   double   x ,   out   double   y ) \n   { \n   x   =   this . X ; \n   y   =   this . Y ; \n   }  }",
            "title": "Tuples"
        },
        {
            "location": "/dotNET/C#/#ref-return-values",
            "text": "public   static   ref   int   Find3 ( int [,]   matrix ,   Func < int ,   bool >   predicate )  { \n   for   ( int   i   =   0 ;   i   <   matrix . GetLength ( 0 );   i ++) \n              for   ( int   j   =   0 ;   j   <   matrix . GetLength ( 1 );   j ++) \n                       if   ( predicate ( matrix [ i ,   j ])) \n                           return   ref   matrix [ i ,   j ]; \n   throw   new   InvalidOperationException ( \"Not found\" );  }  ref   var   item   =   ref   MatrixSearch . Find3 ( matrix ,   ( val )   =>   val   ==   42 );  Console . WriteLine ( item );  item   =   24 ;  Console . WriteLine ( matrix [ 4 ,   2 ]);",
            "title": "Ref return values"
        },
        {
            "location": "/dotNET/C#/#local-functions",
            "text": "public   static   IEnumerable < char >   AlphabetSubset3 ( char   start ,   char   end )  { \n     if   (( start   <   'a' )   ||   ( start   >   'z' )) \n         throw   new   ArgumentOutOfRangeException ( paramName :   nameof ( start ),   message :   \"start must be a letter\" ); \n     if   (( end   <   'a' )   ||   ( end   >   'z' )) \n         throw   new   ArgumentOutOfRangeException ( paramName :   nameof ( end ),   message :   \"end must be a letter\" ); \n     if   ( end   <=   start ) \n         throw   new   ArgumentException ( $ \"{nameof(end)} must be greater than {nameof(start)}\" ); \n     return   alphabetSubsetImplementation (); \n\n     IEnumerable < char >   alphabetSubsetImplementation () \n     { \n         for   ( var   c   =   start ;   c   <   end ;   c ++) \n             yield   return   c ; \n     }  }",
            "title": "Local functions"
        },
        {
            "location": "/dotNET/Multithreading/",
            "text": "Advanced .NET Threading\n\u00b6\n\n\nAdvanced .NET Threading, Part 1: Thread Fundamentals\n\n\nAdvanced .NET Threading, Part 2: Compute-Bound Async Operations\n\n\nAdvanced .NET Threading, Part 3: I/O-Bound Async Operations\n\n\nAdvanced .NET Threading, Part 4: Thread Synchronization Primitives\n\n\nAdvanced .NET Threading, Part 5: Thread Synchronization Locks",
            "title": "Multithreading"
        },
        {
            "location": "/dotNET/Multithreading/#advanced-net-threading",
            "text": "Advanced .NET Threading, Part 1: Thread Fundamentals  Advanced .NET Threading, Part 2: Compute-Bound Async Operations  Advanced .NET Threading, Part 3: I/O-Bound Async Operations  Advanced .NET Threading, Part 4: Thread Synchronization Primitives  Advanced .NET Threading, Part 5: Thread Synchronization Locks",
            "title": "Advanced .NET Threading"
        },
        {
            "location": "/dotNET/WPF/",
            "text": "Useful Links\n\u00b6\n\n\nWPF tutorial\n\n\nWPF Documentation\n\n\nWPF Samples\n\n\nWPF Tools\n\n\nApplication\n\u00b6\n\n\n<Application\n \nx:Class=\n\"ExpenseIt.App\"\n\n     \nxmlns=\n\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"\n\n     \nxmlns:x=\n\"http://schemas.microsoft.com/winfx/2006/xaml\"\n\n     \nStartupUri=\n\"MainWindow.xaml\"\n>\n\n    \n<Application.Resources>\n\n    \n</Application.Resources>\n\n\n</Application>\n\n\n\n\n\n\nCommands\n\u00b6\n\n\nWPF Commands\n\n\nWPF provides a set of predefined commands. The command library consists of the following classes: ApplicationCommands, NavigationCommands, MediaCommands, EditingCommands, and the ComponentCommands.\n\n\n<StackPanel>\n\n\n\n<StackPanel.ContextMenu>\n\n    \n<ContextMenu>\n\n      \n<MenuItem\n \nCommand=\n\"ApplicationCommands.Properties\"\n \n/>\n\n    \n</ContextMenu>\n\n\n</StackPanel.ContextMenu>\n\n  \n<Menu>\n\n    \n<MenuItem\n \nCommand=\n\"ApplicationCommands.Paste\"\n \n/>\n\n  \n</Menu>\n\n  \n<TextBox\n \n/></StackPanel>\n\n\n\n<Window.InputBindings>\n\n  \n<KeyBinding\n \nKey=\n\"B\"\n\n              \nModifiers=\n\"Control\"\n \n              \nCommand=\n\"ApplicationCommands.Open\"\n \n/></Window.InputBindings>\n\n\n\n\n\n\nPage\n\u00b6\n\n\n<Page\n\n  \nxmlns=\n\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"\n\n  \nxmlns:x=\n\"http://schemas.microsoft.com/winfx/2006/xaml\"\n\n  \nx:Class=\n\"ExampleNamespace.ExampleCode\"\n\n  \n>\n\n  \n<StackPanel>\n\n    \n<Button>\nButton 1\n</Button>\n\n    \n<Button>\nButton 2\n</Button>\n\n    \n<Button>\nButton 3\n</Button>\n\n  \n</StackPanel>\n\n\n</Page>\n\n\n\n\n\n\nStyles\n\u00b6\n\n\n<Window.Resources>\n<!--A Style that affects all TextBlocks-->\n\n\n<Style\n \nTargetType=\n\"TextBlock\"\n>\n\n  \n<Setter\n \nProperty=\n\"HorizontalAlignment\"\n \nValue=\n\"Center\"\n \n/>\n\n  \n<Setter\n \nProperty=\n\"FontFamily\"\n \nValue=\n\"Comic Sans MS\"\n/>\n\n  \n<Setter\n \nProperty=\n\"FontSize\"\n \nValue=\n\"14\"\n/></Style></Window.Resources>\n\n\n<!--A Style that extends the previous TextBlock Style--><!--This is a \"named style\" with an x:Key of TitleText-->\n\n\n<Style\n \nBasedOn=\n\"{StaticResource {x:Type TextBlock}}\"\n\n       \nTargetType=\n\"TextBlock\"\n\n       \nx:Key=\n\"TitleText\"\n>\n\n  \n<Setter\n \nProperty=\n\"FontSize\"\n \nValue=\n\"26\"\n/>\n\n  \n<Setter\n \nProperty=\n\"Foreground\"\n>\n\n  \n<Setter.Value>\n\n      \n<LinearGradientBrush\n \nStartPoint=\n\"0.5,0\"\n \nEndPoint=\n\"0.5,1\"\n>\n\n        \n<LinearGradientBrush.GradientStops>\n\n          \n<GradientStop\n \nOffset=\n\"0.0\"\n \nColor=\n\"#90DDDD\"\n \n/>\n\n          \n<GradientStop\n \nOffset=\n\"1.0\"\n \nColor=\n\"#5BFFFF\"\n \n/>\n\n        \n</LinearGradientBrush.GradientStops>\n\n      \n</LinearGradientBrush>\n\n    \n</Setter.Value>\n\n  \n</Setter></Style>\n\n\n\n\n\n\nTriggers\n\u00b6\n\n\n<Style\n \nx:Key=\n\"SpecialButton\"\n \nTargetType=\n\"{x:Type Button}\"\n>\n\n  \n<Style.Triggers>\n\n    \n<Trigger\n \nProperty=\n\"Button.IsMouseOver\"\n \nValue=\n\"true\"\n>\n\n      \n<Setter\n \nProperty =\n \n\"Background\"\n \nValue=\n\"Red\"\n/>\n\n    \n</Trigger>\n\n    \n<Trigger\n \nProperty=\n\"Button.IsPressed\"\n \nValue=\n\"true\"\n>\n\n      \n<Setter\n \nProperty =\n \n\"Foreground\"\n \nValue=\n\"Green\"\n/>\n\n    \n</Trigger>\n\n  \n</Style.Triggers></Style>",
            "title": "WPF"
        },
        {
            "location": "/dotNET/WPF/#useful-links",
            "text": "WPF tutorial  WPF Documentation  WPF Samples  WPF Tools",
            "title": "Useful Links"
        },
        {
            "location": "/dotNET/WPF/#application",
            "text": "<Application   x:Class= \"ExpenseIt.App\" \n      xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" \n      xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" \n      StartupUri= \"MainWindow.xaml\" > \n     <Application.Resources> \n     </Application.Resources>  </Application>",
            "title": "Application"
        },
        {
            "location": "/dotNET/WPF/#commands",
            "text": "WPF Commands  WPF provides a set of predefined commands. The command library consists of the following classes: ApplicationCommands, NavigationCommands, MediaCommands, EditingCommands, and the ComponentCommands.  <StackPanel>  <StackPanel.ContextMenu> \n     <ContextMenu> \n       <MenuItem   Command= \"ApplicationCommands.Properties\"   /> \n     </ContextMenu>  </StackPanel.ContextMenu> \n   <Menu> \n     <MenuItem   Command= \"ApplicationCommands.Paste\"   /> \n   </Menu> \n   <TextBox   /></StackPanel>  <Window.InputBindings> \n   <KeyBinding   Key= \"B\" \n               Modifiers= \"Control\"  \n               Command= \"ApplicationCommands.Open\"   /></Window.InputBindings>",
            "title": "Commands"
        },
        {
            "location": "/dotNET/WPF/#page",
            "text": "<Page \n   xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" \n   xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" \n   x:Class= \"ExampleNamespace.ExampleCode\" \n   > \n   <StackPanel> \n     <Button> Button 1 </Button> \n     <Button> Button 2 </Button> \n     <Button> Button 3 </Button> \n   </StackPanel>  </Page>",
            "title": "Page"
        },
        {
            "location": "/dotNET/WPF/#styles",
            "text": "<Window.Resources> <!--A Style that affects all TextBlocks-->  <Style   TargetType= \"TextBlock\" > \n   <Setter   Property= \"HorizontalAlignment\"   Value= \"Center\"   /> \n   <Setter   Property= \"FontFamily\"   Value= \"Comic Sans MS\" /> \n   <Setter   Property= \"FontSize\"   Value= \"14\" /></Style></Window.Resources>  <!--A Style that extends the previous TextBlock Style--><!--This is a \"named style\" with an x:Key of TitleText-->  <Style   BasedOn= \"{StaticResource {x:Type TextBlock}}\" \n        TargetType= \"TextBlock\" \n        x:Key= \"TitleText\" > \n   <Setter   Property= \"FontSize\"   Value= \"26\" /> \n   <Setter   Property= \"Foreground\" > \n   <Setter.Value> \n       <LinearGradientBrush   StartPoint= \"0.5,0\"   EndPoint= \"0.5,1\" > \n         <LinearGradientBrush.GradientStops> \n           <GradientStop   Offset= \"0.0\"   Color= \"#90DDDD\"   /> \n           <GradientStop   Offset= \"1.0\"   Color= \"#5BFFFF\"   /> \n         </LinearGradientBrush.GradientStops> \n       </LinearGradientBrush> \n     </Setter.Value> \n   </Setter></Style>",
            "title": "Styles"
        },
        {
            "location": "/dotNET/WPF/#triggers",
            "text": "<Style   x:Key= \"SpecialButton\"   TargetType= \"{x:Type Button}\" > \n   <Style.Triggers> \n     <Trigger   Property= \"Button.IsMouseOver\"   Value= \"true\" > \n       <Setter   Property =   \"Background\"   Value= \"Red\" /> \n     </Trigger> \n     <Trigger   Property= \"Button.IsPressed\"   Value= \"true\" > \n       <Setter   Property =   \"Foreground\"   Value= \"Green\" /> \n     </Trigger> \n   </Style.Triggers></Style>",
            "title": "Triggers"
        }
    ]
}